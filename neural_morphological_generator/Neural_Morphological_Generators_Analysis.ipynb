{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23V6Lmrzr_2O"
      },
      "source": [
        "<div align=right>\n",
        "LAP 3 / EMLCT Computational Morphology<br>\n",
        "M. Ribalta i Albado<br>\n",
        "Fall 2023\n",
        "</div>\n",
        "\n",
        "<h1 align=center>Neural Morphological Generators in Catalan and German</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R0FMJt7r_2S"
      },
      "source": [
        "In this exercise we create training, validation and test data for a morphological generator with the Transformer. Then, we train different models with different datasets and analyze the results and impact. We have done this in two languages and compared them.\n",
        "\n",
        "$ $\n",
        "\n",
        "* All data has been gathered from the [Unimorph](https://unimorph.github.io/) website.\n",
        "\n",
        "* The two languages have been:\n",
        "  * **Catalan**, the native language of the author of this notebook. This dataset **only contains verbs**.\n",
        "  * **German**, which the author has basic notions of. This dataset is a bit more complex since it contains **verbs and adjectives**.\n",
        "\n",
        "* For each language, 3 datasets have been created using three strategies, for brevity we will be referring to them as:\n",
        "  * `random`: the samples populating training, validation and test have been chosen randomly.\n",
        "  * `no_overlap`: the samples populating training and validation are different from the test ones. The stems used in test have never been seen during the training step.\n",
        "  * `fake_copy`: same as the previous one. However, the training samples include the stems from the test set with a \"mock\" inflection. This inflection is `COPY` and the source and target words are the same (e.g. source is `walk # COPY` and target is `walk`).\n",
        "* Since Datasets were of different sizes, we have chosen training, validation and test size according to proportions rather than strict numbers.\n",
        "  * Training represents 70% of the data.\n",
        "  * Validation represents 10% of the data.\n",
        "  * Test represents 20% of the data.\n",
        "* To train, we have used Transformers available in the library of [fairseq](https://github.com/facebookresearch/fairseq) from Facebook Research.\n",
        "\n",
        "\n",
        "$ $\n",
        "\n",
        "Below, it is shown the table of contents:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Reproducibility constraints](#scrollTo=c8b91EYl4eY9)\n",
        "\n",
        ">[ðŸ›  Imports, functions and set up](#scrollTo=V0d7WFr6xF3F)\n",
        "\n",
        ">[ðŸ‰ðŸŒ¹ Catalan Neural Morphological Generator](#scrollTo=FZn8v9xyXlbX)\n",
        "\n",
        ">>[Preprocessing](#scrollTo=ldQLAZ5twqJV)\n",
        "\n",
        ">>>[EDA](#scrollTo=yenoYOuMuHBV)\n",
        "\n",
        ">>[Tokenization](#scrollTo=eCd9SRY9ekUw)\n",
        "\n",
        ">>>[Random](#scrollTo=5R9Amk20Sogu)\n",
        "\n",
        ">>>[No overlap](#scrollTo=77BP2ucIf-Db)\n",
        "\n",
        ">>>[Fake Copy](#scrollTo=xuBj13WUt2Xi)\n",
        "\n",
        ">>[Training](#scrollTo=6NhE9JKCYopt)\n",
        "\n",
        ">>>[Random](#scrollTo=hOYXsQivAAWW)\n",
        "\n",
        ">>>[No overlap](#scrollTo=FlOFXpu3gDlP)\n",
        "\n",
        ">>>[Fake copy](#scrollTo=yJHwb_PfAJpH)\n",
        "\n",
        ">>[Evaluation](#scrollTo=y51WWydWAVX4)\n",
        "\n",
        ">>[Conclusions, improvements and future work](#scrollTo=izHY71n9QvIw)\n",
        "\n",
        ">[ðŸ‡©ðŸ‡ª German Neural Morphological Generator](#scrollTo=GlUiV9vfpNP0)\n",
        "\n",
        ">>[Preprocessing](#scrollTo=dwww3CJnupmq)\n",
        "\n",
        ">>>[EDA](#scrollTo=snbk9s4musT1)\n",
        "\n",
        ">>[Tokenization](#scrollTo=7gTRMW2Rrx91)\n",
        "\n",
        ">>>[Random](#scrollTo=dHg75H3Vu0Oo)\n",
        "\n",
        ">>>[No overlap](#scrollTo=7yTXnHN3u2RW)\n",
        "\n",
        ">>>[Fake copy](#scrollTo=kbKfbBcPu4lh)\n",
        "\n",
        ">>[Training](#scrollTo=3PPhOvsRsxIa)\n",
        "\n",
        ">>>[Random](#scrollTo=3PPhOvsRsxIa)\n",
        "\n",
        ">>>[No overlap](#scrollTo=ciu-M6-dv5XI)\n",
        "\n",
        ">>>[Fake copy](#scrollTo=JZRXl3Nvv21K)\n",
        "\n",
        ">>[Evaluation](#scrollTo=bR15htWkEmu1)\n",
        "\n",
        ">>[Conclusions, improvements and future work](#scrollTo=BciHvtvc4Kb2)\n",
        "\n",
        ">[General Conclusions](#scrollTo=6U2ZofN8O8io)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "8W-rMpb9tbiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting...\n",
        "\n",
        "# Reproducibility constraints\n",
        "\n",
        "If you desire to reproduce step by step all this notebook you need to:\n",
        "\n",
        "* Allow this collab to acces Google Drive when asked in the following section.\n",
        "* Be aware that many cells (specially in the training section) take a bit long to execute, GPU usage is highly recommended in this case. Because of it, it is recommended to simply look at the output of the cells without executing them.\n",
        "* Some cells in this report read or execute from files external to this collab. These have been linked in the handover of the project. (e.g. the preprocess, training and test scripts, the input data, the datasets with the results of the test data after evaluated, etc)"
      ],
      "metadata": {
        "id": "c8b91EYl4eY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ›  Libraries, functions and set up\n",
        "\n",
        "For better readibility of the analysis, some of the functions have been encapsulated inside this section.\n",
        "\n",
        "Feel free to simply execute this group of cells without expanding this section by clicking on the play button at the left corner below. Alternatively, you can also read the different methods implemented for this analysis and execute it one by one."
      ],
      "metadata": {
        "id": "V0d7WFr6xF3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here, we mount Google drive so that Colab can access files in your Google drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/MyDrive/morphology_project\n",
        "!ls -al"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DTK4DY8w8Nj",
        "outputId": "92ec2cfe-6521-4204-fca9-758ecd4f81d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/MyDrive/morphology_project\n",
            "total 122142\n",
            "drwx------ 2 root root     4096 Dec 31 15:38 cat\n",
            "-rw------- 1 root root  2902458 Dec 31 10:15 cat.txt\n",
            "drwx------ 2 root root     4096 Dec 31 16:09 checkpoints\n",
            "drwx------ 2 root root     4096 Dec 31 18:06 data-bin\n",
            "-rw------- 1 root root 18932560 Jan  1 15:32 deu.txt\n",
            "-rw------- 1 root root   279074 Jan  1 14:42 devel.cat.fake_copy.input\n",
            "-rw------- 1 root root   150496 Jan  1 14:42 devel.cat.fake_copy.output\n",
            "-rw------- 1 root root   279074 Jan  1 14:42 devel.cat.no_overlap.input\n",
            "-rw------- 1 root root   150496 Jan  1 14:42 devel.cat.no_overlap.output\n",
            "-rw------- 1 root root   280154 Jan  1 14:42 devel.cat.random.input\n",
            "-rw------- 1 root root   152400 Jan  1 14:42 devel.cat.random.output\n",
            "-rw------- 1 root root  1870504 Jan  1 17:01 devel.deu.fake_copy.input\n",
            "-rw------- 1 root root  1114724 Jan  1 17:01 devel.deu.fake_copy.output\n",
            "-rw------- 1 root root  1870504 Jan  1 17:01 devel.deu.no_overlap.input\n",
            "-rw------- 1 root root  1114724 Jan  1 17:01 devel.deu.no_overlap.output\n",
            "-rw------- 1 root root  1837306 Jan  1 17:01 devel.deu.random.input\n",
            "-rw------- 1 root root  1098538 Jan  1 17:01 devel.deu.random.output\n",
            "-rw------- 1 root root      600 Jan  1 14:43 preprocess.sh\n",
            "drwx------ 2 root root     4096 Dec 31 18:11 results\n",
            "-rw------- 1 root root   280146 Jan  1 14:42 test.cat.fake_copy.input\n",
            "-rw------- 1 root root   152925 Jan  1 14:42 test.cat.fake_copy.output\n",
            "-rw------- 1 root root   280146 Jan  1 14:42 test.cat.no_overlap.input\n",
            "-rw------- 1 root root   152925 Jan  1 14:42 test.cat.no_overlap.output\n",
            "-rw------- 1 root root   279951 Jan  1 14:42 test.cat.random.input\n",
            "-rw------- 1 root root   151941 Jan  1 14:42 test.cat.random.output\n",
            "-rw------- 1 root root  1832352 Jan  1 17:01 test.deu.fake_copy.input\n",
            "-rw------- 1 root root  1101058 Jan  1 17:01 test.deu.fake_copy.output\n",
            "-rw------- 1 root root  1832352 Jan  1 17:01 test.deu.no_overlap.input\n",
            "-rw------- 1 root root  1101058 Jan  1 17:01 test.deu.no_overlap.output\n",
            "-rw------- 1 root root  1836250 Jan  1 17:01 test.deu.random.input\n",
            "-rw------- 1 root root  1096467 Jan  1 17:01 test.deu.random.output\n",
            "-rw------- 1 root root  2017675 Jan  1 10:41 test_random_results.txt\n",
            "-rw------- 1 root root      307 Jan  1 11:05 test.sh\n",
            "-rw------- 1 root root  2239259 Jan  1 14:42 train.cat.fake_copy.input\n",
            "-rw------- 1 root root  1215866 Jan  1 14:42 train.cat.fake_copy.output\n",
            "-rw------- 1 root root  2235637 Jan  1 14:42 train.cat.no_overlap.input\n",
            "-rw------- 1 root root  1213174 Jan  1 14:42 train.cat.no_overlap.output\n",
            "-rw------- 1 root root  2234752 Jan  1 14:42 train.cat.random.input\n",
            "-rw------- 1 root root  1212254 Jan  1 14:42 train.cat.random.output\n",
            "-rw------- 1 root root 14775533 Jan  1 17:01 train.deu.fake_copy.input\n",
            "-rw------- 1 root root  8848828 Jan  1 17:01 train.deu.fake_copy.output\n",
            "-rw------- 1 root root 14669183 Jan  1 17:01 train.deu.no_overlap.input\n",
            "-rw------- 1 root root  8766106 Jan  1 17:01 train.deu.no_overlap.output\n",
            "-rw------- 1 root root 14698483 Jan  1 17:01 train.deu.random.input\n",
            "-rw------- 1 root root  8786883 Jan  1 17:01 train.deu.random.output\n",
            "-rw------- 1 root root     1765 Dec 31 16:53 train.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing and EDA\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Dataset Splits\n",
        "import random\n",
        "\n",
        "# Dataset Evaluation\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from IPython.display import display_html\n",
        "from itertools import chain,cycle"
      ],
      "metadata": {
        "id": "X-orgT-px8c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fairseq\n",
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvZvtHhTxnqe",
        "outputId": "95c52a15-06d3-4250-b738-fdc8e1b6bdd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fairseq in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.7)\n",
            "Requirement already satisfied: hydra-core<1.1,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.0.7)\n",
            "Requirement already satisfied: omegaconf<2.1 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2023.6.3)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.1)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.9.2)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.23.5)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.10/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq) (4.8)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.5.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (2.6.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def completely_random_splits(data: list):\n",
        "  # randomize\n",
        "  random.seed(42) # Set a random seed so we always get the same (reproducible) order\n",
        "  tokens = data.copy()\n",
        "  random.shuffle(tokens)\n",
        "\n",
        "  # split into datasets\n",
        "  length = len(tokens)\n",
        "  train_idx = int(length*0.8)\n",
        "  devel_idx = int(length*0.9)\n",
        "\n",
        "  train = tokens[:train_idx]\n",
        "  devel = tokens[train_idx:devel_idx]\n",
        "  test = tokens[devel_idx:]\n",
        "\n",
        "  return train, devel, test"
      ],
      "metadata": {
        "id": "en5cSWs8xb4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def no_overlap_splits(data):\n",
        "  # randomize lemmas\n",
        "  random.seed(42)\n",
        "  tokens = data.copy()\n",
        "\n",
        "  lemmas = list(set([item[0].split('#')[0] for item in tokens]))\n",
        "  random.shuffle(lemmas)\n",
        "\n",
        "  # split lemmas into datasets, no overlap\n",
        "  length = len(lemmas)\n",
        "  train_idx = int(length*0.8)\n",
        "  devel_idx = int(length*0.9)\n",
        "\n",
        "  train_lemmas = lemmas[:train_idx]\n",
        "  devel_lemmas = lemmas[train_idx:devel_idx]\n",
        "  test_lemmas = lemmas[devel_idx:]\n",
        "\n",
        "  # take each sample from each lemma and add it to the corresponding dataset\n",
        "  train, devel, test = [], [], []\n",
        "  for sample in tokens:\n",
        "    lemma = sample[0].split('#')[0]\n",
        "    if lemma in train_lemmas:\n",
        "      train.append(sample)\n",
        "    elif lemma in devel_lemmas:\n",
        "      devel.append(sample)\n",
        "    else:\n",
        "      test.append(sample)\n",
        "\n",
        "  return train, devel, test, test_lemmas # we will use test_lemmas in the next section"
      ],
      "metadata": {
        "id": "hqvrpr4lxi7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fake_copy_splits(data):\n",
        "  # no overlap samples\n",
        "  train, devel, test, test_lemmas = no_overlap_splits(tokenized)\n",
        "\n",
        "  # add samples from test in fake-copy mode in train\n",
        "  for test_lemma in test_lemmas:\n",
        "    train.append((f'{test_lemma}# COPY', test_lemma))\n",
        "\n",
        "  # shuffle train again\n",
        "  random.shuffle(train)\n",
        "\n",
        "  return train, devel, test"
      ],
      "metadata": {
        "id": "53weJOWsxmNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_file(file, filename):\n",
        "  with open(filename, 'w') as fp:\n",
        "    for item in file:\n",
        "        # write each item on a new line\n",
        "        fp.write(\"%s\\n\" % item)"
      ],
      "metadata": {
        "id": "qxM3DhumxcgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_input_output_files(data: list, language:str, split_mode: str, dataset_type: str):\n",
        "  input = [sample[0] for sample in data]\n",
        "  output = [sample[1] for sample in data]\n",
        "\n",
        "# devel cat random output\n",
        "  save_to_file(input, f'{dataset_type}.{language}.{split_mode}.input')\n",
        "  save_to_file(output, f'{dataset_type}.{language}.{split_mode}.output')"
      ],
      "metadata": {
        "id": "iX9I1u4Ex2-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_results_table(filename: str) -> pd.DataFrame:\n",
        "    S, T, D = [], [], []\n",
        "    with open(f\"results/{filename}\",\"r\") as fi:\n",
        "        for ln in fi:\n",
        "            ln = ln.replace('\\t', ' @ ').replace('\\n', '')\n",
        "            if ln.startswith(\"S-\"):\n",
        "                S.append(ln[2:])\n",
        "            elif ln.startswith(\"T-\"):\n",
        "                T.append(ln[2:])\n",
        "            elif ln.startswith(\"D-\"):\n",
        "                D.append(ln[2:])\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "    df_s = pd.DataFrame(S)\n",
        "    df_s[['idx', 'source_s']] = df_s[0].str.split('@', expand=True)\n",
        "    df_s = df_s.drop(columns=0).set_index('idx')\n",
        "\n",
        "    df_t = pd.DataFrame(T)\n",
        "    df_t[['idx', 'target']] = df_t[0].str.split('@', expand=True)\n",
        "    df_t = df_t.drop(columns=0).set_index('idx')\n",
        "\n",
        "    df_d = pd.DataFrame(D)\n",
        "    df_d[['idx', 'score_d', 'predicted']] = df_d[0].str.split('@', expand=True)\n",
        "    df_d = df_d.drop(columns=0).set_index('idx')\n",
        "\n",
        "    df = pd.concat([df_s, df_t, df_d], axis=1, join=\"inner\")\n",
        "    df['lemma'] = df['source_s'].map(lambda row: row.split('#')[0]) # add the original lemma of the verb for analytic purposes\n",
        "\n",
        "    return df[['lemma','source_s','target','predicted']]"
      ],
      "metadata": {
        "id": "2YLU5nMwyFy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_verb_ending_results(results_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    results_df['verb_ending'] = results_df.lemma.map(lambda verb: verb[-4:])\n",
        "    results_df['correct'] = results_df.target == results_df.predicted\n",
        "    results_df['counts'] = 1\n",
        "    return results_df[['verb_ending', 'correct', 'counts']].groupby(['verb_ending', 'correct']).count().reset_index()\n",
        "\n",
        "def plot_verb_ending_correctness(results_df: pd.DataFrame):\n",
        "  verb_ending_df = get_verb_ending_results(results_df)\n",
        "\n",
        "  fig, (ax1, ax2) = plt.subplots(2, 1)\n",
        "  fig.suptitle('Verb Ending Correctness')\n",
        "  ax1.bar(\n",
        "      verb_ending_df[verb_ending_df.correct == True].verb_ending,\n",
        "      verb_ending_df[verb_ending_df.correct == True].counts,\n",
        "  )\n",
        "  ax1.set(ylabel='Correct Predictions')\n",
        "  ax2.bar(\n",
        "      verb_ending_df[verb_ending_df.correct == False].verb_ending,\n",
        "      verb_ending_df[verb_ending_df.correct == False].counts,\n",
        "      color='orange'\n",
        "  )\n",
        "  ax2.set(xlabel='Verb Ending', ylabel='Wrong Predictions')\n",
        "  fig.show()"
      ],
      "metadata": {
        "id": "Mot-4ca2D3Wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_grammatical_category(morph_tag: str) -> str:\n",
        "  # we expect to receive something like: 'J a r # N NOM NEUT SG'\n",
        "  morph_tag = morph_tag.split('#')[1]\n",
        "  return morph_tag.split(' ')[1]\n",
        "\n",
        "\n",
        "def get_grammatical_category_results(results_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    results_df['grammatical_category'] = results_df.source_s.map(get_grammatical_category)\n",
        "    results_df['correct'] = results_df.target == results_df.predicted\n",
        "    results_df['counts'] = 1\n",
        "    return results_df[['grammatical_category', 'correct', 'counts']].groupby(['grammatical_category', 'correct']).count().reset_index()\n",
        "\n",
        "def plot_grammatical_category_correctness(results_df: pd.DataFrame):\n",
        "  grammatical_cat_df = get_grammatical_category_results(results_df)\n",
        "\n",
        "  fig, (ax1, ax2) = plt.subplots(2, 1)\n",
        "\n",
        "  fig.suptitle(f'Grammatical Category Correctness')\n",
        "  ax1.bar(\n",
        "      grammatical_cat_df[grammatical_cat_df.correct == True].grammatical_category,\n",
        "      grammatical_cat_df[grammatical_cat_df.correct == True].counts,\n",
        "  )\n",
        "  ax1.set(ylabel='Correct Predictions')\n",
        "  ax2.bar(\n",
        "      grammatical_cat_df[grammatical_cat_df.correct == False].grammatical_category,\n",
        "      grammatical_cat_df[grammatical_cat_df.correct == False].counts,\n",
        "      color='orange'\n",
        "  )\n",
        "  ax2.set(xlabel='Grammatical Category Correctness', ylabel='Wrong Predictions')\n",
        "  fig.show()"
      ],
      "metadata": {
        "id": "7c6FRimw39Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_case(morph_tags: str) -> str:\n",
        "  morph_tags = morph_tags.split('#')[1]\n",
        "  return morph_tags.split(' ')[2]\n",
        "\n",
        "def get_gender(morph_tags: str) -> str:\n",
        "  morph_tags = morph_tags.split('#')[1]\n",
        "  gender = morph_tags.split(' ')[3]\n",
        "  if gender == 'PL' or gender == 'SG':\n",
        "    return 'NA'\n",
        "  return gender\n",
        "\n",
        "def get_number(morph_tags: str) -> str:\n",
        "  morph_tags = morph_tags.split('#')[1]\n",
        "\n",
        "  try:\n",
        "    number = morph_tags.split(' ')[4]\n",
        "  except:\n",
        "    number = morph_tags.split(' ')[3]\n",
        "    if not ((number == 'SG') or (number == 'PL')):\n",
        "      number = 'NA'\n",
        "\n",
        "  return number"
      ],
      "metadata": {
        "id": "x_q6qhkZNOoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_side_by_side(*args,titles=cycle([''])):\n",
        "    html_str=''\n",
        "    for df,title in zip(args, chain(titles,cycle(['</br>'])) ):\n",
        "        html_str+='<th style=\"text-align:center\"><td style=\"vertical-align:top\">'\n",
        "        html_str+=f'<h2 style=\"text-align: center;\">{title}</h2>'\n",
        "        html_str+=df.to_html().replace('table','table style=\"display:inline\"')\n",
        "        html_str+='</td></th>'\n",
        "    display_html(html_str,raw=True)\n"
      ],
      "metadata": {
        "id": "yZrNmMOnqIdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZn8v9xyXlbX"
      },
      "source": [
        "# ðŸ‰ðŸŒ¹ Catalan Neural Morphological Generator\n",
        "\n",
        "* Data used can be downloaded here: https://github.com/unimorph/cat\n",
        "* This dataset contains 81576 samples, formed by a total of 1547 different stems.\n",
        "* In this case, we only have verbs derived to different tenses (in Catalan there are several ways of referring to the past, present and future, as well as subjunctive, imperative and infinitive forms).\n",
        "* Catalan can also have different morphemes depending on the gender of the subject.\n",
        "* It is mostly a non-agglutinative language (unlike German) so there are no great variations or exceptions when applying a tense to a verb."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "ldQLAZ5twqJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EDA"
      ],
      "metadata": {
        "id": "yenoYOuMuHBV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBaqevwqaKDk"
      },
      "source": [
        "We will first read the files downloaded from Unimorph and adapt the format that the neural network expects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrksciCCr_2T"
      },
      "outputs": [],
      "source": [
        "examples = [l.strip() for l in open(\"cat.txt\") if len(l) > 1]\n",
        "examples = [s.replace(' ', '_') for s in examples] # Can't use spaces as tokens, so replace those"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioQSYQdZaamp"
      },
      "source": [
        "Let's take a look at some examples of the dataset, we will also take a look to how many inflections most repeated verbs have and the least ones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Td7iTu0661ro"
      },
      "outputs": [],
      "source": [
        "counter = Counter()\n",
        "\n",
        "for example in examples:\n",
        "  counter[example.split('\\t')[0]] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvAP2hs87Jqk",
        "outputId": "9646248a-9570-4240-d46a-361392222a4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('abaixar', 53),\n",
              " ('abandonar', 53),\n",
              " ('abatre', 53),\n",
              " ('abdicar', 53),\n",
              " ('abeurar', 53),\n",
              " ('abolir', 53),\n",
              " ('abominar', 53),\n",
              " ('abraÃ§ar', 53),\n",
              " ('abrandar', 53),\n",
              " ('abreujar', 53)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "counter.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f4-OFOi7O5s",
        "outputId": "5cd6b1fc-2484-42b1-969d-0d6c7bad210e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('transmetre', 50),\n",
              " ('traure', 50),\n",
              " ('treure', 50),\n",
              " ('vendre', 50),\n",
              " (\"veure's\", 50),\n",
              " ('veure', 50),\n",
              " ('viure', 50),\n",
              " ('haver', 45),\n",
              " ('dar', 39),\n",
              " ('caldre', 17)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "counter.most_common()[-10:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nr_of_verbs_w_samples = Counter(counts for verb, counts in counter.most_common())\n",
        "print(nr_of_verbs_w_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87SVqbktryGJ",
        "outputId": "995930e6-2a66-48c0-d311-fc01211ae95f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({53: 1425, 50: 119, 45: 1, 39: 1, 17: 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Most of our verb stems (1425) have 53 derivations assigned to a same root.\n",
        "* 119 stems have only 50 derived versions of it.\n",
        "\n",
        "* \"Haver\" (an auxiliary similar to \"there is\") has \"only\" 45 derived forms. This verb is constantly used due to its auxiliary nature, however its structure is quite similar and repeated in most tenses. This is probably why there are less samples of it.\n",
        "* \"Dar\" (to give) has only 39 derived forms. As a curiosity, it is not a very common (or used) verb nowadays and mostly known by elderly people. Currently, the most popular version of it is the verb \"donar\" (to give, as well).\n",
        "* \"Caldre\" (similar to \"need\") has only 17 derived forms in the dataset. This verb is mostly used in an impersonal manner \"cal comprar menjar\" (\"there's the need to buy food\").\n",
        "\n",
        "Since the data is originally from Wikipedia, it is not rare that these verbs have less derived forms since they are less used in specific tenses and by the Catalan people. At the end of the day, our data tends to be the reflection of society's usage of it, in this case, the usage of verbs.\n"
      ],
      "metadata": {
        "id": "asBsyczNs6qv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rdzft-9uavZI"
      },
      "source": [
        "Let's now take a look at verb endings. It is interesting to note that verbs in Catalan can finish in -ar, -er, -re or -ir. However, this list also includes reflexive versions of the verbs, see: \"veure\" (to see) and \"veure's\" (to see oneself). It might be interesting to check later in the evaluation step if the model has been able to infer correctly these specific formats of the verbs, since verb endings have, usually, information about what structure should we follow when deriving a verb."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJR1WUxkbsPY"
      },
      "outputs": [],
      "source": [
        "verb_endings = Counter()\n",
        "for verb in counter.keys():\n",
        "  verb_ending = verb[-2:]\n",
        "  verb_endings[verb_ending] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "ZI26xxe1cU0f",
        "outputId": "2fac64bd-7e0c-4aa2-c3a6-bf766210a136"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig = plt.figure(figsize = (10, 2))\n",
        "plt.bar([f\"-{ter}\" for ter in verb_endings.keys()], verb_endings.values())\n",
        "\n",
        "plt.xlabel(\"Termination\")\n",
        "plt.ylabel(\"No. of verbs with that termination\")\n",
        "plt.title(\"Verb termination distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMniUWUucyBh"
      },
      "source": [
        "We can see that there is a great imbalance between verb terminations. This might affect the results of our neural morphological generator, probably being biased towards word forms present in \"-ar\" verbs. It is probably also that reflexive verbs (\"-se\", \"-'s\")\n",
        "\n",
        "Now that we have taken a look at our data, let's tokenize and prepare it for training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sb3s-aWpr_2U",
        "outputId": "4f5fba50-5823-415a-e61a-b77c7085631a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a b a i x a r # V.PTCP PST SG FEM', 'a b a i x a d a'),\n",
              " ('a b a i x a r # V.PTCP PST PL FEM', 'a b a i x a d e s'),\n",
              " ('a b a i x a r # V.PTCP PRS', 'a b a i x a n t'),\n",
              " ('a b a i x a r # V IND FUT 3 PL', 'a b a i x a r a n'),\n",
              " ('a b a i x a r # V IND FUT 2 SG', 'a b a i x a r Ã  s'),\n",
              " ('a b a i x a r # V IND FUT 3 SG', 'a b a i x a r Ã '),\n",
              " ('a b a i x a r # V IND FUT 1 PL', 'a b a i x a r e m'),\n",
              " ('a b a i x a r # V IND PST 1 PL PFV', 'a b a i x Ã  r e m'),\n",
              " ('a b a i x a r # V IND PST 3 PL PFV', 'a b a i x a r e n'),\n",
              " ('a b a i x a r # V IND PST 2 SG PFV', 'a b a i x a r e s')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "tokenized = []\n",
        "for ex in examples:\n",
        "    lemma, inflection, tags = ex.split('\\t')\n",
        "    tagtokens = tags.split(';')\n",
        "    tokenized.append((' '.join(list(lemma)) + \" # \" + ' '.join(tagtokens),\\\n",
        "                      ' '.join(list(inflection))))\n",
        "\n",
        "tokenized[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCd9SRY9ekUw"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "We want to have data in 2-tuples `tokenized` that look like this:\n",
        "[(input string 1, output string 1), (input string 2, output string 2, ...)]\n",
        "\n",
        "Remember that in this step we will randomize in three different ways:\n",
        "\n",
        "1. Completely random.\n",
        "2. No lemma overlap between train and test.\n",
        "3. No lemma overlap and fake copy examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random"
      ],
      "metadata": {
        "id": "5R9Amk20Sogu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vk1ZoJLIfim6"
      },
      "outputs": [],
      "source": [
        "random_train, random_devel, random_test = completely_random_splits(tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryhX5fD1mV3u",
        "outputId": "b89e8d99-1c5b-4a49-eaee-ecf98cc85ba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65260 8158 8158\n"
          ]
        }
      ],
      "source": [
        "print(len(random_train), len(random_devel), len(random_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77BP2ucIf-Db"
      },
      "source": [
        "### No overlap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrJ7M7Wwnfyp"
      },
      "outputs": [],
      "source": [
        "no_overlap_train, no_overlap_devel, no_overlap_test, test_lemmas = no_overlap_splits(tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzXbn_MjwFxi",
        "outputId": "b9d1c5fd-b619-4720-bec4-16e62b287e1f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a b a i x a r # V.PTCP PST SG FEM', 'a b a i x a d a'),\n",
              " ('a b a i x a r # V.PTCP PST PL FEM', 'a b a i x a d e s'),\n",
              " ('a b a i x a r # V.PTCP PRS', 'a b a i x a n t'),\n",
              " ('a b a i x a r # V IND FUT 3 PL', 'a b a i x a r a n'),\n",
              " ('a b a i x a r # V IND FUT 2 SG', 'a b a i x a r Ã  s'),\n",
              " ('a b a i x a r # V IND FUT 3 SG', 'a b a i x a r Ã '),\n",
              " ('a b a i x a r # V IND FUT 1 PL', 'a b a i x a r e m'),\n",
              " ('a b a i x a r # V IND PST 1 PL PFV', 'a b a i x Ã  r e m'),\n",
              " ('a b a i x a r # V IND PST 3 PL PFV', 'a b a i x a r e n'),\n",
              " ('a b a i x a r # V IND PST 2 SG PFV', 'a b a i x a r e s')]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "no_overlap_train[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuBj13WUt2Xi"
      },
      "source": [
        "### Fake Copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p5CuWatvAVQ"
      },
      "outputs": [],
      "source": [
        "fake_copy_train, fake_copy_devel, fake_copy_test = fake_copy_splits(tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYoqNehsxdCP"
      },
      "outputs": [],
      "source": [
        "for i in range(len(fake_copy_train)):\n",
        "  if fake_copy_train[i][0].split('#')[1] == ' COPY':\n",
        "    idx = i\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lr3pcXulw3n9",
        "outputId": "27260c5e-2c29-4d92-c283-6eb885eb82aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('a b r a Ã§ a r # COPY', 'a b r a Ã§ a r ')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "fake_copy_train[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9QNF4ZixwWK"
      },
      "source": [
        "Now we have everything we need. We will store our datasets into files. We will name the files following this style:\n",
        "\n",
        "`{language}.{split_mode}.{dataset_type}.{input_or_output}`\n",
        "\n",
        "Language can be:\n",
        "* `cat` for Catalan.\n",
        "* `deu` for German.\n",
        "\n",
        "Split mode is:\n",
        "* `random` for random split.\n",
        "* `no_overlap`, for no overlap split.\n",
        "* `fake_copy`, for no overlap split but fake copy samples in train.\n",
        "\n",
        "Dataset type:\n",
        "* `train` for training samples.\n",
        "* `devel` for validation samples.\n",
        "* `test` for test samples.\n",
        "\n",
        "Input or output:\n",
        "* `input` for the homonymous.\n",
        "* `output` for the homonymous."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wntRf2I_z1NF"
      },
      "outputs": [],
      "source": [
        "save_input_output_files(random_train, 'cat', 'random', 'train')\n",
        "save_input_output_files(random_devel, 'cat', 'random', 'devel')\n",
        "save_input_output_files(random_test, 'cat', 'random', 'test')\n",
        "\n",
        "save_input_output_files(no_overlap_train, 'cat', 'no_overlap', 'train')\n",
        "save_input_output_files(no_overlap_devel, 'cat', 'no_overlap', 'devel')\n",
        "save_input_output_files(no_overlap_test, 'cat', 'no_overlap', 'test')\n",
        "\n",
        "save_input_output_files(fake_copy_train, 'cat', 'fake_copy', 'train')\n",
        "save_input_output_files(fake_copy_devel, 'cat', 'fake_copy', 'devel')\n",
        "save_input_output_files(fake_copy_test, 'cat', 'fake_copy', 'test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NhE9JKCYopt"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random\n"
      ],
      "metadata": {
        "id": "hOYXsQivAAWW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-DxTrK7pXVl"
      },
      "outputs": [],
      "source": [
        "!bash ./preprocess.sh cat random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash ./train.sh cat random"
      ],
      "metadata": {
        "id": "fSPg3Z0ef53o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!chmod 777 test.sh\n",
        "!./test.sh cat random"
      ],
      "metadata": {
        "id": "PyT20Udzf5bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### No overlap"
      ],
      "metadata": {
        "id": "FlOFXpu3gDlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# interesting to see that in devel and test we have <unk> tokens\n",
        "!bash ./preprocess.sh cat no_overlap"
      ],
      "metadata": {
        "id": "3WeCF1nzeTUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r checkpoints/cat-models/\n",
        "!bash ./train.sh cat no_overlap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a142BPOXfAPq",
        "outputId": "85e8bccf-5c91-4503-c82d-7fac8d2887ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-02 14:17:15.079318: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-02 14:17:15.079378: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-02 14:17:15.100275: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-02 14:17:15.143133: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-02 14:17:19.231896: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-02 14:17:21 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2024-01-02 14:17:28 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 212, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 400, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 400, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 6000, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/cat-models', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=212, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=400, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=400, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=0, max_update=6000, stop_time_hours=0, clip_norm=1.0, sentence_avg=False, update_freq=[1], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/cat-models', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-bin/cat', source_lang='cat.no_overlap.input', target_lang='cat.no_overlap.output', load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=1000, warmup_init_lr=-1, pad=1, eos=2, unk=3, dropout=0.3, attention_dropout=0.3, activation_dropout=0.3, activation_fn='relu', encoder_embed_dim=256, encoder_ffn_embed_dim=1024, encoder_layers=4, encoder_attention_heads=4, encoder_normalize_before=True, decoder_embed_dim=256, decoder_ffn_embed_dim=1024, decoder_layers=4, decoder_attention_heads=4, decoder_normalize_before=True, share_decoder_input_output_embed=True, no_seed_provided=False, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=256, decoder_input_dim=256, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'data-bin/cat', 'source_lang': 'cat.no_overlap.input', 'target_lang': 'cat.no_overlap.output', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 1000, 'warmup_init_lr': -1.0, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-01-02 14:17:29 | INFO | fairseq.tasks.translation | [cat.no_overlap.input] dictionary: 64 types\n",
            "2024-01-02 14:17:29 | INFO | fairseq.tasks.translation | [cat.no_overlap.output] dictionary: 40 types\n",
            "2024-01-02 14:17:29 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(64, 256, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-3): 4 x TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(40, 256, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-3): 4 x TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=256, out_features=40, bias=False)\n",
            "  )\n",
            ")\n",
            "2024-01-02 14:17:29 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2024-01-02 14:17:29 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2024-01-02 14:17:29 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2024-01-02 14:17:29 | INFO | fairseq_cli.train | num. shared model params: 10,555,392 (num. trained: 10,555,392)\n",
            "2024-01-02 14:17:29 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2024-01-02 14:17:30 | INFO | fairseq.data.data_utils | loaded 8,164 examples from: data-bin/cat/valid.cat.no_overlap.input-cat.no_overlap.output.cat.no_overlap.input\n",
            "2024-01-02 14:17:30 | INFO | fairseq.data.data_utils | loaded 8,164 examples from: data-bin/cat/valid.cat.no_overlap.input-cat.no_overlap.output.cat.no_overlap.output\n",
            "2024-01-02 14:17:30 | INFO | fairseq.tasks.translation | data-bin/cat valid cat.no_overlap.input-cat.no_overlap.output 8164 examples\n",
            "2024-01-02 14:17:30 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2024-01-02 14:17:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-01-02 14:17:30 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2024-01-02 14:17:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-01-02 14:17:30 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2024-01-02 14:17:30 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 400\n",
            "2024-01-02 14:17:30 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:17:30 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:17:30 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2024-01-02 14:17:32 | INFO | fairseq.data.data_utils | loaded 65,245 examples from: data-bin/cat/train.cat.no_overlap.input-cat.no_overlap.output.cat.no_overlap.input\n",
            "2024-01-02 14:17:33 | INFO | fairseq.data.data_utils | loaded 65,245 examples from: data-bin/cat/train.cat.no_overlap.input-cat.no_overlap.output.cat.no_overlap.output\n",
            "2024-01-02 14:17:33 | INFO | fairseq.tasks.translation | data-bin/cat train cat.no_overlap.input-cat.no_overlap.output 65245 examples\n",
            "2024-01-02 14:17:33 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2024-01-02 14:17:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 001:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:17:33 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2024-01-02 14:17:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001:   1% 1/165 [00:01<05:09,  1.89s/it]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "epoch 001:  99% 163/165 [00:22<00:00, 10.36it/s, loss=6.212, nll_loss=6.096, ppl=68.41, wps=31225.1, ups=7.73, wpb=4026.6, bsz=396.4, num_updates=100, lr=0.0001, gnorm=2.6, clip=73, train_wall=14, gb_free=13.9, wall=17]2024-01-02 14:17:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   5% 1/22 [00:00<00:02,  8.53it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 17.56it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  27% 6/22 [00:00<00:00, 18.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  41% 9/22 [00:00<00:00, 21.29it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  55% 12/22 [00:00<00:00, 21.76it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  68% 15/22 [00:00<00:00, 21.70it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  82% 18/22 [00:00<00:00, 22.06it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  95% 21/22 [00:00<00:00, 23.34it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:17:57 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.842 | nll_loss 3.454 | ppl 10.96 | wps 90067.1 | wpb 3732.4 | bsz 371.1 | num_updates 165\n",
            "2024-01-02 14:17:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 165 updates\n",
            "2024-01-02 14:17:57 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:17:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:17:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 1 @ 165 updates, score 3.842) (writing took 1.0237509750000129 seconds)\n",
            "2024-01-02 14:17:58 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2024-01-02 14:17:58 | INFO | train | epoch 001 | loss 5.47 | nll_loss 5.299 | ppl 39.37 | wps 29193.1 | ups 7.27 | wpb 4007.6 | bsz 395.4 | num_updates 165 | lr 0.000165 | gnorm 2.156 | clip 72.1 | train_wall 21 | gb_free 13.9 | wall 27\n",
            "2024-01-02 14:17:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 002:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:17:58 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2024-01-02 14:17:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:  99% 164/165 [00:21<00:00,  7.55it/s, loss=2.803, nll_loss=2.224, ppl=4.67, wps=30694.3, ups=7.86, wpb=3904.9, bsz=396.4, num_updates=300, lr=0.0003, gnorm=1.573, clip=87, train_wall=12, gb_free=13.7, wall=45]2024-01-02 14:18:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 17.53it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 19.84it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 20.36it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 20.90it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 20.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  77% 17/22 [00:00<00:00, 21.63it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  91% 20/22 [00:00<00:00, 21.26it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:18:21 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 1.748 | nll_loss 0.764 | ppl 1.7 | wps 85550.1 | wpb 3732.4 | bsz 371.1 | num_updates 330 | best_loss 1.748\n",
            "2024-01-02 14:18:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 330 updates\n",
            "2024-01-02 14:18:21 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:18:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:18:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 2 @ 330 updates, score 1.748) (writing took 1.4329467379999983 seconds)\n",
            "2024-01-02 14:18:22 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2024-01-02 14:18:22 | INFO | train | epoch 002 | loss 2.878 | nll_loss 2.314 | ppl 4.97 | wps 27027.8 | ups 6.74 | wpb 4007.6 | bsz 395.4 | num_updates 330 | lr 0.00033 | gnorm 1.578 | clip 89.1 | train_wall 21 | gb_free 13.6 | wall 52\n",
            "2024-01-02 14:18:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 003:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:18:22 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2024-01-02 14:18:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:  99% 164/165 [00:21<00:00,  8.59it/s, loss=2.029, nll_loss=1.28, ppl=2.43, wps=25223.1, ups=6.21, wpb=4060.7, bsz=400, num_updates=400, lr=0.0004, gnorm=1.489, clip=85, train_wall=13, gb_free=13.8, wall=61]2024-01-02 14:18:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 13.43it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 14.91it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  27% 6/22 [00:00<00:01, 15.77it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 15.83it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  45% 10/22 [00:00<00:00, 15.98it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  55% 12/22 [00:00<00:00, 15.83it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 15.12it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  73% 16/22 [00:01<00:00, 16.07it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  82% 18/22 [00:01<00:00, 16.49it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 15.80it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:18:45 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 1.391 | nll_loss 0.32 | ppl 1.25 | wps 64518.6 | wpb 3732.4 | bsz 371.1 | num_updates 495 | best_loss 1.391\n",
            "2024-01-02 14:18:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 495 updates\n",
            "2024-01-02 14:18:45 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:18:46 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:18:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 3 @ 495 updates, score 1.391) (writing took 1.0979857309999943 seconds)\n",
            "2024-01-02 14:18:47 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2024-01-02 14:18:47 | INFO | train | epoch 003 | loss 1.776 | nll_loss 0.982 | ppl 1.98 | wps 27025.1 | ups 6.74 | wpb 4007.6 | bsz 395.4 | num_updates 495 | lr 0.000495 | gnorm 1.224 | clip 61.2 | train_wall 21 | gb_free 13.5 | wall 76\n",
            "2024-01-02 14:18:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 004:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:18:47 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2024-01-02 14:18:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004:  99% 164/165 [00:22<00:00,  9.54it/s, loss=1.472, nll_loss=0.637, ppl=1.55, wps=30308.3, ups=7.34, wpb=4130, bsz=396.1, num_updates=600, lr=0.0006, gnorm=0.882, clip=24, train_wall=13, gb_free=14, wall=91]2024-01-02 14:19:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 17.13it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 19.71it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 20.74it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 20.84it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 22.08it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  77% 17/22 [00:00<00:00, 21.29it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  91% 20/22 [00:00<00:00, 20.98it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:19:10 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 1.229 | nll_loss 0.21 | ppl 1.16 | wps 86271.1 | wpb 3732.4 | bsz 371.1 | num_updates 660 | best_loss 1.229\n",
            "2024-01-02 14:19:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 660 updates\n",
            "2024-01-02 14:19:10 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:19:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:19:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 4 @ 660 updates, score 1.229) (writing took 0.8923298830000022 seconds)\n",
            "2024-01-02 14:19:11 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2024-01-02 14:19:11 | INFO | train | epoch 004 | loss 1.436 | nll_loss 0.598 | ppl 1.51 | wps 27030 | ups 6.74 | wpb 4007.6 | bsz 395.4 | num_updates 660 | lr 0.00066 | gnorm 0.844 | clip 18.8 | train_wall 22 | gb_free 13.9 | wall 101\n",
            "2024-01-02 14:19:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 005:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:19:11 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2024-01-02 14:19:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:  99% 164/165 [00:21<00:00,  7.60it/s, loss=1.298, nll_loss=0.458, ppl=1.37, wps=30492.6, ups=7.59, wpb=4017.5, bsz=392.4, num_updates=800, lr=0.0008, gnorm=0.716, clip=11, train_wall=13, gb_free=13.8, wall=119]2024-01-02 14:19:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   5% 1/22 [00:00<00:02,  9.13it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  14% 3/22 [00:00<00:01, 15.02it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  23% 5/22 [00:00<00:01, 16.77it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  32% 7/22 [00:00<00:00, 16.50it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  41% 9/22 [00:00<00:00, 16.22it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 15.21it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 15.36it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  68% 15/22 [00:00<00:00, 15.06it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  77% 17/22 [00:01<00:00, 14.82it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  86% 19/22 [00:01<00:00, 14.49it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:19:34 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 1.155 | nll_loss 0.167 | ppl 1.12 | wps 63821.5 | wpb 3732.4 | bsz 371.1 | num_updates 825 | best_loss 1.155\n",
            "2024-01-02 14:19:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 825 updates\n",
            "2024-01-02 14:19:34 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:19:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:19:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 5 @ 825 updates, score 1.155) (writing took 0.9072186980000083 seconds)\n",
            "2024-01-02 14:19:35 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2024-01-02 14:19:35 | INFO | train | epoch 005 | loss 1.29 | nll_loss 0.447 | ppl 1.36 | wps 27124.8 | ups 6.77 | wpb 4007.6 | bsz 395.4 | num_updates 825 | lr 0.000825 | gnorm 0.687 | clip 9.1 | train_wall 21 | gb_free 13.5 | wall 125\n",
            "2024-01-02 14:19:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 006:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:19:35 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2024-01-02 14:19:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006:  99% 164/165 [00:21<00:00,  8.06it/s, loss=1.225, nll_loss=0.382, ppl=1.3, wps=25810.5, ups=6.53, wpb=3955, bsz=392.4, num_updates=900, lr=0.0009, gnorm=0.612, clip=9, train_wall=12, gb_free=13.7, wall=134]2024-01-02 14:19:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 16.82it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 20.03it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 20.49it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 20.84it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 20.63it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  77% 17/22 [00:00<00:00, 20.45it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  91% 20/22 [00:00<00:00, 20.49it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:19:58 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 1.133 | nll_loss 0.154 | ppl 1.11 | wps 83660.7 | wpb 3732.4 | bsz 371.1 | num_updates 990 | best_loss 1.133\n",
            "2024-01-02 14:19:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 990 updates\n",
            "2024-01-02 14:19:58 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:19:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:19:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 6 @ 990 updates, score 1.133) (writing took 0.9051158210000381 seconds)\n",
            "2024-01-02 14:19:59 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2024-01-02 14:19:59 | INFO | train | epoch 006 | loss 1.214 | nll_loss 0.371 | ppl 1.29 | wps 27549.4 | ups 6.87 | wpb 4007.6 | bsz 395.4 | num_updates 990 | lr 0.00099 | gnorm 0.572 | clip 6.1 | train_wall 21 | gb_free 13.8 | wall 149\n",
            "2024-01-02 14:19:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 007:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:19:59 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2024-01-02 14:19:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007:  99% 163/165 [00:22<00:00,  8.84it/s, loss=1.161, nll_loss=0.318, ppl=1.25, wps=30042.6, ups=7.37, wpb=4074.9, bsz=396.1, num_updates=1100, lr=0.000953463, gnorm=0.455, clip=4, train_wall=13, gb_free=13.5, wall=164]2024-01-02 14:20:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 16.71it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 19.29it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  32% 7/22 [00:00<00:01,  8.67it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  45% 10/22 [00:00<00:01, 11.81it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 14.33it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  68% 15/22 [00:01<00:00, 15.35it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  82% 18/22 [00:01<00:00, 17.59it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  95% 21/22 [00:01<00:00, 19.42it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:20:23 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 1.078 | nll_loss 0.138 | ppl 1.1 | wps 62037.3 | wpb 3732.4 | bsz 371.1 | num_updates 1155 | best_loss 1.078\n",
            "2024-01-02 14:20:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1155 updates\n",
            "2024-01-02 14:20:23 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:20:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:20:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 7 @ 1155 updates, score 1.078) (writing took 0.8921462489999499 seconds)\n",
            "2024-01-02 14:20:24 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2024-01-02 14:20:24 | INFO | train | epoch 007 | loss 1.144 | nll_loss 0.299 | ppl 1.23 | wps 26630.2 | ups 6.64 | wpb 4007.6 | bsz 395.4 | num_updates 1155 | lr 0.000930484 | gnorm 0.424 | clip 3 | train_wall 21 | gb_free 13.9 | wall 174\n",
            "2024-01-02 14:20:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 008:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:20:24 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2024-01-02 14:20:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008:  99% 164/165 [00:21<00:00,  8.65it/s, loss=1.112, nll_loss=0.268, ppl=1.2, wps=28674.5, ups=7.16, wpb=4003.3, bsz=392.4, num_updates=1300, lr=0.000877058, gnorm=0.449, clip=3, train_wall=13, gb_free=13.9, wall=193]2024-01-02 14:20:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 13.62it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 14.91it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  27% 6/22 [00:00<00:01, 14.72it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 15.84it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  45% 10/22 [00:00<00:00, 16.46it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 16.73it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  73% 16/22 [00:00<00:00, 18.35it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  82% 18/22 [00:01<00:00, 18.18it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 16.80it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:20:48 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 1.062 | nll_loss 0.117 | ppl 1.08 | wps 68159.9 | wpb 3732.4 | bsz 371.1 | num_updates 1320 | best_loss 1.062\n",
            "2024-01-02 14:20:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1320 updates\n",
            "2024-01-02 14:20:48 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:20:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:20:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 8 @ 1320 updates, score 1.062) (writing took 1.6568973000000256 seconds)\n",
            "2024-01-02 14:20:49 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2024-01-02 14:20:49 | INFO | train | epoch 008 | loss 1.111 | nll_loss 0.266 | ppl 1.2 | wps 26382.5 | ups 6.58 | wpb 4007.6 | bsz 395.4 | num_updates 1320 | lr 0.000870388 | gnorm 0.412 | clip 1.8 | train_wall 21 | gb_free 13.6 | wall 199\n",
            "2024-01-02 14:20:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 009:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:20:49 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2024-01-02 14:20:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 009:  99% 164/165 [00:23<00:00,  7.97it/s, loss=1.088, nll_loss=0.243, ppl=1.18, wps=22643.1, ups=5.67, wpb=3990.6, bsz=396.4, num_updates=1400, lr=0.000845154, gnorm=0.311, clip=1, train_wall=14, gb_free=13.6, wall=211]2024-01-02 14:21:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 17.04it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 19.22it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 21.24it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 21.42it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 21.83it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  77% 17/22 [00:00<00:00, 21.46it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  91% 20/22 [00:00<00:00, 20.84it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:21:14 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 1.057 | nll_loss 0.115 | ppl 1.08 | wps 85916 | wpb 3732.4 | bsz 371.1 | num_updates 1485 | best_loss 1.057\n",
            "2024-01-02 14:21:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1485 updates\n",
            "2024-01-02 14:21:14 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:21:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:21:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 9 @ 1485 updates, score 1.057) (writing took 0.8739221600000064 seconds)\n",
            "2024-01-02 14:21:15 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2024-01-02 14:21:15 | INFO | train | epoch 009 | loss 1.081 | nll_loss 0.236 | ppl 1.18 | wps 25868.2 | ups 6.45 | wpb 4007.6 | bsz 395.4 | num_updates 1485 | lr 0.00082061 | gnorm 0.311 | clip 1.2 | train_wall 22 | gb_free 13.8 | wall 224\n",
            "2024-01-02 14:21:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 010:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:21:15 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2024-01-02 14:21:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010:  99% 164/165 [00:22<00:00,  8.07it/s, loss=1.068, nll_loss=0.223, ppl=1.17, wps=29800.1, ups=7.4, wpb=4026.1, bsz=392.4, num_updates=1600, lr=0.000790569, gnorm=0.321, clip=2, train_wall=13, gb_free=13.5, wall=240]2024-01-02 14:21:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 14.41it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 15.45it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  27% 6/22 [00:00<00:00, 16.33it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 16.19it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  45% 10/22 [00:00<00:00, 16.02it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  55% 12/22 [00:00<00:00, 16.45it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  68% 15/22 [00:00<00:00, 17.64it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  82% 18/22 [00:01<00:00, 17.87it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 18.34it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:21:38 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 1.043 | nll_loss 0.108 | ppl 1.08 | wps 71114.4 | wpb 3732.4 | bsz 371.1 | num_updates 1650 | best_loss 1.043\n",
            "2024-01-02 14:21:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1650 updates\n",
            "2024-01-02 14:21:38 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:21:39 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:21:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 10 @ 1650 updates, score 1.043) (writing took 0.8752765980000277 seconds)\n",
            "2024-01-02 14:21:39 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2024-01-02 14:21:39 | INFO | train | epoch 010 | loss 1.067 | nll_loss 0.222 | ppl 1.17 | wps 27128.1 | ups 6.77 | wpb 4007.6 | bsz 395.4 | num_updates 1650 | lr 0.000778499 | gnorm 0.303 | clip 1.2 | train_wall 21 | gb_free 13.6 | wall 249\n",
            "2024-01-02 14:21:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 011:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:21:39 | INFO | fairseq.trainer | begin training epoch 11\n",
            "2024-01-02 14:21:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 011:  99% 164/165 [00:21<00:00,  7.86it/s, loss=1.058, nll_loss=0.213, ppl=1.16, wps=29128.5, ups=7.22, wpb=4035.1, bsz=396.1, num_updates=1800, lr=0.000745356, gnorm=0.284, clip=0, train_wall=13, gb_free=13.8, wall=269]2024-01-02 14:22:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 17.71it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 19.95it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 20.64it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 21.42it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 21.37it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  77% 17/22 [00:00<00:00, 21.34it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  91% 20/22 [00:00<00:00, 21.29it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:22:02 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 1.042 | nll_loss 0.112 | ppl 1.08 | wps 86207.3 | wpb 3732.4 | bsz 371.1 | num_updates 1815 | best_loss 1.042\n",
            "2024-01-02 14:22:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1815 updates\n",
            "2024-01-02 14:22:02 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:22:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:22:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 11 @ 1815 updates, score 1.042) (writing took 1.0926007560000244 seconds)\n",
            "2024-01-02 14:22:03 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2024-01-02 14:22:03 | INFO | train | epoch 011 | loss 1.061 | nll_loss 0.217 | ppl 1.16 | wps 27413 | ups 6.84 | wpb 4007.6 | bsz 395.4 | num_updates 1815 | lr 0.00074227 | gnorm 0.284 | clip 0.6 | train_wall 21 | gb_free 13.9 | wall 273\n",
            "2024-01-02 14:22:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 012:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:22:03 | INFO | fairseq.trainer | begin training epoch 12\n",
            "2024-01-02 14:22:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 012:  99% 164/165 [00:22<00:00,  8.21it/s, loss=1.062, nll_loss=0.218, ppl=1.16, wps=24843.5, ups=6.3, wpb=3941.1, bsz=396.1, num_updates=1900, lr=0.000725476, gnorm=0.292, clip=0, train_wall=13, gb_free=14, wall=285]2024-01-02 14:22:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 16.67it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 17.96it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  32% 7/22 [00:00<00:00, 20.12it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  41% 9/22 [00:00<00:00, 20.07it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  55% 12/22 [00:00<00:00, 20.32it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  68% 15/22 [00:00<00:00, 19.47it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  82% 18/22 [00:00<00:00, 20.06it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  95% 21/22 [00:01<00:00, 21.38it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:22:27 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 1.045 | nll_loss 0.102 | ppl 1.07 | wps 81394.8 | wpb 3732.4 | bsz 371.1 | num_updates 1980 | best_loss 1.042\n",
            "2024-01-02 14:22:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1980 updates\n",
            "2024-01-02 14:22:27 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:22:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:22:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 12 @ 1980 updates, score 1.045) (writing took 0.4081404670000097 seconds)\n",
            "2024-01-02 14:22:28 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2024-01-02 14:22:28 | INFO | train | epoch 012 | loss 1.055 | nll_loss 0.21 | ppl 1.16 | wps 27196.6 | ups 6.79 | wpb 4007.6 | bsz 395.4 | num_updates 1980 | lr 0.000710669 | gnorm 0.273 | clip 0.6 | train_wall 22 | gb_free 13.2 | wall 297\n",
            "2024-01-02 14:22:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 013:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:22:28 | INFO | fairseq.trainer | begin training epoch 13\n",
            "2024-01-02 14:22:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 013:  99% 164/165 [00:21<00:00,  8.81it/s, loss=1.045, nll_loss=0.2, ppl=1.15, wps=29973.8, ups=7.3, wpb=4105.4, bsz=396.1, num_updates=2100, lr=0.000690066, gnorm=0.231, clip=0, train_wall=13, gb_free=13.8, wall=314]2024-01-02 14:22:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 013 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 17.73it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 18.95it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 18.70it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  45% 10/22 [00:00<00:00, 17.69it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  55% 12/22 [00:00<00:00, 18.05it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  68% 15/22 [00:00<00:00, 18.10it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  77% 17/22 [00:00<00:00, 17.36it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  86% 19/22 [00:01<00:00, 16.69it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  95% 21/22 [00:01<00:00, 17.48it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:22:51 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 1.047 | nll_loss 0.109 | ppl 1.08 | wps 69739.5 | wpb 3732.4 | bsz 371.1 | num_updates 2145 | best_loss 1.042\n",
            "2024-01-02 14:22:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2145 updates\n",
            "2024-01-02 14:22:51 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:22:51 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:22:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 13 @ 2145 updates, score 1.047) (writing took 0.49223523899996735 seconds)\n",
            "2024-01-02 14:22:51 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
            "2024-01-02 14:22:51 | INFO | train | epoch 013 | loss 1.044 | nll_loss 0.2 | ppl 1.15 | wps 28005.2 | ups 6.99 | wpb 4007.6 | bsz 395.4 | num_updates 2145 | lr 0.000682789 | gnorm 0.236 | clip 0 | train_wall 21 | gb_free 14.5 | wall 321\n",
            "2024-01-02 14:22:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 014:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:22:51 | INFO | fairseq.trainer | begin training epoch 14\n",
            "2024-01-02 14:22:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 014:  99% 164/165 [00:22<00:00,  7.67it/s, loss=1.044, nll_loss=0.2, ppl=1.15, wps=29583.8, ups=7.48, wpb=3956, bsz=392.4, num_updates=2300, lr=0.00065938, gnorm=0.256, clip=1, train_wall=13, gb_free=13.6, wall=342]2024-01-02 14:23:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 014 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 16.74it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 19.48it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 20.78it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 21.19it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 21.45it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  77% 17/22 [00:00<00:00, 21.83it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  91% 20/22 [00:00<00:00, 21.24it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:23:15 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 1.038 | nll_loss 0.104 | ppl 1.07 | wps 86048 | wpb 3732.4 | bsz 371.1 | num_updates 2310 | best_loss 1.038\n",
            "2024-01-02 14:23:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2310 updates\n",
            "2024-01-02 14:23:15 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:23:15 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:23:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 14 @ 2310 updates, score 1.038) (writing took 1.0973631349999664 seconds)\n",
            "2024-01-02 14:23:16 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
            "2024-01-02 14:23:16 | INFO | train | epoch 014 | loss 1.043 | nll_loss 0.199 | ppl 1.15 | wps 26956.4 | ups 6.73 | wpb 4007.6 | bsz 395.4 | num_updates 2310 | lr 0.000657952 | gnorm 0.246 | clip 0.6 | train_wall 21 | gb_free 13.7 | wall 345\n",
            "2024-01-02 14:23:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 015:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:23:16 | INFO | fairseq.trainer | begin training epoch 15\n",
            "2024-01-02 14:23:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 015:  99% 164/165 [00:26<00:00,  4.97it/s, loss=1.039, nll_loss=0.194, ppl=1.14, wps=22571.8, ups=5.61, wpb=4026.7, bsz=396.1, num_updates=2400, lr=0.000645497, gnorm=0.227, clip=0, train_wall=15, gb_free=13.7, wall=360]2024-01-02 14:23:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 015 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:   5% 1/22 [00:00<00:03,  5.61it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  14% 3/22 [00:00<00:01, 11.27it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  23% 5/22 [00:00<00:01, 10.12it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  32% 7/22 [00:00<00:01, 10.46it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  41% 9/22 [00:00<00:01, 11.40it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  50% 11/22 [00:01<00:01, 10.16it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  59% 13/22 [00:01<00:00, 10.78it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  68% 15/22 [00:01<00:00, 11.87it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  82% 18/22 [00:01<00:00, 14.72it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  95% 21/22 [00:01<00:00, 17.31it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:23:44 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 1.043 | nll_loss 0.106 | ppl 1.08 | wps 54296.5 | wpb 3732.4 | bsz 371.1 | num_updates 2475 | best_loss 1.038\n",
            "2024-01-02 14:23:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2475 updates\n",
            "2024-01-02 14:23:44 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:23:45 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:23:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 15 @ 2475 updates, score 1.043) (writing took 0.41891917499992815 seconds)\n",
            "2024-01-02 14:23:45 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
            "2024-01-02 14:23:45 | INFO | train | epoch 015 | loss 1.037 | nll_loss 0.192 | ppl 1.14 | wps 22746.5 | ups 5.68 | wpb 4007.6 | bsz 395.4 | num_updates 2475 | lr 0.000635642 | gnorm 0.222 | clip 0 | train_wall 26 | gb_free 13.9 | wall 374\n",
            "2024-01-02 14:23:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 016:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:23:45 | INFO | fairseq.trainer | begin training epoch 16\n",
            "2024-01-02 14:23:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 016:  99% 164/165 [00:21<00:00,  7.02it/s, loss=1.036, nll_loss=0.191, ppl=1.14, wps=29517.7, ups=7.4, wpb=3988.9, bsz=396.1, num_updates=2600, lr=0.000620174, gnorm=0.203, clip=0, train_wall=13, gb_free=13.4, wall=391]2024-01-02 14:24:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 016 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 11.64it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 14.25it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  27% 6/22 [00:00<00:01, 12.03it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  36% 8/22 [00:00<00:01, 13.22it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  45% 10/22 [00:00<00:00, 12.38it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  55% 12/22 [00:00<00:00, 13.21it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  64% 14/22 [00:01<00:00, 12.95it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  73% 16/22 [00:01<00:00, 13.15it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  82% 18/22 [00:01<00:00, 14.05it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 13.85it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:24:09 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 1.04 | nll_loss 0.105 | ppl 1.08 | wps 54293.6 | wpb 3732.4 | bsz 371.1 | num_updates 2640 | best_loss 1.038\n",
            "2024-01-02 14:24:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 2640 updates\n",
            "2024-01-02 14:24:09 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:24:10 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:24:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 16 @ 2640 updates, score 1.04) (writing took 1.1339028999999528 seconds)\n",
            "2024-01-02 14:24:10 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
            "2024-01-02 14:24:10 | INFO | train | epoch 016 | loss 1.035 | nll_loss 0.19 | ppl 1.14 | wps 26628 | ups 6.64 | wpb 4007.6 | bsz 395.4 | num_updates 2640 | lr 0.000615457 | gnorm 0.211 | clip 0 | train_wall 21 | gb_free 13.6 | wall 399\n",
            "2024-01-02 14:24:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 017:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:24:10 | INFO | fairseq.trainer | begin training epoch 17\n",
            "2024-01-02 14:24:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 017:  99% 164/165 [00:28<00:00,  5.58it/s, loss=1.034, nll_loss=0.19, ppl=1.14, wps=21495.7, ups=5.34, wpb=4023.5, bsz=396.4, num_updates=2800, lr=0.000597614, gnorm=0.207, clip=0, train_wall=18, gb_free=13.9, wall=427]2024-01-02 14:24:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 017 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:   5% 1/22 [00:00<00:04,  5.14it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:   9% 2/22 [00:00<00:02,  6.79it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  14% 3/22 [00:00<00:03,  6.11it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  18% 4/22 [00:00<00:02,  6.89it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  23% 5/22 [00:00<00:02,  6.73it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  32% 7/22 [00:00<00:01,  9.04it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  41% 9/22 [00:01<00:01, 10.47it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  50% 11/22 [00:01<00:01, 10.42it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  59% 13/22 [00:01<00:00, 10.23it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  68% 15/22 [00:01<00:00, 10.43it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  77% 17/22 [00:01<00:00, 11.69it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  86% 19/22 [00:01<00:00, 13.12it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  95% 21/22 [00:01<00:00, 14.44it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:24:40 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 1.042 | nll_loss 0.105 | ppl 1.08 | wps 43821.5 | wpb 3732.4 | bsz 371.1 | num_updates 2805 | best_loss 1.038\n",
            "2024-01-02 14:24:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 2805 updates\n",
            "2024-01-02 14:24:40 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:24:41 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:24:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 17 @ 2805 updates, score 1.042) (writing took 1.1025273980000065 seconds)\n",
            "2024-01-02 14:24:41 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
            "2024-01-02 14:24:41 | INFO | train | epoch 017 | loss 1.033 | nll_loss 0.188 | ppl 1.14 | wps 20843.2 | ups 5.2 | wpb 4007.6 | bsz 395.4 | num_updates 2805 | lr 0.000597081 | gnorm 0.212 | clip 0 | train_wall 27 | gb_free 13.7 | wall 431\n",
            "2024-01-02 14:24:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 018:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:24:42 | INFO | fairseq.trainer | begin training epoch 18\n",
            "2024-01-02 14:24:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 018:  99% 164/165 [00:24<00:00,  6.69it/s, loss=1.03, nll_loss=0.186, ppl=1.14, wps=20663.7, ups=5.04, wpb=4097.9, bsz=396.4, num_updates=2900, lr=0.00058722, gnorm=0.199, clip=0, train_wall=16, gb_free=13.2, wall=447]2024-01-02 14:25:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 018 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:   5% 1/22 [00:00<00:02,  9.05it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:   9% 2/22 [00:00<00:02,  7.59it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  14% 3/22 [00:00<00:02,  8.31it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  23% 5/22 [00:00<00:01, 10.91it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  32% 7/22 [00:00<00:01, 13.45it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  41% 9/22 [00:00<00:00, 14.36it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 14.51it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  59% 13/22 [00:01<00:00, 13.98it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  68% 15/22 [00:01<00:00, 14.16it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  77% 17/22 [00:01<00:00, 14.99it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  86% 19/22 [00:01<00:00, 13.91it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  95% 21/22 [00:01<00:00, 14.59it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:25:08 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 1.043 | nll_loss 0.109 | ppl 1.08 | wps 53600.4 | wpb 3732.4 | bsz 371.1 | num_updates 2970 | best_loss 1.038\n",
            "2024-01-02 14:25:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 2970 updates\n",
            "2024-01-02 14:25:08 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:25:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:25:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 18 @ 2970 updates, score 1.043) (writing took 0.7287024510000037 seconds)\n",
            "2024-01-02 14:25:09 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
            "2024-01-02 14:25:09 | INFO | train | epoch 018 | loss 1.029 | nll_loss 0.184 | ppl 1.14 | wps 24108.1 | ups 6.02 | wpb 4007.6 | bsz 395.4 | num_updates 2970 | lr 0.000580259 | gnorm 0.208 | clip 0.6 | train_wall 24 | gb_free 14 | wall 458\n",
            "2024-01-02 14:25:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 019:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:25:09 | INFO | fairseq.trainer | begin training epoch 19\n",
            "2024-01-02 14:25:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 019:  99% 164/165 [00:23<00:00,  7.52it/s, loss=1.028, nll_loss=0.183, ppl=1.14, wps=27245.9, ups=6.95, wpb=3918.2, bsz=392.4, num_updates=3100, lr=0.000567962, gnorm=0.192, clip=0, train_wall=14, gb_free=13.9, wall=477]2024-01-02 14:25:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 019 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:   5% 1/22 [00:00<00:04,  4.30it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  14% 3/22 [00:00<00:02,  7.30it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  18% 4/22 [00:00<00:02,  7.13it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  27% 6/22 [00:00<00:01,  8.55it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  36% 8/22 [00:00<00:01,  9.04it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  45% 10/22 [00:01<00:01, 10.43it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  59% 13/22 [00:01<00:00, 13.19it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  68% 15/22 [00:01<00:00, 14.00it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  82% 18/22 [00:01<00:00, 16.05it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 16.43it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:25:34 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 1.036 | nll_loss 0.1 | ppl 1.07 | wps 53603.9 | wpb 3732.4 | bsz 371.1 | num_updates 3135 | best_loss 1.036\n",
            "2024-01-02 14:25:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3135 updates\n",
            "2024-01-02 14:25:34 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:25:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:25:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 19 @ 3135 updates, score 1.036) (writing took 1.1500826679999818 seconds)\n",
            "2024-01-02 14:25:36 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
            "2024-01-02 14:25:36 | INFO | train | epoch 019 | loss 1.028 | nll_loss 0.183 | ppl 1.14 | wps 24738.9 | ups 6.17 | wpb 4007.6 | bsz 395.4 | num_updates 3135 | lr 0.000564782 | gnorm 0.194 | clip 0 | train_wall 23 | gb_free 13.7 | wall 485\n",
            "2024-01-02 14:25:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 020:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:25:36 | INFO | fairseq.trainer | begin training epoch 20\n",
            "2024-01-02 14:25:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 020:  99% 164/165 [00:23<00:00,  7.96it/s, loss=1.026, nll_loss=0.181, ppl=1.13, wps=22834.5, ups=5.85, wpb=3903.7, bsz=396.1, num_updates=3200, lr=0.000559017, gnorm=0.186, clip=0, train_wall=13, gb_free=13.7, wall=494]2024-01-02 14:25:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 020 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 15.65it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  23% 5/22 [00:00<00:01, 15.40it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 17.47it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  45% 10/22 [00:00<00:00, 17.22it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  55% 12/22 [00:00<00:00, 16.71it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 17.17it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  73% 16/22 [00:00<00:00, 17.77it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  82% 18/22 [00:01<00:00, 17.75it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 17.07it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:26:01 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 1.039 | nll_loss 0.104 | ppl 1.07 | wps 69483.6 | wpb 3732.4 | bsz 371.1 | num_updates 3300 | best_loss 1.036\n",
            "2024-01-02 14:26:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3300 updates\n",
            "2024-01-02 14:26:01 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:26:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:26:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 20 @ 3300 updates, score 1.039) (writing took 0.6545695979999664 seconds)\n",
            "2024-01-02 14:26:01 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
            "2024-01-02 14:26:01 | INFO | train | epoch 020 | loss 1.025 | nll_loss 0.18 | ppl 1.13 | wps 25653.3 | ups 6.4 | wpb 4007.6 | bsz 395.4 | num_updates 3300 | lr 0.000550482 | gnorm 0.182 | clip 0 | train_wall 23 | gb_free 13.9 | wall 511\n",
            "2024-01-02 14:26:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 021:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:26:02 | INFO | fairseq.trainer | begin training epoch 21\n",
            "2024-01-02 14:26:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 021:  99% 164/165 [00:23<00:00,  7.94it/s, loss=1.028, nll_loss=0.183, ppl=1.14, wps=24305.9, ups=6.08, wpb=3997.8, bsz=396.4, num_updates=3400, lr=0.000542326, gnorm=0.222, clip=0, train_wall=14, gb_free=13.8, wall=525]2024-01-02 14:26:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 021 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 17.15it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 19.32it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 19.91it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 20.18it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 19.85it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  73% 16/22 [00:00<00:00, 19.64it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  86% 19/22 [00:00<00:00, 20.29it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:26:26 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 1.034 | nll_loss 0.096 | ppl 1.07 | wps 81192.4 | wpb 3732.4 | bsz 371.1 | num_updates 3465 | best_loss 1.034\n",
            "2024-01-02 14:26:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 3465 updates\n",
            "2024-01-02 14:26:26 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:26:26 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:26:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 21 @ 3465 updates, score 1.034) (writing took 0.9403035179999506 seconds)\n",
            "2024-01-02 14:26:27 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
            "2024-01-02 14:26:27 | INFO | train | epoch 021 | loss 1.026 | nll_loss 0.181 | ppl 1.13 | wps 25846.9 | ups 6.45 | wpb 4007.6 | bsz 395.4 | num_updates 3465 | lr 0.000537215 | gnorm 0.228 | clip 0.6 | train_wall 22 | gb_free 13.8 | wall 536\n",
            "2024-01-02 14:26:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 022:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:26:27 | INFO | fairseq.trainer | begin training epoch 22\n",
            "2024-01-02 14:26:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 022:  99% 164/165 [00:21<00:00,  7.37it/s, loss=1.023, nll_loss=0.178, ppl=1.13, wps=29374, ups=7.42, wpb=3960.5, bsz=400, num_updates=3600, lr=0.000527046, gnorm=0.171, clip=0, train_wall=13, gb_free=13.5, wall=555]2024-01-02 14:26:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 022 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 14.94it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 15.00it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  27% 6/22 [00:00<00:01, 15.19it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 15.76it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  45% 10/22 [00:00<00:00, 15.89it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  55% 12/22 [00:00<00:00, 15.93it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 14.77it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  73% 16/22 [00:01<00:00, 14.98it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  82% 18/22 [00:01<00:00, 15.53it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 15.62it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:26:51 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 1.032 | nll_loss 0.093 | ppl 1.07 | wps 62801.9 | wpb 3732.4 | bsz 371.1 | num_updates 3630 | best_loss 1.032\n",
            "2024-01-02 14:26:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 3630 updates\n",
            "2024-01-02 14:26:51 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:26:51 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:26:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 22 @ 3630 updates, score 1.032) (writing took 1.0820414219999748 seconds)\n",
            "2024-01-02 14:26:52 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
            "2024-01-02 14:26:52 | INFO | train | epoch 022 | loss 1.026 | nll_loss 0.181 | ppl 1.13 | wps 26818.7 | ups 6.69 | wpb 4007.6 | bsz 395.4 | num_updates 3630 | lr 0.000524864 | gnorm 0.185 | clip 0 | train_wall 21 | gb_free 13.4 | wall 561\n",
            "2024-01-02 14:26:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 023:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:26:52 | INFO | fairseq.trainer | begin training epoch 23\n",
            "2024-01-02 14:26:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 023:  99% 164/165 [00:22<00:00,  8.17it/s, loss=1.024, nll_loss=0.179, ppl=1.13, wps=25867.6, ups=6.5, wpb=3978.7, bsz=388.5, num_updates=3700, lr=0.000519875, gnorm=0.179, clip=0, train_wall=12, gb_free=13, wall=570]2024-01-02 14:27:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 023 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 17.44it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 19.49it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 20.40it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 20.65it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 20.51it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  77% 17/22 [00:00<00:00, 21.36it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  91% 20/22 [00:00<00:00, 21.29it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:27:15 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 1.032 | nll_loss 0.094 | ppl 1.07 | wps 84827.5 | wpb 3732.4 | bsz 371.1 | num_updates 3795 | best_loss 1.032\n",
            "2024-01-02 14:27:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 3795 updates\n",
            "2024-01-02 14:27:15 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:27:15 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:27:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 23 @ 3795 updates, score 1.032) (writing took 0.9387675090000585 seconds)\n",
            "2024-01-02 14:27:16 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
            "2024-01-02 14:27:16 | INFO | train | epoch 023 | loss 1.022 | nll_loss 0.177 | ppl 1.13 | wps 27133.8 | ups 6.77 | wpb 4007.6 | bsz 395.4 | num_updates 3795 | lr 0.000513327 | gnorm 0.172 | clip 0 | train_wall 22 | gb_free 13.5 | wall 586\n",
            "2024-01-02 14:27:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 024:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:27:16 | INFO | fairseq.trainer | begin training epoch 24\n",
            "2024-01-02 14:27:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 024:  99% 164/165 [00:22<00:00,  7.98it/s, loss=1.02, nll_loss=0.175, ppl=1.13, wps=29825, ups=7.29, wpb=4091.6, bsz=396.1, num_updates=3900, lr=0.00050637, gnorm=0.175, clip=1, train_wall=13, gb_free=13.7, wall=600]2024-01-02 14:27:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 024 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:   5% 1/22 [00:00<00:02,  8.20it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 15.93it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  32% 7/22 [00:00<00:00, 19.65it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  41% 9/22 [00:00<00:00, 19.57it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 19.67it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 19.65it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  73% 16/22 [00:00<00:00, 19.37it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  86% 19/22 [00:00<00:00, 20.59it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:27:40 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 1.03 | nll_loss 0.092 | ppl 1.07 | wps 83366.8 | wpb 3732.4 | bsz 371.1 | num_updates 3960 | best_loss 1.03\n",
            "2024-01-02 14:27:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 3960 updates\n",
            "2024-01-02 14:27:40 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:27:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:27:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 24 @ 3960 updates, score 1.03) (writing took 0.8850790370000823 seconds)\n",
            "2024-01-02 14:27:40 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
            "2024-01-02 14:27:40 | INFO | train | epoch 024 | loss 1.023 | nll_loss 0.178 | ppl 1.13 | wps 27040.9 | ups 6.75 | wpb 4007.6 | bsz 395.4 | num_updates 3960 | lr 0.000502519 | gnorm 0.19 | clip 1.8 | train_wall 21 | gb_free 13.6 | wall 610\n",
            "2024-01-02 14:27:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 025:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:27:40 | INFO | fairseq.trainer | begin training epoch 25\n",
            "2024-01-02 14:27:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 025:  99% 164/165 [00:21<00:00,  9.04it/s, loss=1.02, nll_loss=0.175, ppl=1.13, wps=29430.2, ups=7.21, wpb=4079.8, bsz=396.4, num_updates=4100, lr=0.000493865, gnorm=0.185, clip=1, train_wall=13, gb_free=13.9, wall=629]2024-01-02 14:28:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 025 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 17.65it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  18% 4/22 [00:00<00:00, 18.22it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  32% 7/22 [00:00<00:00, 20.71it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  45% 10/22 [00:00<00:00, 20.96it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 20.61it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  73% 16/22 [00:00<00:00, 20.76it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  86% 19/22 [00:00<00:00, 21.35it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:28:04 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 1.032 | nll_loss 0.093 | ppl 1.07 | wps 84304.5 | wpb 3732.4 | bsz 371.1 | num_updates 4125 | best_loss 1.03\n",
            "2024-01-02 14:28:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 4125 updates\n",
            "2024-01-02 14:28:04 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:28:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:28:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 25 @ 4125 updates, score 1.032) (writing took 0.4371011959999578 seconds)\n",
            "2024-01-02 14:28:04 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
            "2024-01-02 14:28:04 | INFO | train | epoch 025 | loss 1.021 | nll_loss 0.176 | ppl 1.13 | wps 28093.6 | ups 7.01 | wpb 4007.6 | bsz 395.4 | num_updates 4125 | lr 0.000492366 | gnorm 0.178 | clip 0.6 | train_wall 21 | gb_free 13.7 | wall 634\n",
            "2024-01-02 14:28:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 026:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:28:04 | INFO | fairseq.trainer | begin training epoch 26\n",
            "2024-01-02 14:28:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 026:  99% 164/165 [00:22<00:00,  7.89it/s, loss=1.018, nll_loss=0.172, ppl=1.13, wps=26118.8, ups=6.46, wpb=4042, bsz=392.4, num_updates=4200, lr=0.00048795, gnorm=0.158, clip=0, train_wall=13, gb_free=13.5, wall=644]2024-01-02 14:28:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 026 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 17.53it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 18.85it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 20.70it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 21.08it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 21.22it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  77% 17/22 [00:00<00:00, 21.39it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  91% 20/22 [00:00<00:00, 20.20it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:28:28 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 1.031 | nll_loss 0.091 | ppl 1.06 | wps 83667.9 | wpb 3732.4 | bsz 371.1 | num_updates 4290 | best_loss 1.03\n",
            "2024-01-02 14:28:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 4290 updates\n",
            "2024-01-02 14:28:28 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:28:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:28:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 26 @ 4290 updates, score 1.031) (writing took 0.4058445189999702 seconds)\n",
            "2024-01-02 14:28:28 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n",
            "2024-01-02 14:28:28 | INFO | train | epoch 026 | loss 1.02 | nll_loss 0.175 | ppl 1.13 | wps 27345.8 | ups 6.82 | wpb 4007.6 | bsz 395.4 | num_updates 4290 | lr 0.000482805 | gnorm 0.183 | clip 0 | train_wall 22 | gb_free 13.9 | wall 658\n",
            "2024-01-02 14:28:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 027:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:28:28 | INFO | fairseq.trainer | begin training epoch 27\n",
            "2024-01-02 14:28:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 027:  99% 164/165 [00:21<00:00,  7.67it/s, loss=1.017, nll_loss=0.172, ppl=1.13, wps=30037.7, ups=7.44, wpb=4037.4, bsz=396.4, num_updates=4400, lr=0.000476731, gnorm=0.165, clip=0, train_wall=13, gb_free=13.7, wall=673]2024-01-02 14:28:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 027 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 16.13it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 19.73it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 20.66it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 16.91it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 12.84it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  68% 15/22 [00:01<00:00, 12.77it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  77% 17/22 [00:01<00:00, 14.27it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 16.35it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:28:51 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 1.03 | nll_loss 0.094 | ppl 1.07 | wps 64419 | wpb 3732.4 | bsz 371.1 | num_updates 4455 | best_loss 1.03\n",
            "2024-01-02 14:28:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 4455 updates\n",
            "2024-01-02 14:28:51 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:28:52 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:28:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 27 @ 4455 updates, score 1.03) (writing took 1.1304423459999953 seconds)\n",
            "2024-01-02 14:28:52 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n",
            "2024-01-02 14:28:52 | INFO | train | epoch 027 | loss 1.018 | nll_loss 0.172 | ppl 1.13 | wps 27319.9 | ups 6.82 | wpb 4007.6 | bsz 395.4 | num_updates 4455 | lr 0.000473779 | gnorm 0.167 | clip 0.6 | train_wall 21 | gb_free 13.8 | wall 682\n",
            "2024-01-02 14:28:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 028:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:28:52 | INFO | fairseq.trainer | begin training epoch 28\n",
            "2024-01-02 14:28:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 028:  99% 164/165 [00:22<00:00,  7.70it/s, loss=1.016, nll_loss=0.17, ppl=1.13, wps=30082.7, ups=7.37, wpb=4083.4, bsz=396.1, num_updates=4600, lr=0.000466252, gnorm=0.152, clip=0, train_wall=13, gb_free=13.8, wall=702]2024-01-02 14:29:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 028 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 17.23it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  18% 4/22 [00:00<00:00, 18.53it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  32% 7/22 [00:00<00:00, 21.05it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  45% 10/22 [00:00<00:00, 21.94it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 21.27it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  73% 16/22 [00:00<00:00, 20.50it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  86% 19/22 [00:00<00:00, 20.81it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:29:16 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 1.034 | nll_loss 0.096 | ppl 1.07 | wps 84721.1 | wpb 3732.4 | bsz 371.1 | num_updates 4620 | best_loss 1.03\n",
            "2024-01-02 14:29:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 4620 updates\n",
            "2024-01-02 14:29:16 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:29:16 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:29:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 28 @ 4620 updates, score 1.034) (writing took 0.44725890600000184 seconds)\n",
            "2024-01-02 14:29:16 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n",
            "2024-01-02 14:29:16 | INFO | train | epoch 028 | loss 1.017 | nll_loss 0.172 | ppl 1.13 | wps 27613.4 | ups 6.89 | wpb 4007.6 | bsz 395.4 | num_updates 4620 | lr 0.000465242 | gnorm 0.166 | clip 0.6 | train_wall 21 | gb_free 13.5 | wall 706\n",
            "2024-01-02 14:29:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 029:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:29:16 | INFO | fairseq.trainer | begin training epoch 29\n",
            "2024-01-02 14:29:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 029:  99% 164/165 [00:21<00:00,  7.42it/s, loss=1.017, nll_loss=0.171, ppl=1.13, wps=26499, ups=6.65, wpb=3984.1, bsz=400, num_updates=4700, lr=0.000461266, gnorm=0.155, clip=0, train_wall=13, gb_free=13.7, wall=717]2024-01-02 14:29:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 029 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 14.61it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 15.09it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  27% 6/22 [00:00<00:00, 16.42it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 16.00it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  45% 10/22 [00:00<00:00, 15.85it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 16.96it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  68% 15/22 [00:00<00:00, 15.65it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  77% 17/22 [00:01<00:00, 15.97it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  86% 19/22 [00:01<00:00, 16.90it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset: 100% 22/22 [00:01<00:00, 19.80it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:29:39 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 1.031 | nll_loss 0.094 | ppl 1.07 | wps 66277.2 | wpb 3732.4 | bsz 371.1 | num_updates 4785 | best_loss 1.03\n",
            "2024-01-02 14:29:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 4785 updates\n",
            "2024-01-02 14:29:39 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:29:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:29:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 29 @ 4785 updates, score 1.031) (writing took 0.5283156760000338 seconds)\n",
            "2024-01-02 14:29:40 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n",
            "2024-01-02 14:29:40 | INFO | train | epoch 029 | loss 1.017 | nll_loss 0.171 | ppl 1.13 | wps 27893.7 | ups 6.96 | wpb 4007.6 | bsz 395.4 | num_updates 4785 | lr 0.00045715 | gnorm 0.16 | clip 0 | train_wall 21 | gb_free 13.9 | wall 730\n",
            "2024-01-02 14:29:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 030:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:29:40 | INFO | fairseq.trainer | begin training epoch 30\n",
            "2024-01-02 14:29:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 030:  99% 164/165 [00:22<00:00,  8.10it/s, loss=1.016, nll_loss=0.171, ppl=1.13, wps=29733, ups=7.44, wpb=3995.9, bsz=396.1, num_updates=4900, lr=0.000451754, gnorm=0.15, clip=0, train_wall=13, gb_free=13.8, wall=746]2024-01-02 14:30:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 030 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 17.09it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  18% 4/22 [00:00<00:00, 18.67it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  32% 7/22 [00:00<00:00, 20.18it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  45% 10/22 [00:00<00:00, 20.52it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 20.08it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  73% 16/22 [00:00<00:00, 20.27it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  86% 19/22 [00:00<00:00, 20.92it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:30:04 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 1.029 | nll_loss 0.092 | ppl 1.07 | wps 82293 | wpb 3732.4 | bsz 371.1 | num_updates 4950 | best_loss 1.029\n",
            "2024-01-02 14:30:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 4950 updates\n",
            "2024-01-02 14:30:04 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:30:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:30:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 30 @ 4950 updates, score 1.029) (writing took 0.9468737880000617 seconds)\n",
            "2024-01-02 14:30:05 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n",
            "2024-01-02 14:30:05 | INFO | train | epoch 030 | loss 1.016 | nll_loss 0.17 | ppl 1.13 | wps 26757.6 | ups 6.68 | wpb 4007.6 | bsz 395.4 | num_updates 4950 | lr 0.000449467 | gnorm 0.145 | clip 0 | train_wall 22 | gb_free 13.6 | wall 754\n",
            "2024-01-02 14:30:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 031:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:30:05 | INFO | fairseq.trainer | begin training epoch 31\n",
            "2024-01-02 14:30:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 031:  99% 164/165 [00:21<00:00,  7.73it/s, loss=1.015, nll_loss=0.169, ppl=1.12, wps=29734.5, ups=7.5, wpb=3962.6, bsz=396.1, num_updates=5100, lr=0.000442807, gnorm=0.146, clip=0, train_wall=13, gb_free=13.9, wall=774]2024-01-02 14:30:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 031 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 12.74it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 13.34it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  27% 6/22 [00:00<00:01, 14.30it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  41% 9/22 [00:00<00:00, 15.54it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 14.98it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 15.42it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  68% 15/22 [00:00<00:00, 16.54it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  77% 17/22 [00:01<00:00, 16.40it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  86% 19/22 [00:01<00:00, 16.19it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset: 100% 22/22 [00:01<00:00, 18.88it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:30:28 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 1.033 | nll_loss 0.096 | ppl 1.07 | wps 63375.5 | wpb 3732.4 | bsz 371.1 | num_updates 5115 | best_loss 1.029\n",
            "2024-01-02 14:30:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 5115 updates\n",
            "2024-01-02 14:30:28 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:30:29 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:30:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 31 @ 5115 updates, score 1.033) (writing took 0.5209634320000305 seconds)\n",
            "2024-01-02 14:30:29 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n",
            "2024-01-02 14:30:29 | INFO | train | epoch 031 | loss 1.015 | nll_loss 0.17 | ppl 1.12 | wps 27647.9 | ups 6.9 | wpb 4007.6 | bsz 395.4 | num_updates 5115 | lr 0.000442158 | gnorm 0.145 | clip 0 | train_wall 21 | gb_free 13.6 | wall 778\n",
            "2024-01-02 14:30:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 032:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:30:29 | INFO | fairseq.trainer | begin training epoch 32\n",
            "2024-01-02 14:30:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 032:  99% 164/165 [00:22<00:00,  7.76it/s, loss=1.016, nll_loss=0.171, ppl=1.13, wps=25726.2, ups=6.51, wpb=3951.1, bsz=396.4, num_updates=5200, lr=0.000438529, gnorm=0.151, clip=0, train_wall=13, gb_free=13.8, wall=790]2024-01-02 14:30:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 032 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 16.99it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 19.28it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 20.94it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 20.03it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 21.13it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  77% 17/22 [00:00<00:00, 21.01it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  91% 20/22 [00:00<00:00, 21.04it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:30:52 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 1.03 | nll_loss 0.094 | ppl 1.07 | wps 85069 | wpb 3732.4 | bsz 371.1 | num_updates 5280 | best_loss 1.029\n",
            "2024-01-02 14:30:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 5280 updates\n",
            "2024-01-02 14:30:52 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:30:52 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:30:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 32 @ 5280 updates, score 1.03) (writing took 0.4058086210000056 seconds)\n",
            "2024-01-02 14:30:52 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)\n",
            "2024-01-02 14:30:52 | INFO | train | epoch 032 | loss 1.014 | nll_loss 0.169 | ppl 1.12 | wps 27799.2 | ups 6.94 | wpb 4007.6 | bsz 395.4 | num_updates 5280 | lr 0.000435194 | gnorm 0.146 | clip 0 | train_wall 21 | gb_free 13.9 | wall 802\n",
            "2024-01-02 14:30:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 033:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:30:52 | INFO | fairseq.trainer | begin training epoch 33\n",
            "2024-01-02 14:30:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 033:  99% 164/165 [00:21<00:00,  8.30it/s, loss=1.013, nll_loss=0.167, ppl=1.12, wps=30167.8, ups=7.46, wpb=4043, bsz=396.1, num_updates=5400, lr=0.000430331, gnorm=0.122, clip=0, train_wall=13, gb_free=13.8, wall=818]2024-01-02 14:31:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 033 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 13.13it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 15.26it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  27% 6/22 [00:00<00:01, 15.86it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 14.15it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 15.34it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 15.60it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  73% 16/22 [00:01<00:00, 16.78it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  82% 18/22 [00:01<00:00, 16.06it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 15.72it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:31:16 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 1.032 | nll_loss 0.097 | ppl 1.07 | wps 64056.1 | wpb 3732.4 | bsz 371.1 | num_updates 5445 | best_loss 1.029\n",
            "2024-01-02 14:31:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 5445 updates\n",
            "2024-01-02 14:31:16 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:31:16 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:31:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 33 @ 5445 updates, score 1.032) (writing took 0.5445703330000242 seconds)\n",
            "2024-01-02 14:31:16 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)\n",
            "2024-01-02 14:31:16 | INFO | train | epoch 033 | loss 1.014 | nll_loss 0.168 | ppl 1.12 | wps 27623.1 | ups 6.89 | wpb 4007.6 | bsz 395.4 | num_updates 5445 | lr 0.00042855 | gnorm 0.13 | clip 0 | train_wall 21 | gb_free 13.8 | wall 826\n",
            "2024-01-02 14:31:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 034:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:31:16 | INFO | fairseq.trainer | begin training epoch 34\n",
            "2024-01-02 14:31:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 034:  99% 164/165 [00:21<00:00,  7.17it/s, loss=1.013, nll_loss=0.167, ppl=1.12, wps=29330.5, ups=7.38, wpb=3972.4, bsz=400, num_updates=5600, lr=0.000422577, gnorm=0.135, clip=0, train_wall=13, gb_free=13.7, wall=847]2024-01-02 14:31:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 034 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 16.90it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 16.95it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  27% 6/22 [00:00<00:00, 17.71it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  41% 9/22 [00:00<00:00, 19.10it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  55% 12/22 [00:00<00:00, 19.48it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 19.35it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  73% 16/22 [00:00<00:00, 19.10it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  86% 19/22 [00:00<00:00, 20.18it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:31:40 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 1.031 | nll_loss 0.094 | ppl 1.07 | wps 79295.1 | wpb 3732.4 | bsz 371.1 | num_updates 5610 | best_loss 1.029\n",
            "2024-01-02 14:31:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 5610 updates\n",
            "2024-01-02 14:31:40 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:31:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:31:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 34 @ 5610 updates, score 1.031) (writing took 0.4081549630000154 seconds)\n",
            "2024-01-02 14:31:40 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)\n",
            "2024-01-02 14:31:40 | INFO | train | epoch 034 | loss 1.013 | nll_loss 0.167 | ppl 1.12 | wps 28048.9 | ups 7 | wpb 4007.6 | bsz 395.4 | num_updates 5610 | lr 0.0004222 | gnorm 0.129 | clip 0 | train_wall 21 | gb_free 14.6 | wall 849\n",
            "2024-01-02 14:31:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 035:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:31:40 | INFO | fairseq.trainer | begin training epoch 35\n",
            "2024-01-02 14:31:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 035:  99% 164/165 [00:21<00:00,  7.28it/s, loss=1.012, nll_loss=0.166, ppl=1.12, wps=26930.5, ups=6.52, wpb=4133.1, bsz=392.4, num_updates=5700, lr=0.000418854, gnorm=0.128, clip=0, train_wall=13, gb_free=13.8, wall=862]2024-01-02 14:32:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 035 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 14.92it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 16.19it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  27% 6/22 [00:00<00:01, 15.12it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  41% 9/22 [00:00<00:00, 17.58it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 16.46it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 16.57it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  73% 16/22 [00:00<00:00, 17.26it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  86% 19/22 [00:01<00:00, 18.34it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset: 100% 22/22 [00:01<00:00, 20.40it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:32:03 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 1.031 | nll_loss 0.096 | ppl 1.07 | wps 69044.6 | wpb 3732.4 | bsz 371.1 | num_updates 5775 | best_loss 1.029\n",
            "2024-01-02 14:32:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 5775 updates\n",
            "2024-01-02 14:32:03 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:32:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:32:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 35 @ 5775 updates, score 1.031) (writing took 0.5580540589999146 seconds)\n",
            "2024-01-02 14:32:04 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n",
            "2024-01-02 14:32:04 | INFO | train | epoch 035 | loss 1.013 | nll_loss 0.167 | ppl 1.12 | wps 27611 | ups 6.89 | wpb 4007.6 | bsz 395.4 | num_updates 5775 | lr 0.000416125 | gnorm 0.129 | clip 0 | train_wall 21 | gb_free 13.6 | wall 873\n",
            "2024-01-02 14:32:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 036:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:32:04 | INFO | fairseq.trainer | begin training epoch 36\n",
            "2024-01-02 14:32:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 036:  99% 164/165 [00:21<00:00,  7.45it/s, loss=1.013, nll_loss=0.167, ppl=1.12, wps=29318.7, ups=7.46, wpb=3932.3, bsz=396.1, num_updates=5900, lr=0.000411693, gnorm=0.135, clip=0, train_wall=13, gb_free=13.9, wall=890]2024-01-02 14:32:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 036 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 15.79it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 19.48it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 20.90it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 21.66it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 22.15it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  77% 17/22 [00:00<00:00, 20.01it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  91% 20/22 [00:00<00:00, 19.80it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:32:27 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 1.028 | nll_loss 0.094 | ppl 1.07 | wps 83502.1 | wpb 3732.4 | bsz 371.1 | num_updates 5940 | best_loss 1.028\n",
            "2024-01-02 14:32:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 5940 updates\n",
            "2024-01-02 14:32:27 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:32:27 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 14:32:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 36 @ 5940 updates, score 1.028) (writing took 0.9108536230000936 seconds)\n",
            "2024-01-02 14:32:28 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)\n",
            "2024-01-02 14:32:28 | INFO | train | epoch 036 | loss 1.013 | nll_loss 0.167 | ppl 1.12 | wps 27684.3 | ups 6.91 | wpb 4007.6 | bsz 395.4 | num_updates 5940 | lr 0.000410305 | gnorm 0.137 | clip 0 | train_wall 21 | gb_free 13.9 | wall 897\n",
            "2024-01-02 14:32:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 165\n",
            "epoch 037:   0% 0/165 [00:00<?, ?it/s]2024-01-02 14:32:28 | INFO | fairseq.trainer | begin training epoch 37\n",
            "2024-01-02 14:32:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 037:  36% 59/165 [00:08<00:12,  8.80it/s]2024-01-02 14:32:36 | INFO | fairseq_cli.train | Stopping training due to num_updates: 6000 >= max_update: 6000\n",
            "2024-01-02 14:32:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 037 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 15.38it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 18.93it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 20.58it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 20.63it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 20.95it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  77% 17/22 [00:00<00:00, 19.39it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  86% 19/22 [00:00<00:00, 19.48it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset: 100% 22/22 [00:01<00:00, 21.90it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-02 14:32:37 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 1.03 | nll_loss 0.094 | ppl 1.07 | wps 80126.6 | wpb 3732.4 | bsz 371.1 | num_updates 6000 | best_loss 1.028\n",
            "2024-01-02 14:32:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 6000 updates\n",
            "2024-01-02 14:32:37 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:32:38 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-02 14:32:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 37 @ 6000 updates, score 1.03) (writing took 0.5236608089999208 seconds)\n",
            "2024-01-02 14:32:38 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)\n",
            "2024-01-02 14:32:38 | INFO | train | epoch 037 | loss 1.013 | nll_loss 0.167 | ppl 1.12 | wps 23715.5 | ups 5.97 | wpb 3972.1 | bsz 400 | num_updates 6000 | lr 0.000408248 | gnorm 0.131 | clip 0 | train_wall 8 | gb_free 13.6 | wall 907\n",
            "2024-01-02 14:32:38 | INFO | fairseq_cli.train | done training in 904.6 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 777 test.sh\n",
        "!./test.sh cat no_overlap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "My8ri9pMgK8g",
        "outputId": "2f93dd29-6ef6-4f0f-dadc-0aa6ec05f317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-02 15:04:44.438393: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-02 15:04:44.438441: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-02 15:04:44.439927: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-02 15:04:44.447792: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-02 15:04:45.436032: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-02 15:04:49 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/cat-models/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin/cat/', 'source_lang': 'cat.no_overlap.input', 'target_lang': 'cat.no_overlap.output', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-01-02 15:04:49 | INFO | fairseq.tasks.translation | [cat.no_overlap.input] dictionary: 64 types\n",
            "2024-01-02 15:04:49 | INFO | fairseq.tasks.translation | [cat.no_overlap.output] dictionary: 40 types\n",
            "2024-01-02 15:04:49 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-02 15:04:50 | INFO | fairseq.data.data_utils | loaded 8,167 examples from: data-bin/cat/test.cat.no_overlap.input-cat.no_overlap.output.cat.no_overlap.input\n",
            "2024-01-02 15:04:51 | INFO | fairseq.data.data_utils | loaded 8,167 examples from: data-bin/cat/test.cat.no_overlap.input-cat.no_overlap.output.cat.no_overlap.output\n",
            "2024-01-02 15:04:51 | INFO | fairseq.tasks.translation | data-bin/cat/ test cat.no_overlap.input-cat.no_overlap.output 8167 examples\n",
            "2024-01-02 15:05:32 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2024-01-02 15:05:32 | INFO | fairseq_cli.generate | Translated 8,167 sentences (82,826 tokens) in 11.0s (740.49 sentences/s, 7509.76 tokens/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fake copy"
      ],
      "metadata": {
        "id": "yJHwb_PfAJpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash ./preprocess.sh cat fake_copy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmk_ryf7gT0x",
        "outputId": "0ab59f67-9c90-4885-9586-65392951bd20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-01 15:10:38.323570: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-01 15:10:38.323641: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-01 15:10:38.325670: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-01 15:10:38.337663: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-01 15:10:39.831151: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-01 15:10:42 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer='space', bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='cat.fake_copy.input', target_lang='cat.fake_copy.output', trainpref='train', validpref='devel', testpref='test', align_suffix=None, destdir='data-bin/cat', thresholdtgt=5, thresholdsrc=5, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=1, dict_only=False)\n",
            "2024-01-01 15:10:44 | INFO | fairseq_cli.preprocess | [cat.fake_copy.input] Dictionary: 64 types\n",
            "2024-01-01 15:10:52 | INFO | fairseq_cli.preprocess | [cat.fake_copy.input] train.cat.fake_copy.input: 65400 sents, 985074 tokens, 0.0% replaced (by <unk>)\n",
            "2024-01-01 15:10:52 | INFO | fairseq_cli.preprocess | [cat.fake_copy.input] Dictionary: 64 types\n",
            "2024-01-01 15:10:53 | INFO | fairseq_cli.preprocess | [cat.fake_copy.input] devel.cat.fake_copy.input: 8164 sents, 122751 tokens, 0.0% replaced (by <unk>)\n",
            "2024-01-01 15:10:53 | INFO | fairseq_cli.preprocess | [cat.fake_copy.input] Dictionary: 64 types\n",
            "2024-01-01 15:10:54 | INFO | fairseq_cli.preprocess | [cat.fake_copy.input] test.cat.fake_copy.input: 8167 sents, 123114 tokens, 0.0% replaced (by <unk>)\n",
            "2024-01-01 15:10:54 | INFO | fairseq_cli.preprocess | [cat.fake_copy.output] Dictionary: 40 types\n",
            "2024-01-01 15:11:01 | INFO | fairseq_cli.preprocess | [cat.fake_copy.output] train.cat.fake_copy.output: 65400 sents, 662679 tokens, 0.000905% replaced (by <unk>)\n",
            "2024-01-01 15:11:01 | INFO | fairseq_cli.preprocess | [cat.fake_copy.output] Dictionary: 40 types\n",
            "2024-01-01 15:11:02 | INFO | fairseq_cli.preprocess | [cat.fake_copy.output] devel.cat.fake_copy.output: 8164 sents, 82112 tokens, 0.00244% replaced (by <unk>)\n",
            "2024-01-01 15:11:02 | INFO | fairseq_cli.preprocess | [cat.fake_copy.output] Dictionary: 40 types\n",
            "2024-01-01 15:11:03 | INFO | fairseq_cli.preprocess | [cat.fake_copy.output] test.cat.fake_copy.output: 8167 sents, 83191 tokens, 0.0024% replaced (by <unk>)\n",
            "2024-01-01 15:11:03 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/cat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r checkpoints/cat-models/\n",
        "!bash ./train.sh cat fake_copy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WodI1kkxgTpW",
        "outputId": "993e1413-2aec-409e-c2be-73bb2aaad3ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-01 15:11:10.199697: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-01 15:11:10.199754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-01 15:11:10.201577: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-01 15:11:10.211941: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-01 15:11:12.194099: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-01 15:11:13 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2024-01-01 15:11:16 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 212, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 400, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 400, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 6000, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/cat-models', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=212, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=400, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=400, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=0, max_update=6000, stop_time_hours=0, clip_norm=1.0, sentence_avg=False, update_freq=[1], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/cat-models', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-bin/cat', source_lang='cat.fake_copy.input', target_lang='cat.fake_copy.output', load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=1000, warmup_init_lr=-1, pad=1, eos=2, unk=3, dropout=0.3, attention_dropout=0.3, activation_dropout=0.3, activation_fn='relu', encoder_embed_dim=256, encoder_ffn_embed_dim=1024, encoder_layers=4, encoder_attention_heads=4, encoder_normalize_before=True, decoder_embed_dim=256, decoder_ffn_embed_dim=1024, decoder_layers=4, decoder_attention_heads=4, decoder_normalize_before=True, share_decoder_input_output_embed=True, no_seed_provided=False, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=256, decoder_input_dim=256, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'data-bin/cat', 'source_lang': 'cat.fake_copy.input', 'target_lang': 'cat.fake_copy.output', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 1000, 'warmup_init_lr': -1.0, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-01-01 15:11:16 | INFO | fairseq.tasks.translation | [cat.fake_copy.input] dictionary: 64 types\n",
            "2024-01-01 15:11:16 | INFO | fairseq.tasks.translation | [cat.fake_copy.output] dictionary: 40 types\n",
            "2024-01-01 15:11:16 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(64, 256, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-3): 4 x TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(40, 256, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-3): 4 x TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=256, out_features=40, bias=False)\n",
            "  )\n",
            ")\n",
            "2024-01-01 15:11:16 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2024-01-01 15:11:16 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2024-01-01 15:11:16 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2024-01-01 15:11:16 | INFO | fairseq_cli.train | num. shared model params: 10,555,392 (num. trained: 10,555,392)\n",
            "2024-01-01 15:11:16 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2024-01-01 15:11:16 | INFO | fairseq.data.data_utils | loaded 8,164 examples from: data-bin/cat/valid.cat.fake_copy.input-cat.fake_copy.output.cat.fake_copy.input\n",
            "2024-01-01 15:11:16 | INFO | fairseq.data.data_utils | loaded 8,164 examples from: data-bin/cat/valid.cat.fake_copy.input-cat.fake_copy.output.cat.fake_copy.output\n",
            "2024-01-01 15:11:16 | INFO | fairseq.tasks.translation | data-bin/cat valid cat.fake_copy.input-cat.fake_copy.output 8164 examples\n",
            "2024-01-01 15:11:16 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2024-01-01 15:11:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-01-01 15:11:16 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2024-01-01 15:11:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-01-01 15:11:16 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2024-01-01 15:11:16 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 400\n",
            "2024-01-01 15:11:16 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:11:16 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:11:16 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2024-01-01 15:11:16 | INFO | fairseq.data.data_utils | loaded 65,400 examples from: data-bin/cat/train.cat.fake_copy.input-cat.fake_copy.output.cat.fake_copy.input\n",
            "2024-01-01 15:11:16 | INFO | fairseq.data.data_utils | loaded 65,400 examples from: data-bin/cat/train.cat.fake_copy.input-cat.fake_copy.output.cat.fake_copy.output\n",
            "2024-01-01 15:11:16 | INFO | fairseq.tasks.translation | data-bin/cat train cat.fake_copy.input-cat.fake_copy.output 65400 examples\n",
            "2024-01-01 15:11:16 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2024-01-01 15:11:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 001:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:11:16 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2024-01-01 15:11:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001:  99% 163/164 [00:24<00:00,  6.87it/s, loss=6.122, nll_loss=6.003, ppl=64.14, wps=31848.7, ups=7.76, wpb=4092.1, bsz=400, num_updates=100, lr=0.0001, gnorm=2.589, clip=73, train_wall=13, gb_free=13.6, wall=14]2024-01-01 15:11:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   5% 1/22 [00:00<00:03,  6.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 16.20it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  32% 7/22 [00:00<00:00, 18.57it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  41% 9/22 [00:00<00:00, 18.74it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  55% 12/22 [00:00<00:00, 19.92it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  68% 15/22 [00:00<00:00, 20.59it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  82% 18/22 [00:00<00:00, 20.46it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  95% 21/22 [00:01<00:00, 21.66it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:11:42 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.821 | nll_loss 3.428 | ppl 10.76 | wps 85532.9 | wpb 3732.4 | bsz 371.1 | num_updates 164\n",
            "2024-01-01 15:11:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 164 updates\n",
            "2024-01-01 15:11:42 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:11:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:11:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 1 @ 164 updates, score 3.821) (writing took 1.5043820690000302 seconds)\n",
            "2024-01-01 15:11:44 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2024-01-01 15:11:44 | INFO | train | epoch 001 | loss 5.431 | nll_loss 5.259 | ppl 38.29 | wps 25029.6 | ups 6.19 | wpb 4040.7 | bsz 398.8 | num_updates 164 | lr 0.000164 | gnorm 2.148 | clip 74.4 | train_wall 24 | gb_free 13.6 | wall 28\n",
            "2024-01-01 15:11:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 002:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:11:44 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2024-01-01 15:11:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:  99% 163/164 [00:21<00:00,  7.93it/s, loss=2.793, nll_loss=2.208, ppl=4.62, wps=29173.3, ups=7.44, wpb=3920, bsz=400, num_updates=300, lr=0.0003, gnorm=1.602, clip=85, train_wall=13, gb_free=13.8, wall=46]2024-01-01 15:12:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 13.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 17.28it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  32% 7/22 [00:00<00:00, 17.47it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  41% 9/22 [00:00<00:00, 16.63it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 16.15it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 17.89it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  73% 16/22 [00:00<00:00, 16.01it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  82% 18/22 [00:01<00:00, 16.81it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 16.30it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:12:07 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 1.745 | nll_loss 0.754 | ppl 1.69 | wps 67117 | wpb 3732.4 | bsz 371.1 | num_updates 328 | best_loss 1.745\n",
            "2024-01-01 15:12:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 328 updates\n",
            "2024-01-01 15:12:07 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:12:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:12:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 2 @ 328 updates, score 1.745) (writing took 1.144368219999933 seconds)\n",
            "2024-01-01 15:12:08 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2024-01-01 15:12:08 | INFO | train | epoch 002 | loss 2.88 | nll_loss 2.315 | ppl 4.98 | wps 27193.3 | ups 6.73 | wpb 4040.7 | bsz 398.8 | num_updates 328 | lr 0.000328 | gnorm 1.565 | clip 87.8 | train_wall 21 | gb_free 13.6 | wall 52\n",
            "2024-01-01 15:12:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 003:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:12:08 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2024-01-01 15:12:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:  99% 162/164 [00:21<00:00,  8.82it/s, loss=1.957, nll_loss=1.196, ppl=2.29, wps=26368.6, ups=6.47, wpb=4076.9, bsz=400, num_updates=400, lr=0.0004, gnorm=1.322, clip=78, train_wall=13, gb_free=13.4, wall=62]2024-01-01 15:12:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 18.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 20.26it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 20.98it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 21.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 20.94it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  77% 17/22 [00:00<00:00, 21.38it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  91% 20/22 [00:00<00:00, 20.43it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:12:32 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 1.387 | nll_loss 0.301 | ppl 1.23 | wps 84290.9 | wpb 3732.4 | bsz 371.1 | num_updates 492 | best_loss 1.387\n",
            "2024-01-01 15:12:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 492 updates\n",
            "2024-01-01 15:12:32 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:12:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:12:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 3 @ 492 updates, score 1.387) (writing took 0.9202293400001054 seconds)\n",
            "2024-01-01 15:12:33 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2024-01-01 15:12:33 | INFO | train | epoch 003 | loss 1.734 | nll_loss 0.933 | ppl 1.91 | wps 27400.5 | ups 6.78 | wpb 4040.7 | bsz 398.8 | num_updates 492 | lr 0.000492 | gnorm 1.142 | clip 56.1 | train_wall 21 | gb_free 13.5 | wall 77\n",
            "2024-01-01 15:12:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 004:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:12:33 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2024-01-01 15:12:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004:  99% 163/164 [00:21<00:00,  7.86it/s, loss=1.475, nll_loss=0.641, ppl=1.56, wps=30037.4, ups=7.25, wpb=4143.8, bsz=400, num_updates=600, lr=0.0006, gnorm=0.94, clip=29, train_wall=13, gb_free=13.9, wall=91]2024-01-01 15:12:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 14.75it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 15.65it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  27% 6/22 [00:00<00:01, 15.01it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 15.11it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  45% 10/22 [00:00<00:00, 15.98it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  55% 12/22 [00:00<00:00, 16.02it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 15.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  73% 16/22 [00:01<00:00, 15.69it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  82% 18/22 [00:01<00:00, 15.60it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 15.42it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:12:56 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 1.216 | nll_loss 0.192 | ppl 1.14 | wps 63632.1 | wpb 3732.4 | bsz 371.1 | num_updates 656 | best_loss 1.216\n",
            "2024-01-01 15:12:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 656 updates\n",
            "2024-01-01 15:12:56 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:12:56 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:12:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 4 @ 656 updates, score 1.216) (writing took 1.132344299999886 seconds)\n",
            "2024-01-01 15:12:57 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2024-01-01 15:12:57 | INFO | train | epoch 004 | loss 1.441 | nll_loss 0.604 | ppl 1.52 | wps 27292.4 | ups 6.75 | wpb 4040.7 | bsz 398.8 | num_updates 656 | lr 0.000656 | gnorm 0.875 | clip 25 | train_wall 21 | gb_free 13.9 | wall 101\n",
            "2024-01-01 15:12:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 005:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:12:57 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2024-01-01 15:12:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:  99% 163/164 [00:21<00:00,  8.09it/s, loss=1.286, nll_loss=0.446, ppl=1.36, wps=30042.9, ups=7.27, wpb=4135, bsz=398, num_updates=800, lr=0.0008, gnorm=0.667, clip=14, train_wall=13, gb_free=13.8, wall=120]2024-01-01 15:13:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 17.12it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 18.68it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  32% 7/22 [00:00<00:00, 18.62it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  41% 9/22 [00:00<00:00, 19.07it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 19.35it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 19.15it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  73% 16/22 [00:00<00:00, 19.60it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  86% 19/22 [00:00<00:00, 20.59it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:13:20 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 1.176 | nll_loss 0.182 | ppl 1.13 | wps 80501.8 | wpb 3732.4 | bsz 371.1 | num_updates 820 | best_loss 1.176\n",
            "2024-01-01 15:13:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 820 updates\n",
            "2024-01-01 15:13:20 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:13:20 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:13:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 5 @ 820 updates, score 1.176) (writing took 0.9440472189999127 seconds)\n",
            "2024-01-01 15:13:21 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2024-01-01 15:13:21 | INFO | train | epoch 005 | loss 1.294 | nll_loss 0.453 | ppl 1.37 | wps 27481.1 | ups 6.8 | wpb 4040.7 | bsz 398.8 | num_updates 820 | lr 0.00082 | gnorm 0.689 | clip 12.8 | train_wall 21 | gb_free 13.5 | wall 125\n",
            "2024-01-01 15:13:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 006:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:13:21 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2024-01-01 15:13:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006:  99% 163/164 [00:22<00:00,  7.63it/s, loss=1.226, nll_loss=0.382, ppl=1.3, wps=25373, ups=6.38, wpb=3975.4, bsz=398, num_updates=900, lr=0.0009, gnorm=0.551, clip=5, train_wall=13, gb_free=13.8, wall=136]2024-01-01 15:13:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 12.51it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 14.29it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  27% 6/22 [00:00<00:01, 14.66it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 15.49it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  45% 10/22 [00:00<00:00, 15.10it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  55% 12/22 [00:00<00:00, 15.27it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 14.78it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  73% 16/22 [00:01<00:00, 14.95it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  82% 18/22 [00:01<00:00, 14.79it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 14.66it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:13:45 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 1.113 | nll_loss 0.132 | ppl 1.1 | wps 60541.4 | wpb 3732.4 | bsz 371.1 | num_updates 984 | best_loss 1.113\n",
            "2024-01-01 15:13:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 984 updates\n",
            "2024-01-01 15:13:45 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:13:45 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:13:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 6 @ 984 updates, score 1.113) (writing took 0.9661557449999236 seconds)\n",
            "2024-01-01 15:13:46 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2024-01-01 15:13:46 | INFO | train | epoch 006 | loss 1.194 | nll_loss 0.349 | ppl 1.27 | wps 26939.4 | ups 6.67 | wpb 4040.7 | bsz 398.8 | num_updates 984 | lr 0.000984 | gnorm 0.496 | clip 3.7 | train_wall 21 | gb_free 13.8 | wall 149\n",
            "2024-01-01 15:13:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 007:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:13:46 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2024-01-01 15:13:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007:  99% 162/164 [00:21<00:00,  8.88it/s, loss=1.168, nll_loss=0.325, ppl=1.25, wps=29658.3, ups=7.23, wpb=4101.8, bsz=398, num_updates=1100, lr=0.000953463, gnorm=0.475, clip=4, train_wall=13, gb_free=13.7, wall=166]2024-01-01 15:14:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 16.18it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  18% 4/22 [00:00<00:00, 18.05it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  27% 6/22 [00:00<00:00, 18.87it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 19.26it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  45% 10/22 [00:00<00:00, 19.22it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 19.75it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  73% 16/22 [00:00<00:00, 20.40it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  86% 19/22 [00:01<00:00, 10.87it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:14:09 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 1.072 | nll_loss 0.128 | ppl 1.09 | wps 59115.2 | wpb 3732.4 | bsz 371.1 | num_updates 1148 | best_loss 1.072\n",
            "2024-01-01 15:14:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1148 updates\n",
            "2024-01-01 15:14:09 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:14:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:14:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 7 @ 1148 updates, score 1.072) (writing took 0.9465993110002273 seconds)\n",
            "2024-01-01 15:14:10 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2024-01-01 15:14:10 | INFO | train | epoch 007 | loss 1.154 | nll_loss 0.31 | ppl 1.24 | wps 27240.4 | ups 6.74 | wpb 4040.7 | bsz 398.8 | num_updates 1148 | lr 0.000933317 | gnorm 0.426 | clip 3 | train_wall 21 | gb_free 13.9 | wall 174\n",
            "2024-01-01 15:14:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 008:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:14:10 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2024-01-01 15:14:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008:  99% 163/164 [00:22<00:00,  8.69it/s, loss=1.106, nll_loss=0.262, ppl=1.2, wps=30330.1, ups=7.42, wpb=4085.6, bsz=400, num_updates=1300, lr=0.000877058, gnorm=0.355, clip=1, train_wall=13, gb_free=14, wall=195]2024-01-01 15:14:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 17.63it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 19.93it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 20.54it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 17.90it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 14.51it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  68% 15/22 [00:01<00:00, 12.16it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  77% 17/22 [00:01<00:00, 13.41it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 15.78it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:14:34 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 1.062 | nll_loss 0.113 | ppl 1.08 | wps 63651.5 | wpb 3732.4 | bsz 371.1 | num_updates 1312 | best_loss 1.062\n",
            "2024-01-01 15:14:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1312 updates\n",
            "2024-01-01 15:14:34 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:14:34 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:14:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 8 @ 1312 updates, score 1.062) (writing took 0.9516562950002481 seconds)\n",
            "2024-01-01 15:14:34 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2024-01-01 15:14:34 | INFO | train | epoch 008 | loss 1.103 | nll_loss 0.259 | ppl 1.2 | wps 26870.4 | ups 6.65 | wpb 4040.7 | bsz 398.8 | num_updates 1312 | lr 0.000873038 | gnorm 0.349 | clip 0.6 | train_wall 21 | gb_free 13.6 | wall 198\n",
            "2024-01-01 15:14:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 009:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:14:35 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2024-01-01 15:14:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 009:  99% 163/164 [00:21<00:00,  8.11it/s, loss=1.076, nll_loss=0.23, ppl=1.17, wps=25067.6, ups=6.21, wpb=4037.3, bsz=400, num_updates=1400, lr=0.000845154, gnorm=0.278, clip=0, train_wall=13, gb_free=13.4, wall=211]2024-01-01 15:14:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 16.88it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 19.31it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 19.97it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  45% 10/22 [00:00<00:00, 19.79it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 19.90it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  73% 16/22 [00:00<00:00, 19.88it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  82% 18/22 [00:00<00:00, 18.60it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 18.09it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:14:57 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 1.052 | nll_loss 0.111 | ppl 1.08 | wps 76629.4 | wpb 3732.4 | bsz 371.1 | num_updates 1476 | best_loss 1.052\n",
            "2024-01-01 15:14:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1476 updates\n",
            "2024-01-01 15:14:57 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:14:58 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:14:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 9 @ 1476 updates, score 1.052) (writing took 1.139726231000168 seconds)\n",
            "2024-01-01 15:14:59 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2024-01-01 15:14:59 | INFO | train | epoch 009 | loss 1.081 | nll_loss 0.236 | ppl 1.18 | wps 27558.3 | ups 6.82 | wpb 4040.7 | bsz 398.8 | num_updates 1476 | lr 0.000823108 | gnorm 0.316 | clip 0 | train_wall 21 | gb_free 13.8 | wall 223\n",
            "2024-01-01 15:14:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 010:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:14:59 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2024-01-01 15:14:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010:  99% 163/164 [00:22<00:00,  8.78it/s, loss=1.062, nll_loss=0.217, ppl=1.16, wps=30898.7, ups=7.58, wpb=4078.3, bsz=400, num_updates=1600, lr=0.000790569, gnorm=0.246, clip=0, train_wall=13, gb_free=13.9, wall=240]2024-01-01 15:15:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 16.59it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 17.60it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  32% 7/22 [00:00<00:00, 19.35it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  41% 9/22 [00:00<00:00, 19.54it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 19.61it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 19.34it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  68% 15/22 [00:00<00:00, 18.81it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  82% 18/22 [00:00<00:00, 19.69it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 19.60it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:15:22 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 1.044 | nll_loss 0.107 | ppl 1.08 | wps 78838.1 | wpb 3732.4 | bsz 371.1 | num_updates 1640 | best_loss 1.044\n",
            "2024-01-01 15:15:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1640 updates\n",
            "2024-01-01 15:15:22 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:15:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:15:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 10 @ 1640 updates, score 1.044) (writing took 0.960004834999836 seconds)\n",
            "2024-01-01 15:15:23 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2024-01-01 15:15:23 | INFO | train | epoch 010 | loss 1.067 | nll_loss 0.223 | ppl 1.17 | wps 27001.4 | ups 6.68 | wpb 4040.7 | bsz 398.8 | num_updates 1640 | lr 0.000780869 | gnorm 0.275 | clip 0.6 | train_wall 21 | gb_free 13.6 | wall 247\n",
            "2024-01-01 15:15:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 011:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:15:23 | INFO | fairseq.trainer | begin training epoch 11\n",
            "2024-01-01 15:15:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 011:  99% 163/164 [00:21<00:00,  7.80it/s, loss=1.06, nll_loss=0.216, ppl=1.16, wps=30106, ups=7.38, wpb=4078.9, bsz=398, num_updates=1800, lr=0.000745356, gnorm=0.296, clip=1, train_wall=13, gb_free=13.9, wall=269]2024-01-01 15:15:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   5% 1/22 [00:00<00:02,  8.92it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 16.97it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  27% 6/22 [00:00<00:00, 16.64it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 16.72it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  45% 10/22 [00:00<00:00, 15.80it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  55% 12/22 [00:00<00:00, 15.62it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 15.90it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  73% 16/22 [00:01<00:00, 14.63it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  82% 18/22 [00:01<00:00, 15.95it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 15.82it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:15:46 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 1.042 | nll_loss 0.106 | ppl 1.08 | wps 65953.7 | wpb 3732.4 | bsz 371.1 | num_updates 1804 | best_loss 1.042\n",
            "2024-01-01 15:15:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1804 updates\n",
            "2024-01-01 15:15:46 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:15:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:15:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 11 @ 1804 updates, score 1.042) (writing took 1.499058388999856 seconds)\n",
            "2024-01-01 15:15:48 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2024-01-01 15:15:48 | INFO | train | epoch 011 | loss 1.058 | nll_loss 0.214 | ppl 1.16 | wps 26676.3 | ups 6.6 | wpb 4040.7 | bsz 398.8 | num_updates 1804 | lr 0.000744529 | gnorm 0.271 | clip 0.6 | train_wall 21 | gb_free 13.9 | wall 272\n",
            "2024-01-01 15:15:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 012:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:15:48 | INFO | fairseq.trainer | begin training epoch 12\n",
            "2024-01-01 15:15:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 012:  99% 163/164 [00:21<00:00,  8.26it/s, loss=1.052, nll_loss=0.208, ppl=1.15, wps=24673, ups=6.19, wpb=3982.8, bsz=398, num_updates=1900, lr=0.000725476, gnorm=0.265, clip=0, train_wall=13, gb_free=13.8, wall=285]2024-01-01 15:16:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 15.29it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 18.32it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 19.46it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 19.80it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 19.60it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  73% 16/22 [00:00<00:00, 19.84it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  86% 19/22 [00:00<00:00, 20.29it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:16:11 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 1.043 | nll_loss 0.103 | ppl 1.07 | wps 81185.8 | wpb 3732.4 | bsz 371.1 | num_updates 1968 | best_loss 1.042\n",
            "2024-01-01 15:16:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1968 updates\n",
            "2024-01-01 15:16:11 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:16:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:16:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 12 @ 1968 updates, score 1.043) (writing took 0.3888592360003713 seconds)\n",
            "2024-01-01 15:16:11 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2024-01-01 15:16:11 | INFO | train | epoch 012 | loss 1.05 | nll_loss 0.205 | ppl 1.15 | wps 28215.6 | ups 6.98 | wpb 4040.7 | bsz 398.8 | num_updates 1968 | lr 0.000712832 | gnorm 0.258 | clip 0 | train_wall 21 | gb_free 13.3 | wall 295\n",
            "2024-01-01 15:16:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 013:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:16:11 | INFO | fairseq.trainer | begin training epoch 13\n",
            "2024-01-01 15:16:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 013:  99% 163/164 [00:21<00:00,  8.06it/s, loss=1.042, nll_loss=0.197, ppl=1.15, wps=29871.9, ups=7.14, wpb=4184.9, bsz=400, num_updates=2100, lr=0.000690066, gnorm=0.226, clip=0, train_wall=13, gb_free=13.7, wall=314]2024-01-01 15:16:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 013 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 14.13it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 15.39it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  27% 6/22 [00:00<00:01, 15.28it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 15.07it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  45% 10/22 [00:00<00:00, 15.67it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  55% 12/22 [00:00<00:00, 15.79it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 15.47it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  73% 16/22 [00:01<00:00, 15.37it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  82% 18/22 [00:01<00:00, 15.18it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 16.20it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:16:35 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 1.04 | nll_loss 0.109 | ppl 1.08 | wps 63828.9 | wpb 3732.4 | bsz 371.1 | num_updates 2132 | best_loss 1.04\n",
            "2024-01-01 15:16:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2132 updates\n",
            "2024-01-01 15:16:35 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:16:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:16:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 13 @ 2132 updates, score 1.04) (writing took 1.1561549550001473 seconds)\n",
            "2024-01-01 15:16:36 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
            "2024-01-01 15:16:36 | INFO | train | epoch 013 | loss 1.042 | nll_loss 0.197 | ppl 1.15 | wps 26860.3 | ups 6.65 | wpb 4040.7 | bsz 398.8 | num_updates 2132 | lr 0.000684867 | gnorm 0.222 | clip 0 | train_wall 21 | gb_free 13.7 | wall 320\n",
            "2024-01-01 15:16:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 014:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:16:36 | INFO | fairseq.trainer | begin training epoch 14\n",
            "2024-01-01 15:16:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 014:  99% 163/164 [00:21<00:00,  7.92it/s, loss=1.043, nll_loss=0.198, ppl=1.15, wps=25614.5, ups=6.45, wpb=3972.5, bsz=398, num_updates=2200, lr=0.0006742, gnorm=0.233, clip=0, train_wall=13, gb_free=13.6, wall=329]2024-01-01 15:16:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 014 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 17.15it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  18% 4/22 [00:00<00:00, 18.61it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  27% 6/22 [00:00<00:00, 19.12it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  41% 9/22 [00:00<00:00, 19.56it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  55% 12/22 [00:00<00:00, 20.53it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  68% 15/22 [00:00<00:00, 19.80it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  82% 18/22 [00:00<00:00, 20.62it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  95% 21/22 [00:01<00:00, 21.99it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:16:59 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 1.035 | nll_loss 0.102 | ppl 1.07 | wps 82326.4 | wpb 3732.4 | bsz 371.1 | num_updates 2296 | best_loss 1.035\n",
            "2024-01-01 15:16:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2296 updates\n",
            "2024-01-01 15:16:59 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:16:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:17:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 14 @ 2296 updates, score 1.035) (writing took 1.192299323000043 seconds)\n",
            "2024-01-01 15:17:00 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
            "2024-01-01 15:17:00 | INFO | train | epoch 014 | loss 1.041 | nll_loss 0.196 | ppl 1.15 | wps 27632.4 | ups 6.84 | wpb 4040.7 | bsz 398.8 | num_updates 2296 | lr 0.000659955 | gnorm 0.237 | clip 0 | train_wall 21 | gb_free 13.7 | wall 344\n",
            "2024-01-01 15:17:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 015:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:17:00 | INFO | fairseq.trainer | begin training epoch 15\n",
            "2024-01-01 15:17:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 015:  99% 163/164 [00:21<00:00,  7.46it/s, loss=1.035, nll_loss=0.19, ppl=1.14, wps=30032.3, ups=7.45, wpb=4033.5, bsz=398, num_updates=2400, lr=0.000645497, gnorm=0.206, clip=0, train_wall=13, gb_free=13.5, wall=358]2024-01-01 15:17:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 015 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 12.41it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 14.96it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  32% 7/22 [00:00<00:00, 16.61it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  41% 9/22 [00:00<00:00, 16.07it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 16.53it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 16.16it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  68% 15/22 [00:00<00:00, 17.08it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  77% 17/22 [00:01<00:00, 16.78it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  86% 19/22 [00:01<00:00, 16.20it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  95% 21/22 [00:01<00:00, 17.06it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:17:23 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 1.033 | nll_loss 0.097 | ppl 1.07 | wps 65181.9 | wpb 3732.4 | bsz 371.1 | num_updates 2460 | best_loss 1.033\n",
            "2024-01-01 15:17:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2460 updates\n",
            "2024-01-01 15:17:23 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:17:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:17:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 15 @ 2460 updates, score 1.033) (writing took 1.008973873999821 seconds)\n",
            "2024-01-01 15:17:24 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
            "2024-01-01 15:17:24 | INFO | train | epoch 015 | loss 1.036 | nll_loss 0.191 | ppl 1.14 | wps 27193.1 | ups 6.73 | wpb 4040.7 | bsz 398.8 | num_updates 2460 | lr 0.000637577 | gnorm 0.207 | clip 0 | train_wall 21 | gb_free 13.9 | wall 368\n",
            "2024-01-01 15:17:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 016:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:17:24 | INFO | fairseq.trainer | begin training epoch 16\n",
            "2024-01-01 15:17:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 016:  99% 163/164 [00:21<00:00,  7.95it/s, loss=1.032, nll_loss=0.186, ppl=1.14, wps=29423.2, ups=7.17, wpb=4104.5, bsz=400, num_updates=2600, lr=0.000620174, gnorm=0.191, clip=0, train_wall=13, gb_free=13.4, wall=387]2024-01-01 15:17:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 016 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 16.57it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 18.02it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 20.03it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 20.24it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 20.23it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  77% 17/22 [00:00<00:00, 19.81it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  86% 19/22 [00:00<00:00, 18.87it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:17:48 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 1.039 | nll_loss 0.103 | ppl 1.07 | wps 78773.4 | wpb 3732.4 | bsz 371.1 | num_updates 2624 | best_loss 1.033\n",
            "2024-01-01 15:17:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 2624 updates\n",
            "2024-01-01 15:17:48 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:17:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:17:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 16 @ 2624 updates, score 1.039) (writing took 0.4096549569999297 seconds)\n",
            "2024-01-01 15:17:48 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
            "2024-01-01 15:17:48 | INFO | train | epoch 016 | loss 1.033 | nll_loss 0.187 | ppl 1.14 | wps 28204.9 | ups 6.98 | wpb 4040.7 | bsz 398.8 | num_updates 2624 | lr 0.000617331 | gnorm 0.188 | clip 0 | train_wall 21 | gb_free 13.6 | wall 392\n",
            "2024-01-01 15:17:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 017:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:17:48 | INFO | fairseq.trainer | begin training epoch 17\n",
            "2024-01-01 15:17:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 017:  99% 163/164 [00:23<00:00,  8.77it/s, loss=1.029, nll_loss=0.184, ppl=1.14, wps=25970, ups=6.36, wpb=4086, bsz=398, num_updates=2700, lr=0.000608581, gnorm=0.221, clip=0, train_wall=13, gb_free=13.9, wall=403]2024-01-01 15:18:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 017 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 16.63it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 17.58it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  32% 7/22 [00:00<00:00, 19.58it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  41% 9/22 [00:00<00:00, 19.59it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  55% 12/22 [00:00<00:00, 20.20it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  68% 15/22 [00:00<00:00, 20.34it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  82% 18/22 [00:00<00:00, 19.20it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 19.12it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:18:13 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 1.032 | nll_loss 0.099 | ppl 1.07 | wps 79256.9 | wpb 3732.4 | bsz 371.1 | num_updates 2788 | best_loss 1.032\n",
            "2024-01-01 15:18:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 2788 updates\n",
            "2024-01-01 15:18:13 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:18:13 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:18:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 17 @ 2788 updates, score 1.032) (writing took 0.9001368829999592 seconds)\n",
            "2024-01-01 15:18:14 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
            "2024-01-01 15:18:14 | INFO | train | epoch 017 | loss 1.032 | nll_loss 0.187 | ppl 1.14 | wps 25900.2 | ups 6.41 | wpb 4040.7 | bsz 398.8 | num_updates 2788 | lr 0.000598899 | gnorm 0.216 | clip 0 | train_wall 22 | gb_free 13.7 | wall 418\n",
            "2024-01-01 15:18:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 018:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:18:14 | INFO | fairseq.trainer | begin training epoch 18\n",
            "2024-01-01 15:18:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 018:  99% 163/164 [00:21<00:00,  9.13it/s, loss=1.027, nll_loss=0.183, ppl=1.13, wps=29358.5, ups=7.2, wpb=4075.7, bsz=400, num_updates=2900, lr=0.00058722, gnorm=0.196, clip=0, train_wall=13, gb_free=13.4, wall=433]2024-01-01 15:18:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 018 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 15.06it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 19.20it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 19.80it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 20.30it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 20.49it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  77% 17/22 [00:00<00:00, 20.00it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 19.40it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:18:37 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 1.042 | nll_loss 0.107 | ppl 1.08 | wps 80154.9 | wpb 3732.4 | bsz 371.1 | num_updates 2952 | best_loss 1.032\n",
            "2024-01-01 15:18:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 2952 updates\n",
            "2024-01-01 15:18:37 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:18:37 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:18:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 18 @ 2952 updates, score 1.042) (writing took 0.49561138499984736 seconds)\n",
            "2024-01-01 15:18:37 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
            "2024-01-01 15:18:37 | INFO | train | epoch 018 | loss 1.028 | nll_loss 0.183 | ppl 1.13 | wps 28161.4 | ups 6.97 | wpb 4040.7 | bsz 398.8 | num_updates 2952 | lr 0.000582025 | gnorm 0.197 | clip 0 | train_wall 21 | gb_free 14 | wall 441\n",
            "2024-01-01 15:18:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 019:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:18:37 | INFO | fairseq.trainer | begin training epoch 19\n",
            "2024-01-01 15:18:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 019:  99% 163/164 [00:22<00:00,  8.05it/s, loss=1.027, nll_loss=0.182, ppl=1.13, wps=30644.6, ups=7.59, wpb=4038.7, bsz=398, num_updates=3100, lr=0.000567962, gnorm=0.176, clip=0, train_wall=13, gb_free=13.6, wall=461]2024-01-01 15:18:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 019 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 16.39it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 18.81it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 19.92it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 19.94it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 19.85it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  73% 16/22 [00:00<00:00, 20.22it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  86% 19/22 [00:00<00:00, 19.98it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:19:00 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 1.033 | nll_loss 0.099 | ppl 1.07 | wps 81090.2 | wpb 3732.4 | bsz 371.1 | num_updates 3116 | best_loss 1.032\n",
            "2024-01-01 15:19:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3116 updates\n",
            "2024-01-01 15:19:00 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:19:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:19:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 19 @ 3116 updates, score 1.033) (writing took 0.4021116020003319 seconds)\n",
            "2024-01-01 15:19:01 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
            "2024-01-01 15:19:01 | INFO | train | epoch 019 | loss 1.026 | nll_loss 0.181 | ppl 1.13 | wps 27818.8 | ups 6.88 | wpb 4040.7 | bsz 398.8 | num_updates 3116 | lr 0.000566502 | gnorm 0.183 | clip 0 | train_wall 21 | gb_free 13.7 | wall 465\n",
            "2024-01-01 15:19:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 020:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:19:01 | INFO | fairseq.trainer | begin training epoch 20\n",
            "2024-01-01 15:19:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 020:  99% 163/164 [00:21<00:00,  8.03it/s, loss=1.027, nll_loss=0.182, ppl=1.13, wps=25987.5, ups=6.74, wpb=3856, bsz=398, num_updates=3200, lr=0.000559017, gnorm=0.198, clip=0, train_wall=13, gb_free=13.6, wall=476]2024-01-01 15:19:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 020 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 17.63it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  18% 4/22 [00:00<00:00, 18.54it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  27% 6/22 [00:00<00:00, 19.05it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  41% 9/22 [00:00<00:00, 19.77it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 19.56it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 19.55it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  68% 15/22 [00:00<00:00, 19.44it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  82% 18/22 [00:00<00:00, 19.70it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 19.75it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:19:24 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 1.037 | nll_loss 0.102 | ppl 1.07 | wps 79134.7 | wpb 3732.4 | bsz 371.1 | num_updates 3280 | best_loss 1.032\n",
            "2024-01-01 15:19:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3280 updates\n",
            "2024-01-01 15:19:24 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:19:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:19:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 20 @ 3280 updates, score 1.037) (writing took 0.5647693650003021 seconds)\n",
            "2024-01-01 15:19:24 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
            "2024-01-01 15:19:24 | INFO | train | epoch 020 | loss 1.026 | nll_loss 0.181 | ppl 1.13 | wps 28460.8 | ups 7.04 | wpb 4040.7 | bsz 398.8 | num_updates 3280 | lr 0.000552158 | gnorm 0.205 | clip 0 | train_wall 21 | gb_free 13.9 | wall 488\n",
            "2024-01-01 15:19:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 021:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:19:24 | INFO | fairseq.trainer | begin training epoch 21\n",
            "2024-01-01 15:19:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 021:  99% 163/164 [00:22<00:00,  8.15it/s, loss=1.024, nll_loss=0.18, ppl=1.13, wps=31346.3, ups=7.75, wpb=4044.8, bsz=400, num_updates=3400, lr=0.000542326, gnorm=0.171, clip=0, train_wall=12, gb_free=13.7, wall=504]2024-01-01 15:19:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 021 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 16.90it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 17.40it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  32% 7/22 [00:00<00:00, 20.81it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  45% 10/22 [00:00<00:00, 21.66it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 20.96it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  73% 16/22 [00:00<00:00, 20.69it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  86% 19/22 [00:00<00:00, 21.25it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:19:47 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 1.03 | nll_loss 0.093 | ppl 1.07 | wps 84308.5 | wpb 3732.4 | bsz 371.1 | num_updates 3444 | best_loss 1.03\n",
            "2024-01-01 15:19:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 3444 updates\n",
            "2024-01-01 15:19:47 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:19:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:19:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 21 @ 3444 updates, score 1.03) (writing took 0.9388710270000047 seconds)\n",
            "2024-01-01 15:19:48 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
            "2024-01-01 15:19:48 | INFO | train | epoch 021 | loss 1.023 | nll_loss 0.178 | ppl 1.13 | wps 27400.7 | ups 6.78 | wpb 4040.7 | bsz 398.8 | num_updates 3444 | lr 0.000538851 | gnorm 0.174 | clip 0 | train_wall 21 | gb_free 13.8 | wall 512\n",
            "2024-01-01 15:19:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 022:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:19:48 | INFO | fairseq.trainer | begin training epoch 22\n",
            "2024-01-01 15:19:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 022:  99% 163/164 [00:21<00:00,  8.55it/s, loss=1.022, nll_loss=0.177, ppl=1.13, wps=29321.4, ups=7.49, wpb=3916.2, bsz=400, num_updates=3600, lr=0.000527046, gnorm=0.173, clip=0, train_wall=13, gb_free=13.5, wall=533]2024-01-01 15:20:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 022 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 16.49it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 18.73it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  32% 7/22 [00:00<00:00, 18.95it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  41% 9/22 [00:00<00:00, 16.18it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  55% 12/22 [00:00<00:00, 16.60it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 16.34it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  73% 16/22 [00:00<00:00, 15.81it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  86% 19/22 [00:01<00:00, 16.92it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset: 100% 22/22 [00:01<00:00, 19.44it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:20:11 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 1.031 | nll_loss 0.094 | ppl 1.07 | wps 67934.1 | wpb 3732.4 | bsz 371.1 | num_updates 3608 | best_loss 1.03\n",
            "2024-01-01 15:20:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 3608 updates\n",
            "2024-01-01 15:20:11 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:20:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:20:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 22 @ 3608 updates, score 1.031) (writing took 0.5639110070001152 seconds)\n",
            "2024-01-01 15:20:12 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
            "2024-01-01 15:20:12 | INFO | train | epoch 022 | loss 1.022 | nll_loss 0.177 | ppl 1.13 | wps 27980.5 | ups 6.92 | wpb 4040.7 | bsz 398.8 | num_updates 3608 | lr 0.000526462 | gnorm 0.174 | clip 0 | train_wall 21 | gb_free 13.4 | wall 536\n",
            "2024-01-01 15:20:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 023:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:20:12 | INFO | fairseq.trainer | begin training epoch 23\n",
            "2024-01-01 15:20:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 023:  99% 163/164 [00:22<00:00,  8.17it/s, loss=1.023, nll_loss=0.178, ppl=1.13, wps=26536.8, ups=6.53, wpb=4062, bsz=398, num_updates=3700, lr=0.000519875, gnorm=0.173, clip=0, train_wall=13, gb_free=13.8, wall=549]2024-01-01 15:20:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 023 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 16.71it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 18.30it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 20.53it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 20.80it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 20.46it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  77% 17/22 [00:00<00:00, 19.47it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 20.21it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:20:35 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 1.03 | nll_loss 0.095 | ppl 1.07 | wps 81940.3 | wpb 3732.4 | bsz 371.1 | num_updates 3772 | best_loss 1.03\n",
            "2024-01-01 15:20:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 3772 updates\n",
            "2024-01-01 15:20:35 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:20:36 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:20:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 23 @ 3772 updates, score 1.03) (writing took 0.920801234999999 seconds)\n",
            "2024-01-01 15:20:36 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
            "2024-01-01 15:20:36 | INFO | train | epoch 023 | loss 1.021 | nll_loss 0.176 | ppl 1.13 | wps 27303.5 | ups 6.76 | wpb 4040.7 | bsz 398.8 | num_updates 3772 | lr 0.00051489 | gnorm 0.159 | clip 0 | train_wall 21 | gb_free 13.5 | wall 560\n",
            "2024-01-01 15:20:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 024:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:20:36 | INFO | fairseq.trainer | begin training epoch 24\n",
            "2024-01-01 15:20:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 024:  99% 163/164 [00:21<00:00,  8.24it/s, loss=1.02, nll_loss=0.175, ppl=1.13, wps=29170.8, ups=7.27, wpb=4010, bsz=398, num_updates=3900, lr=0.00050637, gnorm=0.16, clip=0, train_wall=13, gb_free=13.9, wall=578]2024-01-01 15:20:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 024 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 11.55it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 14.43it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  32% 7/22 [00:00<00:00, 17.96it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  41% 9/22 [00:00<00:00, 17.48it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 16.40it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 16.99it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  68% 15/22 [00:00<00:00, 16.93it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  77% 17/22 [00:01<00:00, 17.12it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  86% 19/22 [00:01<00:00, 15.97it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  95% 21/22 [00:01<00:00, 16.95it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:21:00 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 1.031 | nll_loss 0.098 | ppl 1.07 | wps 66874.4 | wpb 3732.4 | bsz 371.1 | num_updates 3936 | best_loss 1.03\n",
            "2024-01-01 15:21:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 3936 updates\n",
            "2024-01-01 15:21:00 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:21:00 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:21:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 24 @ 3936 updates, score 1.031) (writing took 0.5617898069999683 seconds)\n",
            "2024-01-01 15:21:00 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
            "2024-01-01 15:21:00 | INFO | train | epoch 024 | loss 1.019 | nll_loss 0.174 | ppl 1.13 | wps 27819.4 | ups 6.88 | wpb 4040.7 | bsz 398.8 | num_updates 3936 | lr 0.000504049 | gnorm 0.163 | clip 0 | train_wall 21 | gb_free 13.6 | wall 584\n",
            "2024-01-01 15:21:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 025:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:21:00 | INFO | fairseq.trainer | begin training epoch 25\n",
            "2024-01-01 15:21:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 025:  99% 163/164 [00:22<00:00,  9.07it/s, loss=1.02, nll_loss=0.175, ppl=1.13, wps=26485.5, ups=6.59, wpb=4016.5, bsz=400, num_updates=4000, lr=0.0005, gnorm=0.171, clip=1, train_wall=13, gb_free=13.7, wall=593]2024-01-01 15:21:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 025 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 16.37it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 19.23it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  32% 7/22 [00:00<00:00, 18.51it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  45% 10/22 [00:00<00:00, 19.85it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  55% 12/22 [00:00<00:00, 19.07it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  68% 15/22 [00:00<00:00, 19.55it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  77% 17/22 [00:00<00:00, 19.55it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 19.87it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:21:23 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 1.034 | nll_loss 0.101 | ppl 1.07 | wps 80575.9 | wpb 3732.4 | bsz 371.1 | num_updates 4100 | best_loss 1.03\n",
            "2024-01-01 15:21:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 4100 updates\n",
            "2024-01-01 15:21:23 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:21:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:21:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 25 @ 4100 updates, score 1.034) (writing took 0.42839791999995214 seconds)\n",
            "2024-01-01 15:21:24 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
            "2024-01-01 15:21:24 | INFO | train | epoch 025 | loss 1.019 | nll_loss 0.174 | ppl 1.13 | wps 27888.5 | ups 6.9 | wpb 4040.7 | bsz 398.8 | num_updates 4100 | lr 0.000493865 | gnorm 0.168 | clip 0.6 | train_wall 21 | gb_free 13.7 | wall 608\n",
            "2024-01-01 15:21:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 026:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:21:24 | INFO | fairseq.trainer | begin training epoch 26\n",
            "2024-01-01 15:21:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 026:  99% 163/164 [00:21<00:00,  7.91it/s, loss=1.016, nll_loss=0.171, ppl=1.13, wps=26956.7, ups=6.47, wpb=4167.8, bsz=398, num_updates=4200, lr=0.00048795, gnorm=0.15, clip=0, train_wall=13, gb_free=14, wall=622]2024-01-01 15:21:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 026 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 11.85it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  23% 5/22 [00:00<00:01, 16.29it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  32% 7/22 [00:00<00:00, 15.63it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  41% 9/22 [00:00<00:00, 15.97it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 17.02it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 16.47it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  68% 15/22 [00:00<00:00, 15.97it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  77% 17/22 [00:01<00:00, 15.55it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  86% 19/22 [00:01<00:00, 15.50it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  95% 21/22 [00:01<00:00, 16.59it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:21:47 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 1.03 | nll_loss 0.096 | ppl 1.07 | wps 64240.7 | wpb 3732.4 | bsz 371.1 | num_updates 4264 | best_loss 1.03\n",
            "2024-01-01 15:21:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 4264 updates\n",
            "2024-01-01 15:21:47 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:21:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:21:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 26 @ 4264 updates, score 1.03) (writing took 1.1825252009998621 seconds)\n",
            "2024-01-01 15:21:48 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n",
            "2024-01-01 15:21:48 | INFO | train | epoch 026 | loss 1.018 | nll_loss 0.173 | ppl 1.13 | wps 27290.5 | ups 6.75 | wpb 4040.7 | bsz 398.8 | num_updates 4264 | lr 0.000484274 | gnorm 0.157 | clip 0 | train_wall 21 | gb_free 13.9 | wall 632\n",
            "2024-01-01 15:21:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 027:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:21:48 | INFO | fairseq.trainer | begin training epoch 27\n",
            "2024-01-01 15:21:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 027:  99% 163/164 [00:22<00:00,  7.64it/s, loss=1.016, nll_loss=0.171, ppl=1.13, wps=29379.7, ups=7.25, wpb=4053.4, bsz=400, num_updates=4400, lr=0.000476731, gnorm=0.16, clip=0, train_wall=13, gb_free=13.5, wall=651]2024-01-01 15:22:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 027 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 16.56it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 18.74it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  32% 7/22 [00:00<00:00, 18.61it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  41% 9/22 [00:00<00:00, 18.82it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  55% 12/22 [00:00<00:00, 19.17it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  68% 15/22 [00:00<00:00, 19.52it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  77% 17/22 [00:00<00:00, 18.69it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  86% 19/22 [00:01<00:00, 19.01it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:22:12 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 1.03 | nll_loss 0.096 | ppl 1.07 | wps 77242.2 | wpb 3732.4 | bsz 371.1 | num_updates 4428 | best_loss 1.03\n",
            "2024-01-01 15:22:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 4428 updates\n",
            "2024-01-01 15:22:12 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:22:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:22:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 27 @ 4428 updates, score 1.03) (writing took 0.9702097929998672 seconds)\n",
            "2024-01-01 15:22:13 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n",
            "2024-01-01 15:22:13 | INFO | train | epoch 027 | loss 1.016 | nll_loss 0.171 | ppl 1.13 | wps 26830.4 | ups 6.64 | wpb 4040.7 | bsz 398.8 | num_updates 4428 | lr 0.000475222 | gnorm 0.155 | clip 0 | train_wall 22 | gb_free 13.8 | wall 657\n",
            "2024-01-01 15:22:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 028:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:22:13 | INFO | fairseq.trainer | begin training epoch 28\n",
            "2024-01-01 15:22:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 028:  99% 163/164 [00:22<00:00,  7.00it/s, loss=1.016, nll_loss=0.171, ppl=1.13, wps=24994.7, ups=6.23, wpb=4012.4, bsz=398, num_updates=4500, lr=0.000471405, gnorm=0.155, clip=0, train_wall=13, gb_free=13.6, wall=667]2024-01-01 15:22:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 028 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:   5% 1/22 [00:00<00:02,  9.11it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 14.91it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  27% 6/22 [00:00<00:00, 16.13it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 16.11it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  45% 10/22 [00:00<00:00, 15.82it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  55% 12/22 [00:00<00:00, 15.18it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 15.37it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  73% 16/22 [00:01<00:00, 15.12it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  82% 18/22 [00:01<00:00, 14.88it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 14.05it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:22:37 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 1.034 | nll_loss 0.101 | ppl 1.07 | wps 61329.4 | wpb 3732.4 | bsz 371.1 | num_updates 4592 | best_loss 1.03\n",
            "2024-01-01 15:22:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 4592 updates\n",
            "2024-01-01 15:22:37 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:22:37 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:22:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 28 @ 4592 updates, score 1.034) (writing took 0.4822807380000995 seconds)\n",
            "2024-01-01 15:22:37 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n",
            "2024-01-01 15:22:37 | INFO | train | epoch 028 | loss 1.016 | nll_loss 0.171 | ppl 1.13 | wps 27229.9 | ups 6.74 | wpb 4040.7 | bsz 398.8 | num_updates 4592 | lr 0.000466658 | gnorm 0.152 | clip 0 | train_wall 21 | gb_free 13.6 | wall 681\n",
            "2024-01-01 15:22:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 029:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:22:37 | INFO | fairseq.trainer | begin training epoch 29\n",
            "2024-01-01 15:22:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 029:  99% 163/164 [00:21<00:00,  7.61it/s, loss=1.015, nll_loss=0.17, ppl=1.12, wps=30079.3, ups=7.72, wpb=3895.4, bsz=400, num_updates=4700, lr=0.000461266, gnorm=0.145, clip=0, train_wall=12, gb_free=13.7, wall=696]2024-01-01 15:22:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 029 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 14.08it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 19.53it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  32% 7/22 [00:00<00:00, 18.75it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  41% 9/22 [00:00<00:00, 18.94it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 18.66it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 18.93it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  73% 16/22 [00:00<00:00, 19.04it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  82% 18/22 [00:00<00:00, 18.79it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  95% 21/22 [00:01<00:00, 20.88it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:23:00 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 1.03 | nll_loss 0.094 | ppl 1.07 | wps 77911.1 | wpb 3732.4 | bsz 371.1 | num_updates 4756 | best_loss 1.03\n",
            "2024-01-01 15:23:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 4756 updates\n",
            "2024-01-01 15:23:00 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:23:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:23:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 29 @ 4756 updates, score 1.03) (writing took 1.016206864999731 seconds)\n",
            "2024-01-01 15:23:01 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n",
            "2024-01-01 15:23:01 | INFO | train | epoch 029 | loss 1.015 | nll_loss 0.17 | ppl 1.12 | wps 27297 | ups 6.76 | wpb 4040.7 | bsz 398.8 | num_updates 4756 | lr 0.000458542 | gnorm 0.143 | clip 0 | train_wall 21 | gb_free 13.9 | wall 705\n",
            "2024-01-01 15:23:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 030:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:23:02 | INFO | fairseq.trainer | begin training epoch 30\n",
            "2024-01-01 15:23:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 030:  99% 163/164 [00:22<00:00,  7.70it/s, loss=1.014, nll_loss=0.168, ppl=1.12, wps=30043.3, ups=7.44, wpb=4036.6, bsz=398, num_updates=4900, lr=0.000451754, gnorm=0.144, clip=0, train_wall=13, gb_free=13.7, wall=725]2024-01-01 15:23:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 030 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 14.14it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 14.17it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  27% 6/22 [00:00<00:01, 15.75it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 15.13it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  45% 10/22 [00:00<00:00, 15.12it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 16.02it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  68% 15/22 [00:00<00:00, 16.77it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  77% 17/22 [00:01<00:00, 17.28it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  86% 19/22 [00:01<00:00, 16.40it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  95% 21/22 [00:01<00:00, 17.26it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:23:25 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 1.029 | nll_loss 0.092 | ppl 1.07 | wps 64545.5 | wpb 3732.4 | bsz 371.1 | num_updates 4920 | best_loss 1.029\n",
            "2024-01-01 15:23:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 4920 updates\n",
            "2024-01-01 15:23:25 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:23:26 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:23:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 30 @ 4920 updates, score 1.029) (writing took 1.2118244439998307 seconds)\n",
            "2024-01-01 15:23:26 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n",
            "2024-01-01 15:23:26 | INFO | train | epoch 030 | loss 1.015 | nll_loss 0.169 | ppl 1.12 | wps 26571.9 | ups 6.58 | wpb 4040.7 | bsz 398.8 | num_updates 4920 | lr 0.000450835 | gnorm 0.163 | clip 0 | train_wall 21 | gb_free 13.6 | wall 730\n",
            "2024-01-01 15:23:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 031:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:23:26 | INFO | fairseq.trainer | begin training epoch 31\n",
            "2024-01-01 15:23:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 031:  99% 163/164 [00:21<00:00,  8.35it/s, loss=1.014, nll_loss=0.168, ppl=1.12, wps=25520.6, ups=6.29, wpb=4055.9, bsz=400, num_updates=5000, lr=0.000447214, gnorm=0.162, clip=0, train_wall=13, gb_free=13.5, wall=741]2024-01-01 15:23:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 031 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 16.66it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 18.47it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 19.49it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  45% 10/22 [00:00<00:00, 19.59it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  55% 12/22 [00:00<00:00, 19.61it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 19.32it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  73% 16/22 [00:00<00:00, 17.94it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  82% 18/22 [00:00<00:00, 18.37it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 17.96it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:23:50 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 1.035 | nll_loss 0.101 | ppl 1.07 | wps 76228.3 | wpb 3732.4 | bsz 371.1 | num_updates 5084 | best_loss 1.029\n",
            "2024-01-01 15:23:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 5084 updates\n",
            "2024-01-01 15:23:50 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:23:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:23:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 31 @ 5084 updates, score 1.035) (writing took 0.4341934220001349 seconds)\n",
            "2024-01-01 15:23:50 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n",
            "2024-01-01 15:23:50 | INFO | train | epoch 031 | loss 1.014 | nll_loss 0.169 | ppl 1.12 | wps 27907.9 | ups 6.91 | wpb 4040.7 | bsz 398.8 | num_updates 5084 | lr 0.000443504 | gnorm 0.153 | clip 0 | train_wall 21 | gb_free 13.6 | wall 754\n",
            "2024-01-01 15:23:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 032:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:23:50 | INFO | fairseq.trainer | begin training epoch 32\n",
            "2024-01-01 15:23:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 032:  99% 163/164 [00:22<00:00,  7.37it/s, loss=1.013, nll_loss=0.168, ppl=1.12, wps=29446.9, ups=7.41, wpb=3976.3, bsz=400, num_updates=5200, lr=0.000438529, gnorm=0.147, clip=0, train_wall=13, gb_free=13.6, wall=770]2024-01-01 15:24:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 032 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:   5% 1/22 [00:00<00:02,  9.53it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  14% 3/22 [00:00<00:01, 12.41it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  23% 5/22 [00:00<00:01, 12.95it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  32% 7/22 [00:00<00:01, 13.80it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  41% 9/22 [00:00<00:01, 12.77it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 13.86it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 14.91it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  68% 15/22 [00:01<00:00, 15.01it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  77% 17/22 [00:01<00:00, 13.98it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  86% 19/22 [00:01<00:00, 14.73it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:24:14 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 1.026 | nll_loss 0.094 | ppl 1.07 | wps 58507.3 | wpb 3732.4 | bsz 371.1 | num_updates 5248 | best_loss 1.026\n",
            "2024-01-01 15:24:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 5248 updates\n",
            "2024-01-01 15:24:14 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:24:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:24:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_best.pt (epoch 32 @ 5248 updates, score 1.026) (writing took 0.9829304019999654 seconds)\n",
            "2024-01-01 15:24:15 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)\n",
            "2024-01-01 15:24:15 | INFO | train | epoch 032 | loss 1.014 | nll_loss 0.168 | ppl 1.12 | wps 26724.5 | ups 6.61 | wpb 4040.7 | bsz 398.8 | num_updates 5248 | lr 0.000436519 | gnorm 0.149 | clip 0 | train_wall 21 | gb_free 13.9 | wall 779\n",
            "2024-01-01 15:24:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 033:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:24:15 | INFO | fairseq.trainer | begin training epoch 33\n",
            "2024-01-01 15:24:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 033:  99% 163/164 [00:22<00:00,  8.36it/s, loss=1.013, nll_loss=0.167, ppl=1.12, wps=28615.9, ups=7.03, wpb=4072, bsz=400, num_updates=5400, lr=0.000430331, gnorm=0.136, clip=0, train_wall=14, gb_free=13.8, wall=800]2024-01-01 15:24:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 033 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 13.11it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 16.31it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  27% 6/22 [00:00<00:00, 17.78it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  36% 8/22 [00:00<00:00, 17.56it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 19.99it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 19.27it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  73% 16/22 [00:00<00:00, 19.35it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  86% 19/22 [00:00<00:00, 20.68it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:24:38 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 1.027 | nll_loss 0.095 | ppl 1.07 | wps 78604.1 | wpb 3732.4 | bsz 371.1 | num_updates 5412 | best_loss 1.026\n",
            "2024-01-01 15:24:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 5412 updates\n",
            "2024-01-01 15:24:38 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:24:39 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:24:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 33 @ 5412 updates, score 1.027) (writing took 0.4260479299996405 seconds)\n",
            "2024-01-01 15:24:39 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)\n",
            "2024-01-01 15:24:39 | INFO | train | epoch 033 | loss 1.013 | nll_loss 0.168 | ppl 1.12 | wps 27677.5 | ups 6.85 | wpb 4040.7 | bsz 398.8 | num_updates 5412 | lr 0.000429854 | gnorm 0.141 | clip 0 | train_wall 21 | gb_free 13.8 | wall 803\n",
            "2024-01-01 15:24:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 034:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:24:39 | INFO | fairseq.trainer | begin training epoch 34\n",
            "2024-01-01 15:24:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 034:  99% 163/164 [00:22<00:00,  7.76it/s, loss=1.013, nll_loss=0.167, ppl=1.12, wps=25924.2, ups=6.53, wpb=3967.4, bsz=398, num_updates=5500, lr=0.000426401, gnorm=0.155, clip=0, train_wall=13, gb_free=13.8, wall=815]2024-01-01 15:25:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 034 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 16.56it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  18% 4/22 [00:00<00:01, 16.45it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  32% 7/22 [00:00<00:00, 19.74it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  45% 10/22 [00:00<00:00, 20.12it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 19.89it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  68% 15/22 [00:00<00:00, 19.82it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  77% 17/22 [00:00<00:00, 19.44it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 19.52it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:25:03 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 1.029 | nll_loss 0.096 | ppl 1.07 | wps 79556.7 | wpb 3732.4 | bsz 371.1 | num_updates 5576 | best_loss 1.026\n",
            "2024-01-01 15:25:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 5576 updates\n",
            "2024-01-01 15:25:03 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:25:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:25:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 34 @ 5576 updates, score 1.029) (writing took 0.4266145649999089 seconds)\n",
            "2024-01-01 15:25:03 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)\n",
            "2024-01-01 15:25:03 | INFO | train | epoch 034 | loss 1.013 | nll_loss 0.167 | ppl 1.12 | wps 27442.7 | ups 6.79 | wpb 4040.7 | bsz 398.8 | num_updates 5576 | lr 0.000423486 | gnorm 0.15 | clip 0 | train_wall 21 | gb_free 13.4 | wall 827\n",
            "2024-01-01 15:25:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 035:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:25:03 | INFO | fairseq.trainer | begin training epoch 35\n",
            "2024-01-01 15:25:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 035:  99% 163/164 [00:21<00:00,  7.87it/s, loss=1.013, nll_loss=0.167, ppl=1.12, wps=29028.9, ups=7.25, wpb=4001.6, bsz=398, num_updates=5700, lr=0.000418854, gnorm=0.15, clip=0, train_wall=13, gb_free=13.9, wall=844]2024-01-01 15:25:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 035 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 16.44it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 18.74it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  32% 7/22 [00:00<00:00, 18.87it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  41% 9/22 [00:00<00:00, 19.09it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  55% 12/22 [00:00<00:00, 19.87it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  64% 14/22 [00:00<00:00, 19.85it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  77% 17/22 [00:00<00:00, 20.66it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 20.31it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:25:26 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 1.03 | nll_loss 0.099 | ppl 1.07 | wps 81287.1 | wpb 3732.4 | bsz 371.1 | num_updates 5740 | best_loss 1.026\n",
            "2024-01-01 15:25:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 5740 updates\n",
            "2024-01-01 15:25:26 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:25:27 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:25:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 35 @ 5740 updates, score 1.03) (writing took 0.41646696399993743 seconds)\n",
            "2024-01-01 15:25:27 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n",
            "2024-01-01 15:25:27 | INFO | train | epoch 035 | loss 1.012 | nll_loss 0.166 | ppl 1.12 | wps 28230.7 | ups 6.99 | wpb 4040.7 | bsz 398.8 | num_updates 5740 | lr 0.000417392 | gnorm 0.14 | clip 0 | train_wall 21 | gb_free 13.6 | wall 851\n",
            "2024-01-01 15:25:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 036:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:25:27 | INFO | fairseq.trainer | begin training epoch 36\n",
            "2024-01-01 15:25:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 036:  99% 163/164 [00:22<00:00,  7.06it/s, loss=1.012, nll_loss=0.167, ppl=1.12, wps=30180.8, ups=7.47, wpb=4041, bsz=400, num_updates=5900, lr=0.000411693, gnorm=0.134, clip=0, train_wall=13, gb_free=13.6, wall=873]2024-01-01 15:25:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 036 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:   9% 2/22 [00:00<00:01, 15.69it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  23% 5/22 [00:00<00:00, 18.49it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  32% 7/22 [00:00<00:00, 18.58it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  41% 9/22 [00:00<00:00, 19.02it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  50% 11/22 [00:00<00:00, 18.99it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  59% 13/22 [00:00<00:00, 17.95it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  68% 15/22 [00:00<00:00, 18.32it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  77% 17/22 [00:00<00:00, 18.61it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 19.61it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:25:50 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 1.033 | nll_loss 0.1 | ppl 1.07 | wps 77778.8 | wpb 3732.4 | bsz 371.1 | num_updates 5904 | best_loss 1.026\n",
            "2024-01-01 15:25:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 5904 updates\n",
            "2024-01-01 15:25:50 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:25:51 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:25:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 36 @ 5904 updates, score 1.033) (writing took 0.42941888800032757 seconds)\n",
            "2024-01-01 15:25:51 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)\n",
            "2024-01-01 15:25:51 | INFO | train | epoch 036 | loss 1.012 | nll_loss 0.166 | ppl 1.12 | wps 27420.2 | ups 6.79 | wpb 4040.7 | bsz 398.8 | num_updates 5904 | lr 0.000411554 | gnorm 0.13 | clip 0 | train_wall 21 | gb_free 13.9 | wall 875\n",
            "2024-01-01 15:25:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 164\n",
            "epoch 037:   0% 0/164 [00:00<?, ?it/s]2024-01-01 15:25:51 | INFO | fairseq.trainer | begin training epoch 37\n",
            "2024-01-01 15:25:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 037:  58% 95/164 [00:13<00:10,  6.45it/s]2024-01-01 15:26:04 | INFO | fairseq_cli.train | Stopping training due to num_updates: 6000 >= max_update: 6000\n",
            "2024-01-01 15:26:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 037 | valid on 'valid' subset:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:   5% 1/22 [00:00<00:02,  9.55it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  14% 3/22 [00:00<00:01, 13.47it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  23% 5/22 [00:00<00:01, 14.44it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  32% 7/22 [00:00<00:00, 16.26it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  45% 10/22 [00:00<00:00, 18.08it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  55% 12/22 [00:00<00:00, 17.54it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  68% 15/22 [00:00<00:00, 19.27it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  77% 17/22 [00:00<00:00, 19.12it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  91% 20/22 [00:01<00:00, 19.34it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-01-01 15:26:05 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 1.028 | nll_loss 0.098 | ppl 1.07 | wps 75312 | wpb 3732.4 | bsz 371.1 | num_updates 6000 | best_loss 1.026\n",
            "2024-01-01 15:26:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 6000 updates\n",
            "2024-01-01 15:26:05 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:26:06 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/cat-models/checkpoint_last.pt\n",
            "2024-01-01 15:26:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cat-models/checkpoint_last.pt (epoch 37 @ 6000 updates, score 1.028) (writing took 0.40804300400031934 seconds)\n",
            "2024-01-01 15:26:06 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)\n",
            "2024-01-01 15:26:06 | INFO | train | epoch 037 | loss 1.012 | nll_loss 0.166 | ppl 1.12 | wps 25767.1 | ups 6.32 | wpb 4079.4 | bsz 400 | num_updates 6000 | lr 0.000408248 | gnorm 0.131 | clip 0 | train_wall 13 | gb_free 13.7 | wall 890\n",
            "2024-01-01 15:26:06 | INFO | fairseq_cli.train | done training in 889.6 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjP2RShw2qbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f4ef6e4-69c1-4ae6-ac66-3dc07c084590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-01 15:26:20.699481: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-01 15:26:20.699531: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-01 15:26:20.700860: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-01 15:26:20.708118: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-01 15:26:21.776284: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-01 15:26:25 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/cat-models/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin/cat/', 'source_lang': 'cat.fake_copy.input', 'target_lang': 'cat.fake_copy.output', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-01-01 15:26:25 | INFO | fairseq.tasks.translation | [cat.fake_copy.input] dictionary: 64 types\n",
            "2024-01-01 15:26:25 | INFO | fairseq.tasks.translation | [cat.fake_copy.output] dictionary: 40 types\n",
            "2024-01-01 15:26:25 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/cat-models/checkpoint_best.pt\n",
            "2024-01-01 15:26:26 | INFO | fairseq.data.data_utils | loaded 8,167 examples from: data-bin/cat/test.cat.fake_copy.input-cat.fake_copy.output.cat.fake_copy.input\n",
            "2024-01-01 15:26:26 | INFO | fairseq.data.data_utils | loaded 8,167 examples from: data-bin/cat/test.cat.fake_copy.input-cat.fake_copy.output.cat.fake_copy.output\n",
            "2024-01-01 15:26:26 | INFO | fairseq.tasks.translation | data-bin/cat/ test cat.fake_copy.input-cat.fake_copy.output 8167 examples\n",
            "2024-01-01 15:27:09 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2024-01-01 15:27:09 | INFO | fairseq_cli.generate | Translated 8,167 sentences (82,809 tokens) in 11.0s (740.25 sentences/s, 7505.72 tokens/s)\n"
          ]
        }
      ],
      "source": [
        "!./test.sh cat fake_copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "To analyze the results, we obtain for each training a file with several lines. To interpret it, we have preprocessed it before.However, in order to understand the raw data must take into account the following statements ([source](https://github.com/facebookresearch/fairseq/issues/3000)):\n",
        "\n",
        "* **S** - Source, the input passed through the model.\n",
        "* **T** - Target, the expected/ideal output. Also known as gold or reference.\n",
        "* **H** - Hypothesis, the model's output without decoding. Some weird symbols can appear if we have applied a special preprocessing such as byte pair encoding.\n",
        "* **D** - Decoded. The human-readable result.\n",
        "\n",
        "We will apply a function that will transform our results into a table so that we are able to compare."
      ],
      "metadata": {
        "id": "y51WWydWAVX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_random = create_results_table(\"test_cat_random_results.txt\")\n",
        "results_no_overlap = create_results_table(\"test_cat_no_overlap_results.txt\")\n",
        "results_fake_copy = create_results_table(\"test_cat_fake_copy_results.txt\")"
      ],
      "metadata": {
        "id": "nDAtieaTFcVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_random.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "Wnq5dvT63m-6",
        "outputId": "e1c8a86a-baed-4bc7-f697-d53af506f92e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             lemma                         source_s      target   predicted\n",
              "idx                                                                        \n",
              "4363      u s a r          u s a r # V POS IMP 3 PL     u s i n     u s i n\n",
              "1948    m o c a r    m o c a r # V.PTCP PST SG MASC   m o c a t   m o c a t\n",
              "1400    d e u r e    d e u r e # V.PTCP PST SG MASC   d e g u t   d e g u t\n",
              "790     f e r i r    f e r i r # V.PTCP PST SG MASC   f e r i t   f e r i t\n",
              "403     r o b a r    r o b a r # V.PTCP PST SG MASC   r o b a t   r o b a t"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d7844c9f-647b-4e07-9743-e3ecc5f0ca04\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lemma</th>\n",
              "      <th>source_s</th>\n",
              "      <th>target</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>idx</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4363</th>\n",
              "      <td>u s a r</td>\n",
              "      <td>u s a r # V POS IMP 3 PL</td>\n",
              "      <td>u s i n</td>\n",
              "      <td>u s i n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1948</th>\n",
              "      <td>m o c a r</td>\n",
              "      <td>m o c a r # V.PTCP PST SG MASC</td>\n",
              "      <td>m o c a t</td>\n",
              "      <td>m o c a t</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1400</th>\n",
              "      <td>d e u r e</td>\n",
              "      <td>d e u r e # V.PTCP PST SG MASC</td>\n",
              "      <td>d e g u t</td>\n",
              "      <td>d e g u t</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>790</th>\n",
              "      <td>f e r i r</td>\n",
              "      <td>f e r i r # V.PTCP PST SG MASC</td>\n",
              "      <td>f e r i t</td>\n",
              "      <td>f e r i t</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403</th>\n",
              "      <td>r o b a r</td>\n",
              "      <td>r o b a r # V.PTCP PST SG MASC</td>\n",
              "      <td>r o b a t</td>\n",
              "      <td>r o b a t</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d7844c9f-647b-4e07-9743-e3ecc5f0ca04')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d7844c9f-647b-4e07-9743-e3ecc5f0ca04 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d7844c9f-647b-4e07-9743-e3ecc5f0ca04');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9c407f99-fd92-46d2-856c-eee036604e21\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9c407f99-fd92-46d2-856c-eee036604e21')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9c407f99-fd92-46d2-856c-eee036604e21 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y_true, y_pre\n",
        "print(f'accuracy random: {accuracy_score(results_random.target, results_random.predicted)}')\n",
        "print(f'accuracy no_overlap: {accuracy_score(results_no_overlap.target, results_no_overlap.predicted)}')\n",
        "print(f'accuracy fake_copy: {accuracy_score(results_fake_copy.target, results_fake_copy.predicted)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ordAde2oyza",
        "outputId": "0a73cac8-3a5e-479b-b235-c7352d17519a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy random: 0.9886001470948762\n",
            "accuracy no_overlap: 0.9621648095996082\n",
            "accuracy fake_copy: 0.9679196767478878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random split**\n",
        "\n",
        "The random split has almost a 100% of accuracy. Does this mean that the model is perfect and knows how to generalise? It does not. In fact, this kind of split is not ideal and this accuracy could be misleading.\n",
        "\n",
        "The fact that we are using the same verbs in training and in test is actually filtering information to the model when training, so it could be considered as \"cheating\". This way, the model does not learn to derive the morphemes but rather learns the verb itself. Probably, if faced with a never-seen-before verb, the predictions wouldn't be accurate.\n",
        "\n",
        "These little details are really important when deciding how to create the split of train and test, since building a model like this can lead to undesired performance.\n",
        "\n",
        "\n",
        "**No overlap and Fake copy**\n",
        "\n",
        "The remaining two models have a very good performance as well, with only 0.005 difference of accuracy performance between them. We could consider that, in this case, adding copy-fake values does not increase considerably the performance of the model.\n",
        "\n",
        "However, let's take a look at the differences and see where they behave differently. We will take a look at the verb endings we have mentioned in the EDA step."
      ],
      "metadata": {
        "id": "MV77Uw0pPakr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_verb_ending_correctness(results_no_overlap)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "n8K75gP1D_C3",
        "outputId": "55d95788-0527-4f6a-e6bb-c321dc3e492d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_verb_ending_correctness(results_fake_copy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "CeAkMJFvORX2",
        "outputId": "20f817b7-7b0f-4aca-ba20-96924b487810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The differences between these two models seem to rely on the wrong predictions rather than the correct ones, since the distribution of correct predictions is almost identical.\n",
        "\n",
        "The `no_overlap` model seems to predict better -ir ending verbs but worse -re ending verbs. Whereas, on the contrary, `fake_copy` model performs slightly better on -re ending verbs but worse on -ir.\n",
        "\n",
        "When it comes to reflexive verbs (-se ending verbs) both models seem to perform poorly. Note that there are not -'s ending verbs. Given the few samples found in the whole data, it is likely that all samples with this ending have fallen in the input, therefore it has not been possible to evaluate them in the test."
      ],
      "metadata": {
        "id": "XzbebKG-OaGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions, improvements and future work\n",
        "\n",
        "Given the good performance in this language, it is hard to mention possible improvements since we already got a solid model.\n",
        "However, we could draw some conclusions:\n",
        "\n",
        "* Catalan is quite a regular language when it comes to verbs, this enables neural networks to learn patterns and perform well in morphological generators.\n",
        "* Further investigation could be done with reflexive verbs, since most of the verbs can be converted to reflexive using the particle `-se` or `-'s` to better asses the models capabilities with this type of lemma.\n",
        "* This dataset is limited to verbs, it would be interesting to add other types of morphemes such as adjectives.\n",
        "* The model is already small (can be trained under less than 10 minutes), however we could perform some experiments to see how to make the model smaller in order to be even faster."
      ],
      "metadata": {
        "id": "izHY71n9QvIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ‡©ðŸ‡ª German Neural Morphological Generator\n",
        "\n",
        "* Data used can be downloaded here: https://github.com/unimorph/deu\n",
        "\n",
        "* This dataset contains 519,143 samples, formed by a total of 39,373 different stems.\n",
        "* In this case, we have verbs derived to different tenses but we also find adjectives and nouns.\n",
        "* It is worth mentioning that German uses declinations, meaning that words are identified given the particles that conform them and also can be derived to different cases. It is important to note that the morphology annotations here will be considerably different from Catalan.\n",
        "* German, unlike Catalan, does not change words depending on the gender of the subject, we could say it is gender-neutral (except for some specific words that have gender in the nature of the meaning itself). Also, the third person in singular has an additional case: whereas in Catalan there is only the equivalent to \"he\":\"er\", \"she\":\"sie\", German has the equivalent to \"it\":\"es\".\n",
        "* German is an agglutinative language, meaning that many words are formed by the combination and agglutination of other morphemes, making the grammar wide and sometimes complex. We can find very long words meaning a single thing."
      ],
      "metadata": {
        "id": "GlUiV9vfpNP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "dwww3CJnupmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [l.strip() for l in open(\"deu.txt\") if len(l) > 1]\n",
        "examples = [s.replace(' ', '_') for s in examples] # Can't use spaces as tokens, so replace those"
      ],
      "metadata": {
        "id": "tnoAOMK3pV6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EDA"
      ],
      "metadata": {
        "id": "snbk9s4musT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counter = Counter()\n",
        "\n",
        "for example in examples:\n",
        "  counter[example.split('\\t')[0]] += 1"
      ],
      "metadata": {
        "id": "nh685qUOrhcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset has 39373 different stems. Quite different from before, where the number was much smaller:"
      ],
      "metadata": {
        "id": "NEqk8jApwd-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(counter.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcojEJGwwZRq",
        "outputId": "8bcb15f9-3d79-4db4-cbf0-cea26a05b1bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39373"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgoHXIhqriqH",
        "outputId": "2225a8dd-ec5e-49df-f1bd-aaabd40677d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('verticken', 120),\n",
              " ('scheren', 89),\n",
              " ('auspowern', 76),\n",
              " ('Ã¼bersiedeln', 75),\n",
              " ('wandern', 74),\n",
              " ('handeln', 74),\n",
              " ('wildeln', 74),\n",
              " ('krempeln', 74),\n",
              " ('flittern', 74),\n",
              " ('kollern', 74)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter.most_common()[-10:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQIlfG_vrkOY",
        "outputId": "8462a929-abdb-4c4d-ada1-af2221ac8695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('inner', 2),\n",
              " ('warzig', 2),\n",
              " ('knalleng', 2),\n",
              " ('einheitlich', 2),\n",
              " ('genÃ¼gend', 2),\n",
              " ('mutlos', 2),\n",
              " ('antigrippal', 2),\n",
              " ('erstrangig', 2),\n",
              " ('minder', 1),\n",
              " ('berlinfern', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is interesting to see how the number of occurences vary a lot inside the same dataset.\n",
        "* The most common verb, with difference, is \"verticken\" (to sell) with 120 different entrances.\n",
        "* The second is far below, with 89 samples, \"scheren\" (to cut) is the second most common German verb.\n",
        "\n",
        "As for the least common verbs, we have very few samples of some of them.\n",
        "* These verbs will be interesting to look at, since they might fall only on the test dataset, in which case we would be facing a **zero-shot** case-scenarion; or fall in train and test, in which case we would be facing a **few-shot** case-scenario.\n",
        "* These will be interesting to take into account in the evaluation step, since they are likely to **lower the accuracy** due to its few (or unexisting) appearances in the training set.\n",
        "* On the other hand, the model might be able to generalize and predict them well if the rest of the data has helped in the inference learning process."
      ],
      "metadata": {
        "id": "tc1Xk6zc7NBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regular verbs in German end in `-en`, `-ern` or `-eln`, however there are exceptions."
      ],
      "metadata": {
        "id": "x5B99uuUqZA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(examples)\n",
        "df[['stem', 'derivative', 'morphology']] = df[0].str.split('\\t', expand=True)\n",
        "df = df.drop(columns=[0])\n",
        "df['grammatical_category'] = df['morphology'].map(lambda text: text.split(';')[0])"
      ],
      "metadata": {
        "id": "lGg9Z2O8XYsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this dataset we see that we have 4 different grammatical categories:\n",
        "* Noun.\n",
        "* Verb.\n",
        "* Verb in participle.\n",
        "* Adjectives.\n",
        "\n",
        "And the distribution of samples is the following:"
      ],
      "metadata": {
        "id": "vqB0e6KDaXVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.grammatical_category.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asYsliEfX3Yx",
        "outputId": "f14a31b1-a630-4147-8f0f-932e36fe0b8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "N         219104\n",
              "V         198913\n",
              "ADJ        87503\n",
              "V.PTCP     13623\n",
              "Name: grammatical_category, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the diversity of this dataset, it is likely that it performs a bit worse than our previous model. It is interesting in the evaluation step, to check which which of these categories perform better."
      ],
      "metadata": {
        "id": "xYPIJAPJang2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "7gTRMW2Rrx91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = []\n",
        "for ex in examples:\n",
        "    lemma, inflection, tags = ex.split('\\t')\n",
        "    tagtokens = tags.split(';')\n",
        "    tokenized.append((' '.join(list(lemma)) + \" # \" + ' '.join(tagtokens),\\\n",
        "                      ' '.join(list(inflection))))\n",
        "\n",
        "tokenized[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU2kQAETryv0",
        "outputId": "e3c20b3b-1fe5-4185-8469-0a39b0ed2b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('W a s h i n g t o n # N NOM NEUT SG', 'W a s h i n g t o n'),\n",
              " ('W a s h i n g t o n # N GEN NEUT SG', 'W a s h i n g t o n s'),\n",
              " ('W a s h i n g t o n # N DAT NEUT SG', 'W a s h i n g t o n'),\n",
              " ('W a s h i n g t o n # N ACC NEUT SG', 'W a s h i n g t o n'),\n",
              " ('H i r s c h h o r n s a l z # N NOM NEUT SG',\n",
              "  'H i r s c h h o r n s a l z'),\n",
              " ('H i r s c h h o r n s a l z # N GEN NEUT SG',\n",
              "  'H i r s c h h o r n s a l z e s'),\n",
              " ('H i r s c h h o r n s a l z # N DAT NEUT SG',\n",
              "  'H i r s c h h o r n s a l z e'),\n",
              " ('H i r s c h h o r n s a l z # N ACC NEUT SG',\n",
              "  'H i r s c h h o r n s a l z'),\n",
              " ('S p a n n e r # N NOM MASC SG', 'S p a n n e r'),\n",
              " ('S p a n n e r # N NOM MASC PL', 'S p a n n e r')]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random"
      ],
      "metadata": {
        "id": "dHg75H3Vu0Oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deu_random_train, deu_random_devel, deu_random_test = completely_random_splits(tokenized)"
      ],
      "metadata": {
        "id": "fU8HGvApsCki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deu_random_train[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWPwB0g-5Db-",
        "outputId": "825d83f8-6c76-4b4c-bd41-044538af858f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('E h r b a r k e i t # N DAT FEM SG', 'E h r b a r k e i t'),\n",
              " ('g Ã¤ h n e n # V IMP SG 2', 'g Ã¤ h n'),\n",
              " ('k Ã¶ n n e n # V IND SG 1 PRS', 'k a n n'),\n",
              " ('H e s s e # N DAT MASC PL', 'H e s s e n'),\n",
              " ('l o s s c h i e ÃŸ e n # V NFIN', 'l o s s c h i e ÃŸ e n'),\n",
              " ('F e r n s t u d i u m # N NOM NEUT PL', 'F e r n s t u d i e n'),\n",
              " ('n a c h s a g e n # V SBJV SG 3 PRS', 's a g e _ n a c h'),\n",
              " ('s c h r e i e n # V SBJV SG 3 PST', 's c h r i e e'),\n",
              " ('A u s z a h l u n g # N ACC FEM PL', 'A u s z a h l u n g e n'),\n",
              " ('Z o b e l # N NOM MASC PL', 'Z o b e l')]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(deu_random_train), len(deu_random_devel), len(deu_random_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXAJWMGT5Mxn",
        "outputId": "488455c1-906b-461d-b0ae-93d6431bfcbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "415314 51914 51915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the proportions of the German dataset are much bigger than the Catalan (more than 6 times bigger!). For this reason, some of the processes might take more time than before, specially training and some preprocessing."
      ],
      "metadata": {
        "id": "Wj-Yy8va613r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "415314/65000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiynAY7U6ahr",
        "outputId": "b0e51338-16d9-45e8-a17d-959805ae9924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.389446153846154"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### No overlap"
      ],
      "metadata": {
        "id": "7yTXnHN3u2RW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deu_no_overlap_train, deu_no_overlap_devel, deu_no_overlap_test, deu_test_lemmas = no_overlap_splits(tokenized) # this function is specially slow"
      ],
      "metadata": {
        "id": "Joo9udyasC7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deu_no_overlap_test[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5oaT7MB5Ixo",
        "outputId": "566e2e8d-cb7e-45aa-feb6-6c329fcca747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('S p i e l v e r d e r b e r i n # N NOM FEM SG',\n",
              "  'S p i e l v e r d e r b e r i n'),\n",
              " ('S p i e l v e r d e r b e r i n # N NOM FEM PL',\n",
              "  'S p i e l v e r d e r b e r i n n e n'),\n",
              " ('S p i e l v e r d e r b e r i n # N GEN FEM SG',\n",
              "  'S p i e l v e r d e r b e r i n'),\n",
              " ('S p i e l v e r d e r b e r i n # N GEN FEM PL',\n",
              "  'S p i e l v e r d e r b e r i n n e n'),\n",
              " ('S p i e l v e r d e r b e r i n # N DAT FEM SG',\n",
              "  'S p i e l v e r d e r b e r i n'),\n",
              " ('S p i e l v e r d e r b e r i n # N DAT FEM PL',\n",
              "  'S p i e l v e r d e r b e r i n n e n'),\n",
              " ('S p i e l v e r d e r b e r i n # N ACC FEM SG',\n",
              "  'S p i e l v e r d e r b e r i n'),\n",
              " ('S p i e l v e r d e r b e r i n # N ACC FEM PL',\n",
              "  'S p i e l v e r d e r b e r i n n e n'),\n",
              " ('S c h w e s t e r l e i n # N NOM NEUT SG', 'S c h w e s t e r l e i n'),\n",
              " ('S c h w e s t e r l e i n # N NOM NEUT PL', 'S c h w e s t e r l e i n')]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fake copy"
      ],
      "metadata": {
        "id": "kbKfbBcPu4lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deu_fake_copy_train, deu_fake_copy_devel, deu_fake_copy_test = fake_copy_splits(tokenized)"
      ],
      "metadata": {
        "id": "P2XxdCy9sdT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(deu_fake_copy_train)):\n",
        "  if deu_fake_copy_train[i][0].split('#')[1] == ' COPY':\n",
        "    idx = i\n",
        "    break\n",
        "\n",
        "print(deu_fake_copy_train[idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGg6nHtNslMX",
        "outputId": "34fd45fa-fce9-4c47-9ff9-d092b6b59b83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('u n t e r w Ã¼ r f i g # COPY', 'u n t e r w Ã¼ r f i g ')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's save data into files, so that our model can read it and train with it."
      ],
      "metadata": {
        "id": "di9ZNtC9u8FN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_input_output_files(deu_random_train, 'deu', 'random', 'train')\n",
        "save_input_output_files(deu_random_devel, 'deu', 'random', 'devel')\n",
        "save_input_output_files(deu_random_test, 'deu', 'random', 'test')\n",
        "\n",
        "save_input_output_files(deu_no_overlap_train, 'deu', 'no_overlap', 'train')\n",
        "save_input_output_files(deu_no_overlap_devel, 'deu', 'no_overlap', 'devel')\n",
        "save_input_output_files(deu_no_overlap_test, 'deu', 'no_overlap', 'test')\n",
        "\n",
        "save_input_output_files(deu_fake_copy_train, 'deu', 'fake_copy', 'train')\n",
        "save_input_output_files(deu_fake_copy_devel, 'deu', 'fake_copy', 'devel')\n",
        "save_input_output_files(deu_fake_copy_test, 'deu', 'fake_copy', 'test')"
      ],
      "metadata": {
        "id": "YJIVGvH6swh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "### Random"
      ],
      "metadata": {
        "id": "3PPhOvsRsxIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash ./preprocess.sh deu random"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2NdzPiUsx6J",
        "outputId": "7037ae95-b6ed-4f0b-ff14-e2df6a79f51f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-01 17:01:42.564721: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-01 17:01:42.564776: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-01 17:01:42.566197: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-01 17:01:42.573641: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-01 17:01:43.915375: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-01 17:01:47 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer='space', bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='deu.random.input', target_lang='deu.random.output', trainpref='train', validpref='devel', testpref='test', align_suffix=None, destdir='data-bin/deu', thresholdtgt=5, thresholdsrc=5, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=1, dict_only=False)\n",
            "2024-01-01 17:02:06 | INFO | fairseq_cli.preprocess | [deu.random.input] Dictionary: 112 types\n",
            "2024-01-01 17:02:56 | INFO | fairseq_cli.preprocess | [deu.random.input] train.deu.random.input: 415314 sents, 6542965 tokens, 3.06e-05% replaced (by <unk>)\n",
            "2024-01-01 17:02:56 | INFO | fairseq_cli.preprocess | [deu.random.input] Dictionary: 112 types\n",
            "2024-01-01 17:03:03 | INFO | fairseq_cli.preprocess | [deu.random.input] devel.deu.random.input: 51914 sents, 817912 tokens, 0.000245% replaced (by <unk>)\n",
            "2024-01-01 17:03:03 | INFO | fairseq_cli.preprocess | [deu.random.input] Dictionary: 112 types\n",
            "2024-01-01 17:03:09 | INFO | fairseq_cli.preprocess | [deu.random.input] test.deu.random.input: 51915 sents, 817432 tokens, 0.0% replaced (by <unk>)\n",
            "2024-01-01 17:03:09 | INFO | fairseq_cli.preprocess | [deu.random.output] Dictionary: 88 types\n",
            "2024-01-01 17:03:48 | INFO | fairseq_cli.preprocess | [deu.random.output] train.deu.random.output: 415314 sents, 4759712 tokens, 0.000105% replaced (by <unk>)\n",
            "2024-01-01 17:03:48 | INFO | fairseq_cli.preprocess | [deu.random.output] Dictionary: 88 types\n",
            "2024-01-01 17:03:52 | INFO | fairseq_cli.preprocess | [deu.random.output] devel.deu.random.output: 51914 sents, 595030 tokens, 0.000504% replaced (by <unk>)\n",
            "2024-01-01 17:03:52 | INFO | fairseq_cli.preprocess | [deu.random.output] Dictionary: 88 types\n",
            "2024-01-01 17:03:56 | INFO | fairseq_cli.preprocess | [deu.random.output] test.deu.random.output: 51915 sents, 593994 tokens, 0.0% replaced (by <unk>)\n",
            "2024-01-01 17:03:56 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/deu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r checkpoints/cat-models/\n",
        "!bash ./train.sh deu random"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErA6Z_HuwDVK",
        "outputId": "b9f65f29-80d9-487f-e1e8-59c836979d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-01 17:04:25.753652: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-01 17:04:25.753725: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-01 17:04:25.755894: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-01 17:04:25.768278: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-01 17:04:27.276923: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-01 17:04:28 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2024-01-01 17:04:31 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 212, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 400, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 400, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 6000, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/deu-models', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=212, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=400, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=400, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=0, max_update=6000, stop_time_hours=0, clip_norm=1.0, sentence_avg=False, update_freq=[1], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/deu-models', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-bin/deu', source_lang='deu.random.input', target_lang='deu.random.output', load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=1000, warmup_init_lr=-1, pad=1, eos=2, unk=3, dropout=0.3, attention_dropout=0.3, activation_dropout=0.3, activation_fn='relu', encoder_embed_dim=256, encoder_ffn_embed_dim=1024, encoder_layers=4, encoder_attention_heads=4, encoder_normalize_before=True, decoder_embed_dim=256, decoder_ffn_embed_dim=1024, decoder_layers=4, decoder_attention_heads=4, decoder_normalize_before=True, share_decoder_input_output_embed=True, no_seed_provided=False, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=256, decoder_input_dim=256, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'data-bin/deu', 'source_lang': 'deu.random.input', 'target_lang': 'deu.random.output', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 1000, 'warmup_init_lr': -1.0, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-01-01 17:04:31 | INFO | fairseq.tasks.translation | [deu.random.input] dictionary: 112 types\n",
            "2024-01-01 17:04:31 | INFO | fairseq.tasks.translation | [deu.random.output] dictionary: 88 types\n",
            "2024-01-01 17:04:31 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(112, 256, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-3): 4 x TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(88, 256, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-3): 4 x TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=256, out_features=88, bias=False)\n",
            "  )\n",
            ")\n",
            "2024-01-01 17:04:31 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2024-01-01 17:04:31 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2024-01-01 17:04:31 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2024-01-01 17:04:31 | INFO | fairseq_cli.train | num. shared model params: 10,579,968 (num. trained: 10,579,968)\n",
            "2024-01-01 17:04:31 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2024-01-01 17:04:31 | INFO | fairseq.data.data_utils | loaded 51,914 examples from: data-bin/deu/valid.deu.random.input-deu.random.output.deu.random.input\n",
            "2024-01-01 17:04:31 | INFO | fairseq.data.data_utils | loaded 51,914 examples from: data-bin/deu/valid.deu.random.input-deu.random.output.deu.random.output\n",
            "2024-01-01 17:04:31 | INFO | fairseq.tasks.translation | data-bin/deu valid deu.random.input-deu.random.output 51914 examples\n",
            "2024-01-01 17:04:31 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2024-01-01 17:04:31 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-01-01 17:04:31 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2024-01-01 17:04:31 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-01-01 17:04:31 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2024-01-01 17:04:31 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 400\n",
            "2024-01-01 17:04:31 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/deu-models/checkpoint_last.pt\n",
            "2024-01-01 17:04:31 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/deu-models/checkpoint_last.pt\n",
            "2024-01-01 17:04:31 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2024-01-01 17:04:31 | INFO | fairseq.data.data_utils | loaded 415,314 examples from: data-bin/deu/train.deu.random.input-deu.random.output.deu.random.input\n",
            "2024-01-01 17:04:31 | INFO | fairseq.data.data_utils | loaded 415,314 examples from: data-bin/deu/train.deu.random.input-deu.random.output.deu.random.output\n",
            "2024-01-01 17:04:31 | INFO | fairseq.tasks.translation | data-bin/deu train deu.random.input-deu.random.output 415314 examples\n",
            "2024-01-01 17:04:31 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2024-01-01 17:04:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1040\n",
            "epoch 001:   0% 0/1040 [00:00<?, ?it/s]2024-01-01 17:04:31 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2024-01-01 17:04:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001: 100% 1039/1040 [02:20<00:00,  7.24it/s, loss=1.539, nll_loss=0.608, ppl=1.52, wps=33344.1, ups=7.22, wpb=4618.9, bsz=400, num_updates=1000, lr=0.001, gnorm=0.602, clip=7, train_wall=13, gb_free=13.7, wall=136]2024-01-01 17:06:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/131 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   1% 1/131 [00:00<00:15,  8.49it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   3% 4/131 [00:00<00:07, 17.83it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   5% 7/131 [00:00<00:06, 20.06it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   7% 9/131 [00:00<00:06, 19.88it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   8% 11/131 [00:00<00:06, 18.39it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  11% 14/131 [00:00<00:05, 21.27it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  13% 17/131 [00:00<00:05, 21.23it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  15% 20/131 [00:00<00:05, 21.23it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  18% 23/131 [00:01<00:05, 21.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  20% 26/131 [00:01<00:04, 21.36it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  22% 29/131 [00:01<00:04, 21.45it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  24% 32/131 [00:01<00:04, 20.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  27% 35/131 [00:01<00:04, 21.43it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  29% 38/131 [00:01<00:04, 21.31it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  31% 41/131 [00:01<00:04, 21.08it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  34% 44/131 [00:02<00:04, 20.28it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  36% 47/131 [00:02<00:04, 20.10it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  38% 50/131 [00:02<00:03, 20.64it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  40% 53/131 [00:02<00:03, 19.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  42% 55/131 [00:02<00:04, 18.10it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  44% 58/131 [00:02<00:03, 18.46it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  46% 60/131 [00:03<00:03, 18.15it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  47% 62/131 [00:03<00:03, 17.43it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  49% 64/131 [00:03<00:04, 16.60it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 66/131 [00:03<00:03, 16.68it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  52% 68/131 [00:03<00:03, 16.41it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  53% 70/131 [00:03<00:03, 16.55it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  55% 72/131 [00:03<00:03, 16.37it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  56% 74/131 [00:03<00:03, 16.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  58% 76/131 [00:04<00:03, 16.17it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  60% 78/131 [00:04<00:03, 16.03it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  61% 80/131 [00:04<00:03, 15.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  63% 82/131 [00:04<00:03, 15.77it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  64% 84/131 [00:04<00:03, 15.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  66% 86/131 [00:04<00:02, 16.36it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  67% 88/131 [00:04<00:02, 16.12it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  69% 90/131 [00:04<00:02, 16.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  70% 92/131 [00:05<00:02, 15.91it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  72% 94/131 [00:05<00:02, 15.50it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  73% 96/131 [00:05<00:02, 15.02it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  75% 98/131 [00:05<00:02, 14.76it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  76% 100/131 [00:05<00:02, 14.68it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  78% 102/131 [00:05<00:01, 14.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  79% 104/131 [00:06<00:03,  7.47it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  81% 106/131 [00:06<00:02,  8.82it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  82% 108/131 [00:06<00:02,  9.82it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  84% 110/131 [00:06<00:01, 11.07it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  85% 112/131 [00:06<00:01, 11.81it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  87% 114/131 [00:06<00:01, 12.41it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  89% 116/131 [00:07<00:01, 12.75it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  90% 118/131 [00:07<00:00, 13.29it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  92% 120/131 [00:07<00:00, 13.30it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  93% 122/131 [00:07<00:00, 14.19it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  95% 124/131 [00:07<00:00, 13.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  96% 126/131 [00:07<00:00, 14.64it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  98% 128/131 [00:07<00:00, 14.60it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  99% 130/131 [00:08<00:00, 13.73it/s]\u001b[A\n",
            "                                                                          \u001b[A2024-01-01 17:07:00 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 1.365 | nll_loss 0.283 | ppl 1.22 | wps 73954.5 | wpb 4542.2 | bsz 396.3 | num_updates 1040\n",
            "2024-01-01 17:07:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1040 updates\n",
            "2024-01-01 17:07:00 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:07:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:07:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/deu-models/checkpoint_best.pt (epoch 1 @ 1040 updates, score 1.365) (writing took 0.9154683770011616 seconds)\n",
            "2024-01-01 17:07:01 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2024-01-01 17:07:01 | INFO | train | epoch 001 | loss 2.748 | nll_loss 1.991 | ppl 3.97 | wps 32058.9 | ups 7 | wpb 4576.6 | bsz 399.3 | num_updates 1040 | lr 0.000980581 | gnorm 1.179 | clip 43 | train_wall 135 | gb_free 12.7 | wall 150\n",
            "2024-01-01 17:07:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1040\n",
            "epoch 002:   0% 0/1040 [00:00<?, ?it/s]2024-01-01 17:07:01 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2024-01-01 17:07:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002: 100% 1038/1040 [02:22<00:00,  7.74it/s, loss=1.255, nll_loss=0.302, ppl=1.23, wps=34526.2, ups=7.19, wpb=4803.8, bsz=400, num_updates=2000, lr=0.000707107, gnorm=0.282, clip=0, train_wall=13, gb_free=13.5, wall=282]2024-01-01 17:09:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/131 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   2% 2/131 [00:00<00:08, 15.48it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   4% 5/131 [00:00<00:06, 19.43it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   6% 8/131 [00:00<00:05, 20.59it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   8% 11/131 [00:00<00:05, 20.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  11% 14/131 [00:00<00:05, 21.28it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  13% 17/131 [00:00<00:05, 21.09it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  15% 20/131 [00:00<00:05, 20.91it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  18% 23/131 [00:01<00:05, 20.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  20% 26/131 [00:01<00:04, 21.38it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  22% 29/131 [00:01<00:04, 20.99it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  24% 32/131 [00:01<00:04, 20.66it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  27% 35/131 [00:01<00:04, 21.41it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  29% 38/131 [00:01<00:04, 21.43it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  31% 41/131 [00:01<00:04, 21.08it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  34% 44/131 [00:02<00:04, 20.11it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  36% 47/131 [00:02<00:04, 19.71it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  38% 50/131 [00:02<00:03, 20.65it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  40% 53/131 [00:02<00:03, 20.53it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  43% 56/131 [00:02<00:03, 21.00it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  45% 59/131 [00:02<00:03, 20.82it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  47% 62/131 [00:02<00:03, 21.11it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 65/131 [00:03<00:03, 21.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  52% 68/131 [00:03<00:02, 21.23it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  54% 71/131 [00:03<00:02, 21.30it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  56% 74/131 [00:03<00:02, 21.17it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  59% 77/131 [00:03<00:02, 21.23it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  61% 80/131 [00:03<00:02, 21.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  63% 83/131 [00:03<00:02, 20.66it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  66% 86/131 [00:04<00:02, 20.39it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  68% 89/131 [00:04<00:02, 18.73it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  70% 92/131 [00:04<00:02, 19.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  72% 94/131 [00:04<00:01, 19.16it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  74% 97/131 [00:04<00:01, 19.66it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  76% 100/131 [00:04<00:01, 20.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  79% 103/131 [00:04<00:01, 20.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  81% 106/131 [00:05<00:01, 20.38it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  83% 109/131 [00:05<00:01, 19.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  85% 112/131 [00:05<00:00, 20.78it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  88% 115/131 [00:05<00:00, 19.88it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  90% 118/131 [00:05<00:00, 17.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  92% 120/131 [00:05<00:00, 17.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  93% 122/131 [00:06<00:00, 16.64it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  95% 124/131 [00:06<00:00, 15.15it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  96% 126/131 [00:06<00:00, 14.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  98% 128/131 [00:06<00:00, 14.24it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  99% 130/131 [00:06<00:00, 13.11it/s]\u001b[A\n",
            "                                                                          \u001b[A2024-01-01 17:09:31 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 1.221 | nll_loss 0.165 | ppl 1.12 | wps 88980.7 | wpb 4542.2 | bsz 396.3 | num_updates 2080 | best_loss 1.221\n",
            "2024-01-01 17:09:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2080 updates\n",
            "2024-01-01 17:09:31 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:09:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:09:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/deu-models/checkpoint_best.pt (epoch 2 @ 2080 updates, score 1.221) (writing took 1.2488301529992896 seconds)\n",
            "2024-01-01 17:09:32 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2024-01-01 17:09:32 | INFO | train | epoch 002 | loss 1.345 | nll_loss 0.4 | ppl 1.32 | wps 31506.1 | ups 6.88 | wpb 4576.6 | bsz 399.3 | num_updates 2080 | lr 0.000693375 | gnorm 0.39 | clip 2.2 | train_wall 137 | gb_free 13.9 | wall 301\n",
            "2024-01-01 17:09:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1040\n",
            "epoch 003:   0% 0/1040 [00:00<?, ?it/s]2024-01-01 17:09:32 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2024-01-01 17:09:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003: 100% 1039/1040 [02:24<00:00,  7.25it/s, loss=1.211, nll_loss=0.256, ppl=1.19, wps=33254.7, ups=7.21, wpb=4612.1, bsz=400, num_updates=3100, lr=0.000567962, gnorm=0.241, clip=0, train_wall=13, gb_free=13.7, wall=443]2024-01-01 17:11:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/131 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   2% 2/131 [00:00<00:08, 15.52it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   4% 5/131 [00:00<00:06, 19.06it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   6% 8/131 [00:00<00:06, 19.85it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   8% 10/131 [00:00<00:06, 19.48it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  10% 13/131 [00:00<00:05, 20.49it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  12% 16/131 [00:00<00:05, 20.76it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  15% 19/131 [00:00<00:05, 20.36it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  17% 22/131 [00:01<00:05, 21.05it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  19% 25/131 [00:01<00:04, 21.52it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  21% 28/131 [00:01<00:04, 20.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  24% 31/131 [00:01<00:04, 20.68it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  26% 34/131 [00:01<00:04, 20.78it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  28% 37/131 [00:01<00:04, 20.91it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  31% 40/131 [00:01<00:04, 20.43it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  33% 43/131 [00:02<00:04, 20.64it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  35% 46/131 [00:02<00:04, 20.65it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  37% 49/131 [00:02<00:04, 19.89it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  40% 52/131 [00:02<00:03, 20.06it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  42% 55/131 [00:02<00:03, 20.08it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  44% 58/131 [00:02<00:03, 20.33it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  47% 61/131 [00:02<00:03, 20.10it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  49% 64/131 [00:03<00:03, 19.75it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  51% 67/131 [00:03<00:03, 20.60it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  53% 70/131 [00:03<00:03, 20.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  56% 73/131 [00:03<00:02, 20.22it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  58% 76/131 [00:03<00:02, 20.82it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  60% 79/131 [00:03<00:02, 20.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  63% 82/131 [00:04<00:02, 20.47it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  65% 85/131 [00:04<00:02, 20.79it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  67% 88/131 [00:04<00:02, 21.02it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  69% 91/131 [00:04<00:01, 20.51it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  72% 94/131 [00:04<00:01, 20.39it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  74% 97/131 [00:04<00:01, 20.28it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  76% 100/131 [00:04<00:01, 20.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  79% 103/131 [00:05<00:01, 20.41it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  81% 106/131 [00:05<00:01, 20.83it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  83% 109/131 [00:05<00:01, 20.92it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  85% 112/131 [00:05<00:00, 20.48it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  88% 115/131 [00:05<00:00, 19.56it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  89% 117/131 [00:05<00:00, 19.54it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  92% 120/131 [00:05<00:00, 19.86it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  93% 122/131 [00:06<00:00, 19.57it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  95% 124/131 [00:06<00:00, 19.16it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  96% 126/131 [00:06<00:00, 18.51it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  98% 128/131 [00:06<00:00, 17.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  99% 130/131 [00:06<00:00, 15.66it/s]\u001b[A\n",
            "                                                                          \u001b[A2024-01-01 17:12:03 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 1.199 | nll_loss 0.141 | ppl 1.1 | wps 91585.1 | wpb 4542.2 | bsz 396.3 | num_updates 3120 | best_loss 1.199\n",
            "2024-01-01 17:12:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 3120 updates\n",
            "2024-01-01 17:12:03 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:12:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:12:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/deu-models/checkpoint_best.pt (epoch 3 @ 3120 updates, score 1.199) (writing took 0.937540060000174 seconds)\n",
            "2024-01-01 17:12:04 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2024-01-01 17:12:04 | INFO | train | epoch 003 | loss 1.237 | nll_loss 0.284 | ppl 1.22 | wps 31325.2 | ups 6.84 | wpb 4576.6 | bsz 399.3 | num_updates 3120 | lr 0.000566139 | gnorm 0.273 | clip 0.3 | train_wall 139 | gb_free 13.7 | wall 453\n",
            "2024-01-01 17:12:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1040\n",
            "epoch 004:   0% 0/1040 [00:00<?, ?it/s]2024-01-01 17:12:04 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2024-01-01 17:12:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004: 100% 1039/1040 [02:24<00:00,  7.26it/s, loss=1.197, nll_loss=0.241, ppl=1.18, wps=30405.4, ups=6.82, wpb=4457.3, bsz=400, num_updates=4100, lr=0.000493865, gnorm=0.227, clip=0, train_wall=14, gb_free=13.6, wall=590]2024-01-01 17:14:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/131 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   2% 2/131 [00:00<00:10, 12.81it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   3% 4/131 [00:00<00:08, 15.51it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   5% 6/131 [00:00<00:07, 15.68it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   6% 8/131 [00:00<00:08, 14.79it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   8% 10/131 [00:00<00:08, 14.40it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   9% 12/131 [00:00<00:07, 15.11it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  11% 14/131 [00:00<00:07, 14.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  12% 16/131 [00:01<00:08, 13.90it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  15% 19/131 [00:01<00:07, 14.59it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  16% 21/131 [00:01<00:07, 15.68it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  18% 23/131 [00:01<00:06, 15.74it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  19% 25/131 [00:01<00:06, 15.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  21% 27/131 [00:01<00:06, 15.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  22% 29/131 [00:01<00:06, 15.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  24% 31/131 [00:02<00:06, 16.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  25% 33/131 [00:02<00:05, 16.61it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  27% 35/131 [00:02<00:05, 17.46it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  28% 37/131 [00:02<00:05, 16.78it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  30% 39/131 [00:02<00:05, 17.09it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  31% 41/131 [00:02<00:05, 15.83it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  33% 43/131 [00:02<00:05, 15.32it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  34% 45/131 [00:02<00:05, 14.64it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  36% 47/131 [00:03<00:05, 14.63it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  37% 49/131 [00:03<00:05, 14.49it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  39% 51/131 [00:03<00:05, 14.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  40% 53/131 [00:03<00:05, 14.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  42% 55/131 [00:03<00:05, 14.92it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  44% 57/131 [00:03<00:04, 15.45it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  45% 59/131 [00:03<00:04, 16.55it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  47% 61/131 [00:03<00:04, 16.14it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  48% 63/131 [00:04<00:03, 17.03it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  50% 66/131 [00:04<00:03, 18.67it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  53% 69/131 [00:04<00:03, 19.25it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  54% 71/131 [00:04<00:03, 19.27it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  56% 74/131 [00:04<00:02, 19.54it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  58% 76/131 [00:04<00:02, 18.64it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  60% 79/131 [00:04<00:02, 19.79it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  62% 81/131 [00:04<00:02, 19.70it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  63% 83/131 [00:05<00:02, 19.22it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  66% 86/131 [00:05<00:02, 20.89it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  68% 89/131 [00:05<00:02, 20.23it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  70% 92/131 [00:05<00:01, 19.84it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  73% 95/131 [00:05<00:01, 20.98it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  75% 98/131 [00:05<00:01, 19.60it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  77% 101/131 [00:05<00:01, 20.67it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  79% 104/131 [00:06<00:01, 21.09it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  82% 107/131 [00:06<00:01, 20.88it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  84% 110/131 [00:06<00:01, 20.89it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  86% 113/131 [00:06<00:00, 20.29it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  89% 116/131 [00:06<00:00, 20.29it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  91% 119/131 [00:06<00:00, 20.17it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  93% 122/131 [00:06<00:00, 19.76it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  95% 124/131 [00:07<00:00, 19.22it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  96% 126/131 [00:07<00:00, 18.74it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  98% 128/131 [00:07<00:00, 18.08it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  99% 130/131 [00:07<00:00, 15.96it/s]\u001b[A\n",
            "                                                                          \u001b[A2024-01-01 17:14:36 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 1.187 | nll_loss 0.134 | ppl 1.1 | wps 80123.6 | wpb 4542.2 | bsz 396.3 | num_updates 4160 | best_loss 1.187\n",
            "2024-01-01 17:14:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 4160 updates\n",
            "2024-01-01 17:14:36 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:14:37 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:14:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/deu-models/checkpoint_best.pt (epoch 4 @ 4160 updates, score 1.187) (writing took 1.0103511690012965 seconds)\n",
            "2024-01-01 17:14:37 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2024-01-01 17:14:37 | INFO | train | epoch 004 | loss 1.209 | nll_loss 0.254 | ppl 1.19 | wps 31129.6 | ups 6.8 | wpb 4576.6 | bsz 399.3 | num_updates 4160 | lr 0.00049029 | gnorm 0.242 | clip 0.5 | train_wall 139 | gb_free 13.6 | wall 606\n",
            "2024-01-01 17:14:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1040\n",
            "epoch 005:   0% 0/1040 [00:00<?, ?it/s]2024-01-01 17:14:37 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2024-01-01 17:14:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005: 100% 1039/1040 [02:24<00:00,  8.29it/s, loss=1.198, nll_loss=0.245, ppl=1.19, wps=33132.8, ups=6.92, wpb=4788.5, bsz=396, num_updates=5100, lr=0.000442807, gnorm=0.272, clip=1, train_wall=14, gb_free=13.5, wall=737]2024-01-01 17:17:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/131 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   2% 2/131 [00:00<00:07, 16.63it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   4% 5/131 [00:00<00:06, 19.23it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   5% 7/131 [00:00<00:06, 18.96it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   8% 10/131 [00:00<00:06, 19.61it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  10% 13/131 [00:00<00:05, 20.49it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  12% 16/131 [00:00<00:05, 20.45it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  15% 19/131 [00:00<00:05, 19.78it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  17% 22/131 [00:01<00:05, 20.82it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  19% 25/131 [00:01<00:05, 20.71it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  21% 28/131 [00:01<00:05, 20.34it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  24% 31/131 [00:01<00:04, 20.34it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  26% 34/131 [00:01<00:05, 19.22it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  27% 36/131 [00:01<00:05, 18.81it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  29% 38/131 [00:01<00:04, 18.98it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  31% 40/131 [00:02<00:04, 18.99it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  32% 42/131 [00:02<00:04, 17.89it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  34% 44/131 [00:02<00:05, 16.54it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  35% 46/131 [00:02<00:05, 15.86it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  37% 48/131 [00:02<00:05, 15.48it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  38% 50/131 [00:02<00:05, 16.16it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  40% 52/131 [00:02<00:05, 15.58it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  41% 54/131 [00:02<00:05, 15.29it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  43% 56/131 [00:03<00:04, 15.24it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  44% 58/131 [00:03<00:04, 15.07it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  46% 60/131 [00:03<00:04, 14.86it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  47% 62/131 [00:03<00:04, 14.67it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  49% 64/131 [00:03<00:04, 14.35it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  50% 66/131 [00:03<00:04, 14.59it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  52% 68/131 [00:03<00:04, 14.60it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  53% 70/131 [00:04<00:04, 14.74it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  55% 72/131 [00:04<00:03, 14.77it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  56% 74/131 [00:04<00:03, 14.77it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  58% 76/131 [00:04<00:03, 14.27it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  60% 78/131 [00:04<00:03, 14.75it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  61% 80/131 [00:04<00:03, 15.51it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  63% 82/131 [00:04<00:02, 16.53it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  64% 84/131 [00:04<00:03, 15.61it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  66% 86/131 [00:05<00:03, 14.62it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  67% 88/131 [00:05<00:02, 14.89it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  69% 90/131 [00:05<00:02, 15.86it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  70% 92/131 [00:05<00:02, 15.87it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  72% 94/131 [00:05<00:02, 16.21it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  73% 96/131 [00:05<00:02, 14.98it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  75% 98/131 [00:05<00:02, 15.22it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  76% 100/131 [00:06<00:02, 15.18it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  78% 102/131 [00:06<00:01, 15.90it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  79% 104/131 [00:06<00:01, 15.89it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  81% 106/131 [00:06<00:01, 16.55it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  82% 108/131 [00:06<00:01, 16.17it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  84% 110/131 [00:06<00:01, 15.63it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  85% 112/131 [00:06<00:01, 14.48it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  87% 114/131 [00:06<00:01, 14.73it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  89% 116/131 [00:07<00:01, 14.70it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  90% 118/131 [00:07<00:00, 15.17it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  92% 120/131 [00:07<00:00, 15.75it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  93% 122/131 [00:07<00:00, 14.45it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  95% 124/131 [00:07<00:00, 15.14it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  96% 126/131 [00:07<00:00, 15.39it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  98% 128/131 [00:07<00:00, 15.59it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  99% 130/131 [00:08<00:00, 14.39it/s]\u001b[A\n",
            "                                                                          \u001b[A2024-01-01 17:17:10 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 1.183 | nll_loss 0.126 | ppl 1.09 | wps 74658.9 | wpb 4542.2 | bsz 396.3 | num_updates 5200 | best_loss 1.183\n",
            "2024-01-01 17:17:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 5200 updates\n",
            "2024-01-01 17:17:10 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:17:10 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:17:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/deu-models/checkpoint_best.pt (epoch 5 @ 5200 updates, score 1.183) (writing took 1.020268024998586 seconds)\n",
            "2024-01-01 17:17:11 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2024-01-01 17:17:11 | INFO | train | epoch 005 | loss 1.194 | nll_loss 0.238 | ppl 1.18 | wps 30965.8 | ups 6.77 | wpb 4576.6 | bsz 399.3 | num_updates 5200 | lr 0.000438529 | gnorm 0.22 | clip 0.4 | train_wall 139 | gb_free 13.5 | wall 760\n",
            "2024-01-01 17:17:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1040\n",
            "epoch 006:   0% 0/1040 [00:00<?, ?it/s]2024-01-01 17:17:11 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2024-01-01 17:17:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006:  77% 799/1040 [01:51<00:32,  7.49it/s, loss=1.188, nll_loss=0.233, ppl=1.18, wps=33234.3, ups=7.12, wpb=4669.2, bsz=400, num_updates=5900, lr=0.000411693, gnorm=0.208, clip=0, train_wall=13, gb_free=14, wall=857]2024-01-01 17:19:02 | INFO | fairseq_cli.train | Stopping training due to num_updates: 6000 >= max_update: 6000\n",
            "2024-01-01 17:19:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/131 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   2% 2/131 [00:00<00:08, 15.63it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   4% 5/131 [00:00<00:06, 18.28it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   6% 8/131 [00:00<00:06, 19.55it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   8% 10/131 [00:00<00:06, 18.89it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  10% 13/131 [00:00<00:05, 20.33it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  12% 16/131 [00:00<00:05, 19.54it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  14% 18/131 [00:00<00:05, 19.53it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  16% 21/131 [00:01<00:05, 19.96it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  18% 23/131 [00:01<00:05, 19.88it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  20% 26/131 [00:01<00:05, 20.07it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  22% 29/131 [00:01<00:05, 19.87it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  24% 32/131 [00:01<00:05, 19.73it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  27% 35/131 [00:01<00:05, 19.17it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  29% 38/131 [00:01<00:04, 19.44it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  31% 41/131 [00:02<00:04, 19.64it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  33% 43/131 [00:02<00:04, 19.61it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  35% 46/131 [00:02<00:04, 19.77it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  37% 48/131 [00:02<00:04, 19.60it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  39% 51/131 [00:02<00:04, 19.99it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  41% 54/131 [00:02<00:03, 20.03it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  43% 56/131 [00:02<00:03, 19.84it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  44% 58/131 [00:02<00:03, 19.77it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  47% 61/131 [00:03<00:03, 19.73it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  48% 63/131 [00:03<00:03, 19.65it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  50% 66/131 [00:03<00:03, 20.12it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  53% 69/131 [00:03<00:03, 20.32it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  55% 72/131 [00:03<00:02, 20.47it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  57% 75/131 [00:03<00:02, 19.58it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  59% 77/131 [00:03<00:02, 19.66it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  61% 80/131 [00:04<00:02, 19.92it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  63% 82/131 [00:04<00:02, 19.77it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  64% 84/131 [00:04<00:02, 19.75it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  66% 86/131 [00:04<00:02, 19.60it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  68% 89/131 [00:04<00:02, 20.29it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  70% 92/131 [00:04<00:01, 20.06it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  73% 95/131 [00:04<00:01, 20.65it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  75% 98/131 [00:04<00:01, 20.29it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  77% 101/131 [00:05<00:01, 19.58it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  79% 103/131 [00:05<00:01, 19.49it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  80% 105/131 [00:05<00:01, 18.16it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  82% 108/131 [00:05<00:01, 19.03it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  85% 111/131 [00:05<00:00, 20.24it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  87% 114/131 [00:05<00:00, 19.44it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  89% 116/131 [00:05<00:00, 19.33it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  91% 119/131 [00:06<00:00, 19.81it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  92% 121/131 [00:06<00:00, 19.45it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  94% 123/131 [00:06<00:00, 19.40it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  95% 125/131 [00:06<00:00, 18.57it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  97% 127/131 [00:06<00:00, 17.91it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  98% 129/131 [00:06<00:00, 16.82it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset: 100% 131/131 [00:06<00:00, 17.35it/s]\u001b[A\n",
            "                                                                          \u001b[A2024-01-01 17:19:09 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 1.179 | nll_loss 0.121 | ppl 1.09 | wps 88907.7 | wpb 4542.2 | bsz 396.3 | num_updates 6000 | best_loss 1.179\n",
            "2024-01-01 17:19:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 6000 updates\n",
            "2024-01-01 17:19:09 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:19:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:19:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/deu-models/checkpoint_best.pt (epoch 6 @ 6000 updates, score 1.179) (writing took 1.2323969890003355 seconds)\n",
            "2024-01-01 17:19:10 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2024-01-01 17:19:10 | INFO | train | epoch 006 | loss 1.184 | nll_loss 0.228 | ppl 1.17 | wps 30613.4 | ups 6.7 | wpb 4568.7 | bsz 399.1 | num_updates 6000 | lr 0.000408248 | gnorm 0.211 | clip 0.2 | train_wall 107 | gb_free 13.8 | wall 879\n",
            "2024-01-01 17:19:10 | INFO | fairseq_cli.train | done training in 878.7 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!chmod 777 test.sh\n",
        "!./test.sh deu random"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyvtNRp1wHD9",
        "outputId": "3a337e18-607f-47cf-dadf-304fd4b8d5c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-01 17:22:39.526797: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-01 17:22:39.526855: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-01 17:22:39.528283: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-01 17:22:39.536080: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-01 17:22:40.611318: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-01 17:22:44 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/deu-models/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin/deu/', 'source_lang': 'deu.random.input', 'target_lang': 'deu.random.output', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-01-01 17:22:44 | INFO | fairseq.tasks.translation | [deu.random.input] dictionary: 112 types\n",
            "2024-01-01 17:22:44 | INFO | fairseq.tasks.translation | [deu.random.output] dictionary: 88 types\n",
            "2024-01-01 17:22:44 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:22:45 | INFO | fairseq.data.data_utils | loaded 51,915 examples from: data-bin/deu/test.deu.random.input-deu.random.output.deu.random.input\n",
            "2024-01-01 17:22:45 | INFO | fairseq.data.data_utils | loaded 51,915 examples from: data-bin/deu/test.deu.random.input-deu.random.output.deu.random.output\n",
            "2024-01-01 17:22:45 | INFO | fairseq.tasks.translation | data-bin/deu/ test deu.random.input-deu.random.output 51915 examples\n",
            "2024-01-01 17:27:27 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2024-01-01 17:27:27 | INFO | fairseq_cli.generate | Translated 51,915 sentences (591,426 tokens) in 68.6s (756.89 sentences/s, 8622.68 tokens/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### No overlap"
      ],
      "metadata": {
        "id": "ciu-M6-dv5XI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash ./preprocess.sh deu no_overlap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRrcyv7QwLwP",
        "outputId": "90f83aba-26c9-4be3-ed2b-e5c18955ac7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-01 17:31:27.984584: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-01 17:31:27.984712: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-01 17:31:27.986645: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-01 17:31:27.997301: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-01 17:31:29.555818: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-01 17:31:32 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer='space', bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='deu.no_overlap.input', target_lang='deu.no_overlap.output', trainpref='train', validpref='devel', testpref='test', align_suffix=None, destdir='data-bin/deu', thresholdtgt=5, thresholdsrc=5, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=1, dict_only=False)\n",
            "2024-01-01 17:31:56 | INFO | fairseq_cli.preprocess | [deu.no_overlap.input] Dictionary: 112 types\n",
            "2024-01-01 17:32:46 | INFO | fairseq_cli.preprocess | [deu.no_overlap.input] train.deu.no_overlap.input: 414720 sents, 6529167 tokens, 6.13e-05% replaced (by <unk>)\n",
            "2024-01-01 17:32:46 | INFO | fairseq_cli.preprocess | [deu.no_overlap.input] Dictionary: 112 types\n",
            "2024-01-01 17:32:52 | INFO | fairseq_cli.preprocess | [deu.no_overlap.input] devel.deu.no_overlap.input: 52898 sents, 833121 tokens, 0.00288% replaced (by <unk>)\n",
            "2024-01-01 17:32:52 | INFO | fairseq_cli.preprocess | [deu.no_overlap.input] Dictionary: 112 types\n",
            "2024-01-01 17:32:58 | INFO | fairseq_cli.preprocess | [deu.no_overlap.input] test.deu.no_overlap.input: 51525 sents, 816021 tokens, 0.00098% replaced (by <unk>)\n",
            "2024-01-01 17:32:58 | INFO | fairseq_cli.preprocess | [deu.no_overlap.output] Dictionary: 88 types\n",
            "2024-01-01 17:33:39 | INFO | fairseq_cli.preprocess | [deu.no_overlap.output] train.deu.no_overlap.output: 414720 sents, 4748709 tokens, 0.000168% replaced (by <unk>)\n",
            "2024-01-01 17:33:39 | INFO | fairseq_cli.preprocess | [deu.no_overlap.output] Dictionary: 88 types\n",
            "2024-01-01 17:33:45 | INFO | fairseq_cli.preprocess | [deu.no_overlap.output] devel.deu.no_overlap.output: 52898 sents, 603969 tokens, 0.00397% replaced (by <unk>)\n",
            "2024-01-01 17:33:45 | INFO | fairseq_cli.preprocess | [deu.no_overlap.output] Dictionary: 88 types\n",
            "2024-01-01 17:33:50 | INFO | fairseq_cli.preprocess | [deu.no_overlap.output] test.deu.no_overlap.output: 51525 sents, 596058 tokens, 0.0% replaced (by <unk>)\n",
            "2024-01-01 17:33:50 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/deu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r checkpoints/deu-models/\n",
        "!bash ./train.sh deu no_overlap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsdkYZCxwMue",
        "outputId": "9aa4e52c-3da6-444b-8911-f1a65db37757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-01 17:38:29.228811: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-01 17:38:29.228866: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-01 17:38:29.230198: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-01 17:38:29.237563: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-01 17:38:30.455744: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-01 17:38:31 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2024-01-01 17:38:34 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 212, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 400, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 400, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 6000, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/deu-models', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=212, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=400, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=400, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=0, max_update=6000, stop_time_hours=0, clip_norm=1.0, sentence_avg=False, update_freq=[1], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/deu-models', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-bin/deu', source_lang='deu.no_overlap.input', target_lang='deu.no_overlap.output', load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=1000, warmup_init_lr=-1, pad=1, eos=2, unk=3, dropout=0.3, attention_dropout=0.3, activation_dropout=0.3, activation_fn='relu', encoder_embed_dim=256, encoder_ffn_embed_dim=1024, encoder_layers=4, encoder_attention_heads=4, encoder_normalize_before=True, decoder_embed_dim=256, decoder_ffn_embed_dim=1024, decoder_layers=4, decoder_attention_heads=4, decoder_normalize_before=True, share_decoder_input_output_embed=True, no_seed_provided=False, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=256, decoder_input_dim=256, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'data-bin/deu', 'source_lang': 'deu.no_overlap.input', 'target_lang': 'deu.no_overlap.output', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 1000, 'warmup_init_lr': -1.0, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-01-01 17:38:34 | INFO | fairseq.tasks.translation | [deu.no_overlap.input] dictionary: 112 types\n",
            "2024-01-01 17:38:34 | INFO | fairseq.tasks.translation | [deu.no_overlap.output] dictionary: 88 types\n",
            "2024-01-01 17:38:34 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(112, 256, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-3): 4 x TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(88, 256, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-3): 4 x TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=256, out_features=88, bias=False)\n",
            "  )\n",
            ")\n",
            "2024-01-01 17:38:34 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2024-01-01 17:38:34 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2024-01-01 17:38:34 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2024-01-01 17:38:34 | INFO | fairseq_cli.train | num. shared model params: 10,579,968 (num. trained: 10,579,968)\n",
            "2024-01-01 17:38:34 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2024-01-01 17:38:34 | INFO | fairseq.data.data_utils | loaded 52,898 examples from: data-bin/deu/valid.deu.no_overlap.input-deu.no_overlap.output.deu.no_overlap.input\n",
            "2024-01-01 17:38:34 | INFO | fairseq.data.data_utils | loaded 52,898 examples from: data-bin/deu/valid.deu.no_overlap.input-deu.no_overlap.output.deu.no_overlap.output\n",
            "2024-01-01 17:38:34 | INFO | fairseq.tasks.translation | data-bin/deu valid deu.no_overlap.input-deu.no_overlap.output 52898 examples\n",
            "2024-01-01 17:38:34 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2024-01-01 17:38:34 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-01-01 17:38:34 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2024-01-01 17:38:34 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-01-01 17:38:34 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2024-01-01 17:38:34 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 400\n",
            "2024-01-01 17:38:34 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/deu-models/checkpoint_last.pt\n",
            "2024-01-01 17:38:34 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/deu-models/checkpoint_last.pt\n",
            "2024-01-01 17:38:34 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2024-01-01 17:38:34 | INFO | fairseq.data.data_utils | loaded 414,720 examples from: data-bin/deu/train.deu.no_overlap.input-deu.no_overlap.output.deu.no_overlap.input\n",
            "2024-01-01 17:38:34 | INFO | fairseq.data.data_utils | loaded 414,720 examples from: data-bin/deu/train.deu.no_overlap.input-deu.no_overlap.output.deu.no_overlap.output\n",
            "2024-01-01 17:38:34 | INFO | fairseq.tasks.translation | data-bin/deu train deu.no_overlap.input-deu.no_overlap.output 414720 examples\n",
            "2024-01-01 17:38:35 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2024-01-01 17:38:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1037\n",
            "epoch 001:   0% 0/1037 [00:00<?, ?it/s]2024-01-01 17:38:35 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2024-01-01 17:38:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001:   1% 13/1037 [00:03<02:38,  6.48it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "epoch 001: 100% 1036/1037 [02:23<00:00,  7.35it/s, loss=1.512, nll_loss=0.578, ppl=1.49, wps=33623.5, ups=7.31, wpb=4599.8, bsz=400, num_updates=1000, lr=0.001, gnorm=0.594, clip=8, train_wall=13, gb_free=13.9, wall=139]2024-01-01 17:40:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/134 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   1% 1/134 [00:00<00:30,  4.39it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   3% 4/134 [00:00<00:09, 13.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   5% 7/134 [00:00<00:08, 15.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   7% 9/134 [00:00<00:08, 15.25it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   8% 11/134 [00:00<00:08, 15.29it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  10% 13/134 [00:00<00:07, 16.25it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  12% 16/134 [00:01<00:06, 17.92it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  13% 18/134 [00:01<00:07, 16.51it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  15% 20/134 [00:01<00:06, 16.38it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  17% 23/134 [00:01<00:06, 17.57it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  19% 25/134 [00:01<00:06, 17.10it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  20% 27/134 [00:01<00:06, 16.11it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  22% 29/134 [00:01<00:06, 15.86it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  23% 31/134 [00:01<00:06, 15.40it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  25% 33/134 [00:02<00:06, 16.08it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  27% 36/134 [00:02<00:05, 17.45it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  28% 38/134 [00:02<00:05, 16.43it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  30% 40/134 [00:02<00:05, 16.06it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  31% 42/134 [00:02<00:05, 15.82it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  33% 44/134 [00:02<00:05, 15.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  34% 46/134 [00:02<00:06, 14.65it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  36% 48/134 [00:03<00:05, 14.74it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  37% 50/134 [00:03<00:05, 15.04it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  39% 52/134 [00:03<00:05, 14.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  40% 54/134 [00:03<00:05, 15.44it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  42% 56/134 [00:03<00:05, 15.42it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  43% 58/134 [00:03<00:05, 14.88it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  45% 60/134 [00:03<00:04, 15.51it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  46% 62/134 [00:03<00:04, 16.48it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  48% 64/134 [00:04<00:04, 15.88it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  49% 66/134 [00:04<00:04, 16.39it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  51% 68/134 [00:04<00:04, 15.31it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  52% 70/134 [00:04<00:03, 16.00it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  54% 72/134 [00:04<00:03, 16.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  55% 74/134 [00:04<00:03, 17.09it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  57% 76/134 [00:04<00:03, 17.08it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  58% 78/134 [00:04<00:03, 16.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  60% 81/134 [00:05<00:03, 16.02it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  62% 83/134 [00:05<00:03, 16.56it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  63% 85/134 [00:05<00:02, 16.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  66% 88/134 [00:05<00:02, 18.65it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  67% 90/134 [00:05<00:02, 18.93it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  69% 92/134 [00:05<00:02, 18.83it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  71% 95/134 [00:05<00:01, 19.69it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  72% 97/134 [00:05<00:01, 19.19it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  75% 100/134 [00:06<00:01, 19.78it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  77% 103/134 [00:06<00:01, 21.09it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  79% 106/134 [00:06<00:02, 10.91it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  81% 109/134 [00:06<00:01, 12.70it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  83% 111/134 [00:07<00:01, 13.61it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  85% 114/134 [00:07<00:01, 15.50it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  87% 116/134 [00:07<00:01, 16.37it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  89% 119/134 [00:07<00:00, 17.60it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  91% 122/134 [00:07<00:00, 18.41it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  93% 124/134 [00:07<00:00, 18.19it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  94% 126/134 [00:07<00:00, 18.53it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  96% 128/134 [00:07<00:00, 18.47it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  97% 130/134 [00:08<00:00, 17.88it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  99% 132/134 [00:08<00:00, 17.31it/s]\u001b[A\n",
            "                                                                          \u001b[A2024-01-01 17:41:07 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 1.4 | nll_loss 0.308 | ppl 1.24 | wps 75631.7 | wpb 4507.2 | bsz 394.8 | num_updates 1037\n",
            "2024-01-01 17:41:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1037 updates\n",
            "2024-01-01 17:41:07 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:41:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:41:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/deu-models/checkpoint_best.pt (epoch 1 @ 1037 updates, score 1.4) (writing took 0.9051612599996588 seconds)\n",
            "2024-01-01 17:41:07 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2024-01-01 17:41:07 | INFO | train | epoch 001 | loss 2.754 | nll_loss 1.997 | ppl 3.99 | wps 31388.6 | ups 6.86 | wpb 4579.3 | bsz 399.9 | num_updates 1037 | lr 0.000981998 | gnorm 1.184 | clip 44.1 | train_wall 137 | gb_free 13 | wall 153\n",
            "2024-01-01 17:41:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1037\n",
            "epoch 002:   0% 0/1037 [00:00<?, ?it/s]2024-01-01 17:41:07 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2024-01-01 17:41:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002: 100% 1035/1037 [02:22<00:00,  8.05it/s, loss=1.269, nll_loss=0.317, ppl=1.25, wps=34147.7, ups=7.19, wpb=4748.9, bsz=400, num_updates=2000, lr=0.000707107, gnorm=0.293, clip=0, train_wall=13, gb_free=13.9, wall=286]2024-01-01 17:43:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/134 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   1% 2/134 [00:00<00:09, 14.45it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   4% 5/134 [00:00<00:07, 17.79it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   5% 7/134 [00:00<00:06, 18.36it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   7% 9/134 [00:00<00:06, 18.25it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   9% 12/134 [00:00<00:06, 20.00it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  11% 15/134 [00:00<00:05, 20.18it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  13% 18/134 [00:00<00:05, 19.65it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  16% 21/134 [00:01<00:05, 20.64it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  18% 24/134 [00:01<00:05, 20.42it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  20% 27/134 [00:01<00:05, 19.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  22% 29/134 [00:01<00:05, 19.40it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  24% 32/134 [00:01<00:05, 19.36it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  26% 35/134 [00:01<00:04, 20.44it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  28% 38/134 [00:01<00:04, 20.37it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  31% 41/134 [00:02<00:04, 20.34it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  33% 44/134 [00:02<00:04, 20.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  35% 47/134 [00:02<00:04, 20.08it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  37% 50/134 [00:02<00:04, 19.81it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  40% 53/134 [00:02<00:04, 20.00it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  42% 56/134 [00:02<00:03, 20.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  44% 59/134 [00:02<00:03, 19.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  46% 62/134 [00:03<00:03, 20.06it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  49% 65/134 [00:03<00:03, 20.26it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  51% 68/134 [00:03<00:03, 20.67it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  53% 71/134 [00:03<00:03, 19.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  55% 74/134 [00:03<00:03, 19.71it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  57% 77/134 [00:03<00:02, 20.29it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  60% 80/134 [00:04<00:02, 20.16it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  62% 83/134 [00:04<00:02, 19.64it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  63% 85/134 [00:04<00:02, 18.80it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  65% 87/134 [00:04<00:02, 17.53it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  66% 89/134 [00:04<00:02, 18.03it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  68% 91/134 [00:04<00:02, 17.57it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  69% 93/134 [00:04<00:02, 16.87it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  71% 95/134 [00:04<00:02, 16.05it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  72% 97/134 [00:05<00:02, 15.53it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  74% 99/134 [00:05<00:02, 15.30it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  75% 101/134 [00:05<00:02, 14.58it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  77% 103/134 [00:05<00:02, 14.49it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  78% 105/134 [00:05<00:02, 14.41it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  80% 107/134 [00:05<00:01, 14.43it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  81% 109/134 [00:05<00:01, 14.24it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  83% 111/134 [00:06<00:01, 13.38it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  84% 113/134 [00:06<00:01, 14.28it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  86% 115/134 [00:06<00:01, 15.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  87% 117/134 [00:06<00:01, 14.83it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  89% 119/134 [00:06<00:00, 15.39it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  90% 121/134 [00:06<00:00, 14.56it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  92% 123/134 [00:06<00:00, 14.40it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  93% 125/134 [00:07<00:00, 14.34it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  95% 127/134 [00:07<00:00, 13.70it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  96% 129/134 [00:07<00:00, 13.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  98% 131/134 [00:07<00:00, 13.56it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  99% 133/134 [00:07<00:00, 14.40it/s]\u001b[A\n",
            "                                                                          \u001b[A2024-01-01 17:43:38 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 1.267 | nll_loss 0.211 | ppl 1.16 | wps 79646.7 | wpb 4507.2 | bsz 394.8 | num_updates 2074 | best_loss 1.267\n",
            "2024-01-01 17:43:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2074 updates\n",
            "2024-01-01 17:43:38 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:43:39 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:43:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/deu-models/checkpoint_best.pt (epoch 2 @ 2074 updates, score 1.267) (writing took 1.194633436998629 seconds)\n",
            "2024-01-01 17:43:39 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2024-01-01 17:43:39 | INFO | train | epoch 002 | loss 1.347 | nll_loss 0.402 | ppl 1.32 | wps 31245.3 | ups 6.82 | wpb 4579.3 | bsz 399.9 | num_updates 2074 | lr 0.000694377 | gnorm 0.389 | clip 1.7 | train_wall 137 | gb_free 13.9 | wall 305\n",
            "2024-01-01 17:43:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1037\n",
            "epoch 003:   0% 0/1037 [00:00<?, ?it/s]2024-01-01 17:43:39 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2024-01-01 17:43:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003: 100% 1036/1037 [02:23<00:00,  7.19it/s, loss=1.231, nll_loss=0.279, ppl=1.21, wps=32991.2, ups=7.15, wpb=4615.2, bsz=400, num_updates=3100, lr=0.000567962, gnorm=0.255, clip=0, train_wall=13, gb_free=13.6, wall=448]2024-01-01 17:46:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/134 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   1% 2/134 [00:00<00:08, 15.50it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   4% 5/134 [00:00<00:06, 18.58it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   5% 7/134 [00:00<00:06, 18.89it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   7% 9/134 [00:00<00:06, 18.48it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   9% 12/134 [00:00<00:06, 20.09it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  11% 15/134 [00:00<00:05, 20.24it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  13% 18/134 [00:00<00:05, 19.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  16% 21/134 [00:01<00:05, 20.89it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  18% 24/134 [00:01<00:05, 20.27it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  20% 27/134 [00:01<00:05, 20.29it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  22% 30/134 [00:01<00:05, 20.12it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  25% 33/134 [00:01<00:04, 20.28it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  27% 36/134 [00:01<00:04, 20.49it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  29% 39/134 [00:01<00:04, 20.58it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  31% 42/134 [00:02<00:04, 20.50it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  34% 45/134 [00:02<00:04, 20.55it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  36% 48/134 [00:02<00:04, 20.49it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  38% 51/134 [00:02<00:04, 19.80it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  40% 53/134 [00:02<00:04, 19.71it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  42% 56/134 [00:02<00:03, 19.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  44% 59/134 [00:02<00:03, 20.17it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  46% 62/134 [00:03<00:03, 19.80it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  49% 65/134 [00:03<00:03, 20.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  51% 68/134 [00:03<00:03, 21.03it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  53% 71/134 [00:03<00:03, 20.04it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  55% 74/134 [00:03<00:03, 19.89it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  57% 77/134 [00:03<00:02, 19.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  60% 80/134 [00:03<00:02, 20.02it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  62% 83/134 [00:04<00:02, 20.60it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  64% 86/134 [00:04<00:02, 20.42it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  66% 89/134 [00:04<00:02, 20.72it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  69% 92/134 [00:04<00:02, 20.36it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  71% 95/134 [00:04<00:01, 20.36it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  73% 98/134 [00:04<00:01, 20.67it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  75% 101/134 [00:05<00:01, 20.54it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  78% 104/134 [00:05<00:01, 20.71it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  80% 107/134 [00:05<00:01, 20.54it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  82% 110/134 [00:05<00:01, 20.84it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  84% 113/134 [00:05<00:01, 20.27it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  87% 116/134 [00:05<00:00, 20.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  89% 119/134 [00:05<00:00, 20.13it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  91% 122/134 [00:06<00:00, 20.14it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  93% 125/134 [00:06<00:00, 19.73it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  95% 127/134 [00:06<00:00, 19.27it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  96% 129/134 [00:06<00:00, 18.52it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  98% 131/134 [00:06<00:00, 18.12it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 134/134 [00:06<00:00, 20.53it/s]\u001b[A\n",
            "                                                                          \u001b[A2024-01-01 17:46:10 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 1.21 | nll_loss 0.157 | ppl 1.11 | wps 91421.9 | wpb 4507.2 | bsz 394.8 | num_updates 3111 | best_loss 1.21\n",
            "2024-01-01 17:46:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 3111 updates\n",
            "2024-01-01 17:46:10 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:46:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:46:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/deu-models/checkpoint_best.pt (epoch 3 @ 3111 updates, score 1.21) (writing took 0.9193563669996365 seconds)\n",
            "2024-01-01 17:46:11 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2024-01-01 17:46:11 | INFO | train | epoch 003 | loss 1.239 | nll_loss 0.287 | ppl 1.22 | wps 31311.1 | ups 6.84 | wpb 4579.3 | bsz 399.9 | num_updates 3111 | lr 0.000566957 | gnorm 0.275 | clip 0.3 | train_wall 138 | gb_free 13.7 | wall 457\n",
            "2024-01-01 17:46:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1037\n",
            "epoch 004:   0% 0/1037 [00:00<?, ?it/s]2024-01-01 17:46:11 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2024-01-01 17:46:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004: 100% 1036/1037 [02:25<00:00,  7.18it/s, loss=1.198, nll_loss=0.243, ppl=1.18, wps=30039.1, ups=6.7, wpb=4486.3, bsz=400, num_updates=4100, lr=0.000493865, gnorm=0.224, clip=0, train_wall=14, gb_free=13.9, wall=596]2024-01-01 17:48:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/134 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   1% 1/134 [00:00<00:16,  7.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   2% 3/134 [00:00<00:09, 14.13it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   4% 5/134 [00:00<00:08, 15.02it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   5% 7/134 [00:00<00:08, 15.14it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   7% 9/134 [00:00<00:07, 16.07it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   9% 12/134 [00:00<00:06, 18.33it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  10% 14/134 [00:00<00:06, 18.70it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  13% 17/134 [00:00<00:05, 19.60it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  15% 20/134 [00:01<00:05, 20.44it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  17% 23/134 [00:01<00:05, 20.72it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  19% 26/134 [00:01<00:05, 19.76it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  22% 29/134 [00:01<00:05, 19.82it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  24% 32/134 [00:01<00:05, 20.10it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  26% 35/134 [00:01<00:04, 20.17it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  28% 38/134 [00:01<00:04, 21.12it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  31% 41/134 [00:02<00:04, 20.23it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  33% 44/134 [00:02<00:04, 20.05it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  35% 47/134 [00:02<00:04, 20.14it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  37% 50/134 [00:02<00:04, 20.48it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  40% 53/134 [00:02<00:04, 20.16it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  42% 56/134 [00:02<00:03, 19.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  44% 59/134 [00:03<00:03, 19.45it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  46% 61/134 [00:03<00:03, 19.14it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  48% 64/134 [00:03<00:03, 19.54it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  49% 66/134 [00:03<00:03, 19.63it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  51% 68/134 [00:03<00:03, 19.67it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  52% 70/134 [00:03<00:03, 19.72it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  54% 72/134 [00:03<00:03, 19.63it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  55% 74/134 [00:03<00:03, 19.59it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  57% 76/134 [00:03<00:02, 19.39it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  58% 78/134 [00:04<00:02, 19.36it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  60% 80/134 [00:04<00:02, 19.16it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  61% 82/134 [00:04<00:02, 19.25it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  63% 84/134 [00:04<00:02, 19.37it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  64% 86/134 [00:04<00:02, 19.47it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  66% 88/134 [00:04<00:02, 19.26it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  67% 90/134 [00:04<00:02, 19.29it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  69% 93/134 [00:04<00:02, 19.46it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  71% 95/134 [00:04<00:02, 18.66it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  73% 98/134 [00:05<00:01, 19.09it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  75% 100/134 [00:05<00:01, 18.45it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  77% 103/134 [00:05<00:01, 20.24it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  79% 106/134 [00:05<00:01, 20.26it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  81% 109/134 [00:05<00:01, 19.33it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  83% 111/134 [00:05<00:01, 18.98it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  85% 114/134 [00:05<00:00, 20.23it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  87% 117/134 [00:06<00:00, 19.91it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  89% 119/134 [00:06<00:00, 19.57it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  91% 122/134 [00:06<00:00, 19.77it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  93% 124/134 [00:06<00:00, 19.48it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  94% 126/134 [00:06<00:00, 19.02it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  96% 128/134 [00:06<00:00, 18.42it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  97% 130/134 [00:06<00:00, 17.78it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  99% 132/134 [00:06<00:00, 17.22it/s]\u001b[A\n",
            "                                                                          \u001b[A2024-01-01 17:48:44 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 1.198 | nll_loss 0.147 | ppl 1.11 | wps 88482.4 | wpb 4507.2 | bsz 394.8 | num_updates 4148 | best_loss 1.198\n",
            "2024-01-01 17:48:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 4148 updates\n",
            "2024-01-01 17:48:44 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:48:44 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:48:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/deu-models/checkpoint_best.pt (epoch 4 @ 4148 updates, score 1.198) (writing took 0.9826622269993095 seconds)\n",
            "2024-01-01 17:48:45 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2024-01-01 17:48:45 | INFO | train | epoch 004 | loss 1.209 | nll_loss 0.255 | ppl 1.19 | wps 30867.9 | ups 6.74 | wpb 4579.3 | bsz 399.9 | num_updates 4148 | lr 0.000490999 | gnorm 0.235 | clip 0.1 | train_wall 140 | gb_free 13.6 | wall 610\n",
            "2024-01-01 17:48:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1037\n",
            "epoch 005:   0% 0/1037 [00:00<?, ?it/s]2024-01-01 17:48:45 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2024-01-01 17:48:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005: 100% 1036/1037 [02:24<00:00,  8.02it/s, loss=1.21, nll_loss=0.258, ppl=1.2, wps=33900.8, ups=6.94, wpb=4882.6, bsz=399.2, num_updates=5100, lr=0.000442807, gnorm=0.198, clip=0, train_wall=14, gb_free=13.5, wall=744]2024-01-01 17:51:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/134 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   1% 2/134 [00:00<00:09, 13.22it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   3% 4/134 [00:00<00:08, 14.45it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   4% 6/134 [00:00<00:09, 13.82it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   6% 8/134 [00:00<00:08, 14.22it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   7% 10/134 [00:00<00:08, 14.61it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   9% 12/134 [00:00<00:08, 14.78it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  10% 14/134 [00:00<00:07, 15.02it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  12% 16/134 [00:01<00:07, 15.38it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  13% 18/134 [00:01<00:07, 14.65it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  16% 21/134 [00:01<00:07, 14.58it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  18% 24/134 [00:01<00:06, 16.51it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  19% 26/134 [00:01<00:06, 16.32it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  21% 28/134 [00:01<00:06, 15.74it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  22% 30/134 [00:01<00:06, 15.78it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  24% 32/134 [00:02<00:06, 15.46it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  25% 34/134 [00:02<00:06, 14.86it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  28% 37/134 [00:02<00:06, 15.83it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  29% 39/134 [00:02<00:06, 15.40it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  31% 41/134 [00:02<00:06, 14.60it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  32% 43/134 [00:02<00:06, 14.29it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  34% 45/134 [00:03<00:06, 14.10it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  35% 47/134 [00:03<00:06, 14.38it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  37% 49/134 [00:03<00:05, 14.33it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  38% 51/134 [00:03<00:05, 14.68it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  40% 53/134 [00:03<00:05, 14.62it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  41% 55/134 [00:03<00:05, 15.04it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  43% 57/134 [00:03<00:05, 15.14it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  44% 59/134 [00:03<00:05, 14.96it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  46% 61/134 [00:04<00:04, 14.98it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  47% 63/134 [00:04<00:04, 14.64it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  49% 65/134 [00:04<00:04, 14.77it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  50% 67/134 [00:04<00:04, 14.75it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  51% 69/134 [00:04<00:04, 14.92it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  54% 72/134 [00:04<00:03, 16.39it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  55% 74/134 [00:04<00:03, 15.21it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  57% 76/134 [00:05<00:03, 15.76it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  59% 79/134 [00:05<00:03, 17.22it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  60% 81/134 [00:05<00:03, 17.56it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  62% 83/134 [00:05<00:02, 17.95it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  63% 85/134 [00:05<00:02, 17.95it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  66% 88/134 [00:05<00:02, 18.88it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  68% 91/134 [00:05<00:02, 19.34it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  69% 93/134 [00:05<00:02, 19.21it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  71% 95/134 [00:06<00:02, 19.25it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  73% 98/134 [00:06<00:01, 19.23it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  75% 100/134 [00:06<00:01, 19.22it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  77% 103/134 [00:06<00:01, 20.05it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  78% 105/134 [00:06<00:01, 19.41it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  81% 108/134 [00:06<00:01, 19.77it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  83% 111/134 [00:06<00:01, 19.15it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  85% 114/134 [00:06<00:01, 19.54it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  87% 116/134 [00:07<00:00, 19.34it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  89% 119/134 [00:07<00:00, 19.48it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  91% 122/134 [00:07<00:00, 19.74it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  93% 124/134 [00:07<00:00, 19.32it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  94% 126/134 [00:07<00:00, 19.16it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  96% 128/134 [00:07<00:00, 18.75it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  97% 130/134 [00:07<00:00, 18.07it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  99% 132/134 [00:07<00:00, 17.40it/s]\u001b[A\n",
            "                                                                          \u001b[A2024-01-01 17:51:18 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 1.195 | nll_loss 0.14 | ppl 1.1 | wps 75764.2 | wpb 4507.2 | bsz 394.8 | num_updates 5185 | best_loss 1.195\n",
            "2024-01-01 17:51:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 5185 updates\n",
            "2024-01-01 17:51:18 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:51:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:51:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/deu-models/checkpoint_best.pt (epoch 5 @ 5185 updates, score 1.195) (writing took 0.999195406000581 seconds)\n",
            "2024-01-01 17:51:19 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2024-01-01 17:51:19 | INFO | train | epoch 005 | loss 1.192 | nll_loss 0.237 | ppl 1.18 | wps 30896.9 | ups 6.75 | wpb 4579.3 | bsz 399.9 | num_updates 5185 | lr 0.000439163 | gnorm 0.212 | clip 0.2 | train_wall 139 | gb_free 13.6 | wall 764\n",
            "2024-01-01 17:51:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1037\n",
            "epoch 006:   0% 0/1037 [00:00<?, ?it/s]2024-01-01 17:51:19 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2024-01-01 17:51:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006:  78% 814/1037 [01:53<00:31,  7.14it/s, loss=1.18, nll_loss=0.224, ppl=1.17, wps=32973.2, ups=7.3, wpb=4518.9, bsz=400, num_updates=5900, lr=0.000411693, gnorm=0.217, clip=0, train_wall=13, gb_free=14.1, wall=864]2024-01-01 17:53:13 | INFO | fairseq_cli.train | Stopping training due to num_updates: 6000 >= max_update: 6000\n",
            "2024-01-01 17:53:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/134 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   1% 2/134 [00:00<00:08, 15.21it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   4% 5/134 [00:00<00:06, 18.55it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   6% 8/134 [00:00<00:06, 19.75it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   8% 11/134 [00:00<00:06, 19.96it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  10% 13/134 [00:00<00:06, 19.33it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  12% 16/134 [00:00<00:05, 20.10it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  13% 18/134 [00:00<00:05, 19.45it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  16% 21/134 [00:01<00:05, 20.40it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  18% 24/134 [00:01<00:05, 20.47it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  20% 27/134 [00:01<00:05, 20.31it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  22% 30/134 [00:01<00:05, 19.85it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  24% 32/134 [00:01<00:05, 19.07it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  25% 34/134 [00:01<00:05, 18.93it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  28% 37/134 [00:01<00:04, 19.59it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  29% 39/134 [00:01<00:04, 19.26it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  31% 41/134 [00:02<00:05, 17.83it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  33% 44/134 [00:02<00:04, 19.01it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  34% 46/134 [00:02<00:04, 17.61it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  36% 48/134 [00:02<00:04, 17.75it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  37% 50/134 [00:02<00:04, 17.57it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  39% 52/134 [00:02<00:05, 16.34it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  40% 54/134 [00:02<00:04, 16.45it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  42% 56/134 [00:03<00:04, 15.71it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  43% 58/134 [00:03<00:04, 15.43it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  45% 60/134 [00:03<00:04, 15.28it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  46% 62/134 [00:03<00:04, 14.78it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  48% 64/134 [00:03<00:04, 14.60it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  49% 66/134 [00:03<00:04, 14.80it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  51% 68/134 [00:03<00:04, 14.19it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  52% 70/134 [00:04<00:04, 13.88it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  54% 72/134 [00:04<00:04, 13.97it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  55% 74/134 [00:04<00:04, 14.85it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  57% 76/134 [00:04<00:03, 14.84it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  58% 78/134 [00:04<00:03, 14.68it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  60% 80/134 [00:04<00:03, 15.47it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  61% 82/134 [00:04<00:03, 16.24it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  63% 84/134 [00:04<00:03, 15.65it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  64% 86/134 [00:05<00:03, 14.36it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  66% 89/134 [00:05<00:02, 15.96it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  68% 91/134 [00:05<00:02, 15.84it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  69% 93/134 [00:05<00:02, 15.40it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  71% 95/134 [00:05<00:02, 15.00it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  72% 97/134 [00:05<00:02, 15.03it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  74% 99/134 [00:05<00:02, 14.44it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  75% 101/134 [00:06<00:02, 14.21it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  77% 103/134 [00:06<00:02, 14.51it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  79% 106/134 [00:06<00:01, 15.96it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  81% 108/134 [00:06<00:01, 15.42it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  82% 110/134 [00:06<00:01, 15.17it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  84% 112/134 [00:06<00:01, 14.48it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  85% 114/134 [00:06<00:01, 14.62it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  87% 116/134 [00:07<00:01, 14.28it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  88% 118/134 [00:07<00:01, 13.70it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  90% 120/134 [00:07<00:00, 14.05it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  91% 122/134 [00:07<00:00, 14.09it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  93% 124/134 [00:07<00:00, 14.06it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  94% 126/134 [00:07<00:00, 14.98it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  96% 128/134 [00:07<00:00, 15.32it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  97% 130/134 [00:08<00:00, 15.47it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  99% 132/134 [00:08<00:00, 15.52it/s]\u001b[A\n",
            "                                                                          \u001b[A2024-01-01 17:53:21 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 1.192 | nll_loss 0.137 | ppl 1.1 | wps 74016.7 | wpb 4507.2 | bsz 394.8 | num_updates 6000 | best_loss 1.192\n",
            "2024-01-01 17:53:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 6000 updates\n",
            "2024-01-01 17:53:21 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:53:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:53:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/deu-models/checkpoint_best.pt (epoch 6 @ 6000 updates, score 1.192) (writing took 1.0114434990009613 seconds)\n",
            "2024-01-01 17:53:22 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2024-01-01 17:53:22 | INFO | train | epoch 006 | loss 1.185 | nll_loss 0.229 | ppl 1.17 | wps 30314.3 | ups 6.61 | wpb 4584.5 | bsz 399.9 | num_updates 6000 | lr 0.000408248 | gnorm 0.202 | clip 0.1 | train_wall 109 | gb_free 13.3 | wall 887\n",
            "2024-01-01 17:53:22 | INFO | fairseq_cli.train | done training in 887.0 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!chmod 777 test.sh\n",
        "!./test.sh deu no_overlap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0aJPTvvwOOk",
        "outputId": "12a96bcb-6c3c-43e4-de09-0c252825bdd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-01 17:53:37.256359: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-01 17:53:37.256418: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-01 17:53:37.257939: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-01 17:53:37.266618: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-01 17:53:38.361665: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-01 17:53:42 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/deu-models/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin/deu/', 'source_lang': 'deu.no_overlap.input', 'target_lang': 'deu.no_overlap.output', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-01-01 17:53:42 | INFO | fairseq.tasks.translation | [deu.no_overlap.input] dictionary: 112 types\n",
            "2024-01-01 17:53:42 | INFO | fairseq.tasks.translation | [deu.no_overlap.output] dictionary: 88 types\n",
            "2024-01-01 17:53:42 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 17:53:42 | INFO | fairseq.data.data_utils | loaded 51,525 examples from: data-bin/deu/test.deu.no_overlap.input-deu.no_overlap.output.deu.no_overlap.input\n",
            "2024-01-01 17:53:42 | INFO | fairseq.data.data_utils | loaded 51,525 examples from: data-bin/deu/test.deu.no_overlap.input-deu.no_overlap.output.deu.no_overlap.output\n",
            "2024-01-01 17:53:42 | INFO | fairseq.tasks.translation | data-bin/deu/ test deu.no_overlap.input-deu.no_overlap.output 51525 examples\n",
            "2024-01-01 17:58:17 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2024-01-01 17:58:17 | INFO | fairseq_cli.generate | Translated 51,525 sentences (597,433 tokens) in 70.4s (732.10 sentences/s, 8488.70 tokens/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fake copy"
      ],
      "metadata": {
        "id": "JZRXl3Nvv21K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash ./preprocess.sh deu fake_copy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0dxBP6Zv45n",
        "outputId": "89837b6f-5cd0-465f-d9c7-0809147f6234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-01 18:00:02.440700: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-01 18:00:02.440757: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-01 18:00:02.442151: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-01 18:00:02.449887: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-01 18:00:03.559426: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-01 18:00:06 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer='space', bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='deu.fake_copy.input', target_lang='deu.fake_copy.output', trainpref='train', validpref='devel', testpref='test', align_suffix=None, destdir='data-bin/deu', thresholdtgt=5, thresholdsrc=5, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=1, dict_only=False)\n",
            "2024-01-01 18:00:25 | INFO | fairseq_cli.preprocess | [deu.fake_copy.input] Dictionary: 112 types\n",
            "2024-01-01 18:01:15 | INFO | fairseq_cli.preprocess | [deu.fake_copy.input] train.deu.fake_copy.input: 418658 sents, 6579968 tokens, 7.6e-05% replaced (by <unk>)\n",
            "2024-01-01 18:01:15 | INFO | fairseq_cli.preprocess | [deu.fake_copy.input] Dictionary: 112 types\n",
            "2024-01-01 18:01:21 | INFO | fairseq_cli.preprocess | [deu.fake_copy.input] devel.deu.fake_copy.input: 52898 sents, 833121 tokens, 0.00288% replaced (by <unk>)\n",
            "2024-01-01 18:01:21 | INFO | fairseq_cli.preprocess | [deu.fake_copy.input] Dictionary: 112 types\n",
            "2024-01-01 18:01:28 | INFO | fairseq_cli.preprocess | [deu.fake_copy.input] test.deu.fake_copy.input: 51525 sents, 816021 tokens, 0.00098% replaced (by <unk>)\n",
            "2024-01-01 18:01:28 | INFO | fairseq_cli.preprocess | [deu.fake_copy.output] Dictionary: 88 types\n",
            "2024-01-01 18:02:07 | INFO | fairseq_cli.preprocess | [deu.fake_copy.output] train.deu.fake_copy.output: 418658 sents, 4791634 tokens, 0.000167% replaced (by <unk>)\n",
            "2024-01-01 18:02:07 | INFO | fairseq_cli.preprocess | [deu.fake_copy.output] Dictionary: 88 types\n",
            "2024-01-01 18:02:13 | INFO | fairseq_cli.preprocess | [deu.fake_copy.output] devel.deu.fake_copy.output: 52898 sents, 603969 tokens, 0.00397% replaced (by <unk>)\n",
            "2024-01-01 18:02:13 | INFO | fairseq_cli.preprocess | [deu.fake_copy.output] Dictionary: 88 types\n",
            "2024-01-01 18:02:18 | INFO | fairseq_cli.preprocess | [deu.fake_copy.output] test.deu.fake_copy.output: 51525 sents, 596058 tokens, 0.0% replaced (by <unk>)\n",
            "2024-01-01 18:02:18 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/deu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r checkpoints/deu-models/\n",
        "!bash ./train.sh deu fake_copy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUzQMe_WwUDw",
        "outputId": "de8efe04-4ee0-44ff-e7d7-14f96063d0eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-01 18:02:22.124687: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-01 18:02:22.124748: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-01 18:02:22.126102: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-01 18:02:22.133882: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-01 18:02:23.346760: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-01 18:02:24 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2024-01-01 18:02:29 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 212, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 400, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 400, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 6000, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/deu-models', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=212, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=400, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=400, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=0, max_update=6000, stop_time_hours=0, clip_norm=1.0, sentence_avg=False, update_freq=[1], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/deu-models', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-bin/deu', source_lang='deu.fake_copy.input', target_lang='deu.fake_copy.output', load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=1000, warmup_init_lr=-1, pad=1, eos=2, unk=3, dropout=0.3, attention_dropout=0.3, activation_dropout=0.3, activation_fn='relu', encoder_embed_dim=256, encoder_ffn_embed_dim=1024, encoder_layers=4, encoder_attention_heads=4, encoder_normalize_before=True, decoder_embed_dim=256, decoder_ffn_embed_dim=1024, decoder_layers=4, decoder_attention_heads=4, decoder_normalize_before=True, share_decoder_input_output_embed=True, no_seed_provided=False, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=256, decoder_input_dim=256, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'data-bin/deu', 'source_lang': 'deu.fake_copy.input', 'target_lang': 'deu.fake_copy.output', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 1000, 'warmup_init_lr': -1.0, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-01-01 18:02:29 | INFO | fairseq.tasks.translation | [deu.fake_copy.input] dictionary: 112 types\n",
            "2024-01-01 18:02:29 | INFO | fairseq.tasks.translation | [deu.fake_copy.output] dictionary: 88 types\n",
            "2024-01-01 18:02:29 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(112, 256, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-3): 4 x TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(88, 256, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-3): 4 x TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=256, out_features=88, bias=False)\n",
            "  )\n",
            ")\n",
            "2024-01-01 18:02:29 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2024-01-01 18:02:29 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2024-01-01 18:02:29 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2024-01-01 18:02:29 | INFO | fairseq_cli.train | num. shared model params: 10,579,968 (num. trained: 10,579,968)\n",
            "2024-01-01 18:02:29 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2024-01-01 18:02:29 | INFO | fairseq.data.data_utils | loaded 52,898 examples from: data-bin/deu/valid.deu.fake_copy.input-deu.fake_copy.output.deu.fake_copy.input\n",
            "2024-01-01 18:02:29 | INFO | fairseq.data.data_utils | loaded 52,898 examples from: data-bin/deu/valid.deu.fake_copy.input-deu.fake_copy.output.deu.fake_copy.output\n",
            "2024-01-01 18:02:29 | INFO | fairseq.tasks.translation | data-bin/deu valid deu.fake_copy.input-deu.fake_copy.output 52898 examples\n",
            "2024-01-01 18:02:29 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2024-01-01 18:02:29 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-01-01 18:02:29 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2024-01-01 18:02:29 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-01-01 18:02:29 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2024-01-01 18:02:29 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 400\n",
            "2024-01-01 18:02:29 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/deu-models/checkpoint_last.pt\n",
            "2024-01-01 18:02:29 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/deu-models/checkpoint_last.pt\n",
            "2024-01-01 18:02:29 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2024-01-01 18:02:29 | INFO | fairseq.data.data_utils | loaded 418,658 examples from: data-bin/deu/train.deu.fake_copy.input-deu.fake_copy.output.deu.fake_copy.input\n",
            "2024-01-01 18:02:29 | INFO | fairseq.data.data_utils | loaded 418,658 examples from: data-bin/deu/train.deu.fake_copy.input-deu.fake_copy.output.deu.fake_copy.output\n",
            "2024-01-01 18:02:29 | INFO | fairseq.tasks.translation | data-bin/deu train deu.fake_copy.input-deu.fake_copy.output 418658 examples\n",
            "2024-01-01 18:02:29 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2024-01-01 18:02:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1048\n",
            "epoch 001:   0% 0/1048 [00:00<?, ?it/s]2024-01-01 18:02:29 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2024-01-01 18:02:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001:   1% 9/1048 [00:02<02:48,  6.16it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "epoch 001: 100% 1047/1048 [02:21<00:00,  7.41it/s, loss=1.559, nll_loss=0.629, ppl=1.55, wps=34688.8, ups=7.48, wpb=4636, bsz=400, num_updates=1000, lr=0.001, gnorm=0.627, clip=9, train_wall=13, gb_free=14, wall=135]2024-01-01 18:04:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/134 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   1% 1/134 [00:00<00:17,  7.69it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   3% 4/134 [00:00<00:07, 17.44it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   4% 6/134 [00:00<00:07, 17.81it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   7% 9/134 [00:00<00:06, 18.85it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   9% 12/134 [00:00<00:06, 19.80it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  11% 15/134 [00:00<00:05, 19.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  13% 18/134 [00:00<00:05, 19.83it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  16% 21/134 [00:01<00:05, 20.41it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  18% 24/134 [00:01<00:05, 19.82it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  20% 27/134 [00:01<00:05, 19.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  22% 30/134 [00:01<00:05, 20.08it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  25% 33/134 [00:01<00:05, 20.15it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  27% 36/134 [00:01<00:04, 20.07it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  29% 39/134 [00:01<00:04, 20.04it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  31% 42/134 [00:02<00:04, 20.20it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  34% 45/134 [00:02<00:04, 20.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  36% 48/134 [00:02<00:04, 20.04it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  38% 51/134 [00:02<00:04, 20.36it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  40% 54/134 [00:02<00:04, 19.90it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  42% 56/134 [00:02<00:04, 19.47it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  43% 58/134 [00:02<00:04, 18.50it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  46% 61/134 [00:03<00:03, 19.68it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  48% 64/134 [00:03<00:03, 19.68it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  49% 66/134 [00:03<00:03, 19.14it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  51% 69/134 [00:03<00:03, 19.79it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  53% 71/134 [00:03<00:03, 19.81it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  54% 73/134 [00:03<00:03, 19.76it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  56% 75/134 [00:03<00:02, 19.75it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  57% 77/134 [00:03<00:02, 19.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  59% 79/134 [00:04<00:02, 19.75it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  61% 82/134 [00:04<00:02, 19.71it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  63% 85/134 [00:04<00:02, 19.80it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  66% 88/134 [00:04<00:02, 20.48it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  68% 91/134 [00:05<00:03, 11.01it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  69% 93/134 [00:05<00:03, 12.26it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  72% 96/134 [00:05<00:02, 14.51it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  73% 98/134 [00:05<00:02, 15.24it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  75% 100/134 [00:05<00:02, 15.93it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  77% 103/134 [00:05<00:01, 17.49it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  78% 105/134 [00:05<00:01, 17.68it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  80% 107/134 [00:05<00:01, 18.05it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  81% 109/134 [00:05<00:01, 18.39it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  83% 111/134 [00:06<00:01, 18.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  85% 114/134 [00:06<00:01, 19.29it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  87% 117/134 [00:06<00:00, 20.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  90% 120/134 [00:06<00:00, 18.31it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  91% 122/134 [00:06<00:00, 18.54it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  93% 124/134 [00:06<00:00, 17.05it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  94% 126/134 [00:06<00:00, 17.22it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  96% 128/134 [00:06<00:00, 16.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  97% 130/134 [00:07<00:00, 14.92it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  99% 132/134 [00:07<00:00, 14.58it/s]\u001b[A\n",
            "                                                                          \u001b[A2024-01-01 18:04:59 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 1.39 | nll_loss 0.294 | ppl 1.23 | wps 83254.8 | wpb 4507.2 | bsz 394.8 | num_updates 1048\n",
            "2024-01-01 18:04:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1048 updates\n",
            "2024-01-01 18:04:59 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 18:04:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 18:05:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/deu-models/checkpoint_best.pt (epoch 1 @ 1048 updates, score 1.39) (writing took 1.1521865020004043 seconds)\n",
            "2024-01-01 18:05:00 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2024-01-01 18:05:00 | INFO | train | epoch 001 | loss 2.722 | nll_loss 1.96 | ppl 3.89 | wps 32115.1 | ups 7.02 | wpb 4572.2 | bsz 399.5 | num_updates 1048 | lr 0.000976831 | gnorm 1.169 | clip 43.7 | train_wall 136 | gb_free 13.1 | wall 151\n",
            "2024-01-01 18:05:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1048\n",
            "epoch 002:   0% 0/1048 [00:00<?, ?it/s]2024-01-01 18:05:00 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2024-01-01 18:05:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002: 100% 1047/1048 [02:24<00:00,  8.34it/s, loss=1.25, nll_loss=0.296, ppl=1.23, wps=33355.5, ups=7.23, wpb=4616.1, bsz=400, num_updates=2000, lr=0.000707107, gnorm=0.264, clip=0, train_wall=13, gb_free=13.7, wall=282]2024-01-01 18:07:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/134 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   1% 2/134 [00:00<00:08, 16.08it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   4% 5/134 [00:00<00:06, 18.60it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   6% 8/134 [00:00<00:06, 20.13it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   8% 11/134 [00:00<00:06, 19.23it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  10% 14/134 [00:00<00:05, 20.14it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  13% 17/134 [00:00<00:05, 19.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  15% 20/134 [00:01<00:05, 20.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  17% 23/134 [00:01<00:05, 20.16it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  19% 26/134 [00:01<00:05, 19.65it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  21% 28/134 [00:01<00:05, 19.58it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  23% 31/134 [00:01<00:05, 19.30it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  25% 34/134 [00:01<00:05, 19.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  28% 37/134 [00:01<00:04, 20.16it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  30% 40/134 [00:02<00:04, 19.67it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  31% 42/134 [00:02<00:04, 19.74it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  33% 44/134 [00:02<00:04, 19.77it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  34% 46/134 [00:02<00:04, 19.63it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  37% 49/134 [00:02<00:04, 19.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  39% 52/134 [00:02<00:04, 19.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  40% 54/134 [00:02<00:04, 19.58it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  42% 56/134 [00:02<00:04, 19.42it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  43% 58/134 [00:02<00:04, 18.45it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  46% 61/134 [00:03<00:03, 19.02it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  47% 63/134 [00:03<00:03, 19.13it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  49% 66/134 [00:03<00:03, 20.11it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  51% 69/134 [00:03<00:03, 20.14it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  54% 72/134 [00:03<00:03, 19.66it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  55% 74/134 [00:03<00:03, 19.65it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  57% 76/134 [00:03<00:02, 19.44it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  58% 78/134 [00:03<00:02, 19.50it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  60% 80/134 [00:04<00:02, 19.13it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  62% 83/134 [00:04<00:02, 19.88it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  63% 85/134 [00:04<00:02, 19.08it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  66% 88/134 [00:04<00:02, 19.59it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  68% 91/134 [00:04<00:02, 19.72it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  70% 94/134 [00:04<00:01, 20.36it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  72% 97/134 [00:04<00:01, 19.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  75% 100/134 [00:05<00:01, 19.60it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  77% 103/134 [00:05<00:01, 20.34it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  79% 106/134 [00:05<00:01, 20.04it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  81% 109/134 [00:05<00:01, 19.83it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  83% 111/134 [00:05<00:01, 19.65it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  85% 114/134 [00:05<00:00, 20.54it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  87% 117/134 [00:05<00:00, 20.59it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  90% 120/134 [00:06<00:00, 19.76it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  92% 123/134 [00:06<00:00, 19.77it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  94% 126/134 [00:06<00:00, 19.90it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  96% 128/134 [00:06<00:00, 19.41it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  97% 130/134 [00:06<00:00, 18.55it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  99% 132/134 [00:06<00:00, 17.84it/s]\u001b[A\n",
            "                                                                          \u001b[A2024-01-01 18:07:31 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 1.229 | nll_loss 0.174 | ppl 1.13 | wps 89552.8 | wpb 4507.2 | bsz 394.8 | num_updates 2096 | best_loss 1.229\n",
            "2024-01-01 18:07:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2096 updates\n",
            "2024-01-01 18:07:31 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 18:07:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 18:07:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/deu-models/checkpoint_best.pt (epoch 2 @ 2096 updates, score 1.229) (writing took 0.993017487000543 seconds)\n",
            "2024-01-01 18:07:32 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2024-01-01 18:07:32 | INFO | train | epoch 002 | loss 1.339 | nll_loss 0.394 | ppl 1.31 | wps 31430.8 | ups 6.87 | wpb 4572.2 | bsz 399.5 | num_updates 2096 | lr 0.000690724 | gnorm 0.386 | clip 1.6 | train_wall 139 | gb_free 13.9 | wall 303\n",
            "2024-01-01 18:07:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1048\n",
            "epoch 003:   0% 0/1048 [00:00<?, ?it/s]2024-01-01 18:07:32 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2024-01-01 18:07:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003: 100% 1047/1048 [02:25<00:00,  6.73it/s, loss=1.215, nll_loss=0.26, ppl=1.2, wps=33641.1, ups=7.07, wpb=4757, bsz=400, num_updates=3100, lr=0.000567962, gnorm=0.248, clip=0, train_wall=14, gb_free=13.7, wall=443]2024-01-01 18:09:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/134 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   1% 1/134 [00:00<00:16,  8.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   3% 4/134 [00:00<00:08, 15.50it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   5% 7/134 [00:00<00:06, 19.13it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   7% 9/134 [00:00<00:06, 19.26it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   8% 11/134 [00:00<00:06, 19.30it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  10% 14/134 [00:00<00:06, 19.92it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  12% 16/134 [00:00<00:05, 19.78it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  13% 18/134 [00:00<00:05, 19.58it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  16% 21/134 [00:01<00:05, 20.66it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  18% 24/134 [00:01<00:05, 20.79it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  20% 27/134 [00:01<00:05, 20.54it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  22% 30/134 [00:01<00:05, 20.55it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  25% 33/134 [00:01<00:04, 21.21it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  27% 36/134 [00:01<00:04, 20.68it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  29% 39/134 [00:01<00:04, 20.51it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  31% 42/134 [00:02<00:04, 20.15it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  34% 45/134 [00:02<00:04, 20.72it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  36% 48/134 [00:02<00:04, 20.31it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  38% 51/134 [00:02<00:04, 20.35it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  40% 54/134 [00:02<00:03, 20.31it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  43% 57/134 [00:02<00:03, 20.38it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  45% 60/134 [00:02<00:03, 20.82it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  47% 63/134 [00:03<00:03, 20.60it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  49% 66/134 [00:03<00:03, 20.56it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  51% 69/134 [00:03<00:03, 20.34it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  54% 72/134 [00:03<00:03, 20.07it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  56% 75/134 [00:03<00:02, 20.13it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  58% 78/134 [00:03<00:02, 19.87it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  60% 81/134 [00:04<00:02, 19.41it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  63% 84/134 [00:04<00:02, 20.14it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  65% 87/134 [00:04<00:02, 20.91it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  67% 90/134 [00:04<00:02, 20.45it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  69% 93/134 [00:04<00:02, 20.10it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  72% 96/134 [00:04<00:01, 20.37it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  74% 99/134 [00:04<00:01, 20.73it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  76% 102/134 [00:05<00:01, 21.06it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  78% 105/134 [00:05<00:01, 20.07it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  81% 108/134 [00:05<00:01, 20.76it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  83% 111/134 [00:05<00:01, 19.37it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  85% 114/134 [00:05<00:00, 20.45it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  87% 117/134 [00:05<00:00, 20.90it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  90% 120/134 [00:05<00:00, 20.66it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  92% 123/134 [00:06<00:00, 20.04it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  94% 126/134 [00:06<00:00, 19.92it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  96% 129/134 [00:06<00:00, 18.72it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  98% 131/134 [00:06<00:00, 18.30it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 134/134 [00:06<00:00, 20.50it/s]\u001b[A\n",
            "                                                                          \u001b[A2024-01-01 18:10:05 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 1.211 | nll_loss 0.156 | ppl 1.11 | wps 92017.3 | wpb 4507.2 | bsz 394.8 | num_updates 3144 | best_loss 1.211\n",
            "2024-01-01 18:10:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 3144 updates\n",
            "2024-01-01 18:10:05 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 18:10:05 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 18:10:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/deu-models/checkpoint_best.pt (epoch 3 @ 3144 updates, score 1.211) (writing took 1.2390812159992493 seconds)\n",
            "2024-01-01 18:10:06 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2024-01-01 18:10:06 | INFO | train | epoch 003 | loss 1.239 | nll_loss 0.287 | ppl 1.22 | wps 31171.9 | ups 6.82 | wpb 4572.2 | bsz 399.5 | num_updates 3144 | lr 0.000563974 | gnorm 0.276 | clip 0.7 | train_wall 140 | gb_free 13.7 | wall 457\n",
            "2024-01-01 18:10:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1048\n",
            "epoch 004:   0% 0/1048 [00:00<?, ?it/s]2024-01-01 18:10:06 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2024-01-01 18:10:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004: 100% 1047/1048 [02:26<00:00,  7.28it/s, loss=1.196, nll_loss=0.24, ppl=1.18, wps=33016.8, ups=7.32, wpb=4508.5, bsz=400, num_updates=4100, lr=0.000493865, gnorm=0.233, clip=1, train_wall=13, gb_free=13.9, wall=592]2024-01-01 18:12:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/134 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   1% 2/134 [00:00<00:10, 12.13it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   3% 4/134 [00:00<00:09, 13.07it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   4% 6/134 [00:00<00:09, 14.00it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   6% 8/134 [00:00<00:08, 14.03it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   7% 10/134 [00:00<00:08, 14.54it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  10% 13/134 [00:00<00:08, 15.02it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  11% 15/134 [00:01<00:07, 15.11it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  13% 17/134 [00:01<00:07, 14.64it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  14% 19/134 [00:01<00:07, 14.49it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  16% 21/134 [00:01<00:07, 15.18it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  17% 23/134 [00:01<00:07, 15.27it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  19% 25/134 [00:01<00:07, 14.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  20% 27/134 [00:01<00:07, 14.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  22% 29/134 [00:01<00:07, 14.75it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  23% 31/134 [00:02<00:07, 14.50it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  25% 33/134 [00:02<00:07, 13.64it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  27% 36/134 [00:02<00:06, 15.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  28% 38/134 [00:02<00:05, 16.11it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  30% 40/134 [00:02<00:06, 15.61it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  31% 42/134 [00:02<00:05, 15.85it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  33% 44/134 [00:02<00:05, 15.33it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  34% 46/134 [00:03<00:05, 15.28it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  36% 48/134 [00:03<00:05, 14.76it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  37% 50/134 [00:03<00:05, 15.03it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  39% 52/134 [00:03<00:05, 15.09it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  40% 54/134 [00:03<00:05, 14.60it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  42% 56/134 [00:03<00:05, 14.19it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  43% 58/134 [00:03<00:05, 13.60it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  45% 60/134 [00:04<00:05, 13.83it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  46% 62/134 [00:04<00:05, 14.06it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  48% 64/134 [00:04<00:05, 13.17it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  50% 67/134 [00:04<00:04, 15.79it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  51% 69/134 [00:04<00:04, 16.09it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  53% 71/134 [00:04<00:03, 16.73it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  54% 73/134 [00:04<00:03, 16.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  56% 75/134 [00:04<00:03, 17.34it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  57% 77/134 [00:05<00:03, 17.26it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  60% 80/134 [00:05<00:02, 18.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  61% 82/134 [00:05<00:02, 18.50it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  63% 84/134 [00:05<00:02, 18.37it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  65% 87/134 [00:05<00:02, 19.76it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  66% 89/134 [00:05<00:02, 19.38it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  69% 92/134 [00:05<00:02, 19.87it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  70% 94/134 [00:05<00:02, 19.00it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  72% 96/134 [00:06<00:02, 18.77it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  73% 98/134 [00:06<00:01, 18.77it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  75% 100/134 [00:06<00:01, 18.11it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  77% 103/134 [00:06<00:01, 18.91it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  79% 106/134 [00:06<00:01, 18.85it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  81% 109/134 [00:06<00:01, 19.26it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  83% 111/134 [00:06<00:01, 18.80it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  84% 113/134 [00:06<00:01, 19.09it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  86% 115/134 [00:07<00:01, 18.50it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  87% 117/134 [00:07<00:00, 18.40it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  89% 119/134 [00:07<00:00, 18.62it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  91% 122/134 [00:07<00:00, 19.24it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  93% 124/134 [00:07<00:00, 19.16it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  94% 126/134 [00:07<00:00, 19.22it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  96% 128/134 [00:07<00:00, 18.38it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  97% 130/134 [00:07<00:00, 17.65it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  99% 132/134 [00:08<00:00, 16.99it/s]\u001b[A\n",
            "                                                                          \u001b[A2024-01-01 18:12:41 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 1.199 | nll_loss 0.144 | ppl 1.1 | wps 75354.6 | wpb 4507.2 | bsz 394.8 | num_updates 4192 | best_loss 1.199\n",
            "2024-01-01 18:12:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 4192 updates\n",
            "2024-01-01 18:12:41 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 18:12:41 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 18:12:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/deu-models/checkpoint_best.pt (epoch 4 @ 4192 updates, score 1.199) (writing took 0.9966941330003465 seconds)\n",
            "2024-01-01 18:12:42 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2024-01-01 18:12:42 | INFO | train | epoch 004 | loss 1.209 | nll_loss 0.255 | ppl 1.19 | wps 30712.2 | ups 6.72 | wpb 4572.2 | bsz 399.5 | num_updates 4192 | lr 0.000488415 | gnorm 0.247 | clip 0.6 | train_wall 141 | gb_free 13.6 | wall 613\n",
            "2024-01-01 18:12:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1048\n",
            "epoch 005:   0% 0/1048 [00:00<?, ?it/s]2024-01-01 18:12:42 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2024-01-01 18:12:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005: 100% 1047/1048 [02:25<00:00,  8.23it/s, loss=1.185, nll_loss=0.228, ppl=1.17, wps=33722.3, ups=6.93, wpb=4867.1, bsz=400, num_updates=5200, lr=0.000438529, gnorm=0.192, clip=0, train_wall=14, gb_free=13.7, wall=753]2024-01-01 18:15:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/134 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   1% 2/134 [00:00<00:08, 16.49it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   4% 5/134 [00:00<00:06, 18.98it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   6% 8/134 [00:00<00:06, 19.62it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   8% 11/134 [00:00<00:05, 20.83it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  10% 14/134 [00:00<00:05, 20.60it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  13% 17/134 [00:00<00:05, 21.07it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  15% 20/134 [00:00<00:05, 20.75it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  17% 23/134 [00:01<00:05, 19.42it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  19% 26/134 [00:01<00:05, 18.84it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  21% 28/134 [00:01<00:05, 18.16it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  22% 30/134 [00:01<00:06, 16.20it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  25% 33/134 [00:01<00:05, 17.20it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  26% 35/134 [00:01<00:06, 16.20it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  28% 37/134 [00:02<00:06, 15.95it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  29% 39/134 [00:02<00:05, 16.42it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  31% 42/134 [00:02<00:05, 17.21it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  33% 44/134 [00:02<00:05, 16.65it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  34% 46/134 [00:02<00:05, 16.14it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  36% 48/134 [00:02<00:05, 15.27it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  38% 51/134 [00:02<00:05, 15.63it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  40% 53/134 [00:03<00:05, 14.89it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  41% 55/134 [00:03<00:04, 15.86it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  43% 57/134 [00:03<00:05, 15.40it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  45% 60/134 [00:03<00:04, 16.22it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  46% 62/134 [00:03<00:04, 16.26it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  48% 64/134 [00:03<00:04, 15.71it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  49% 66/134 [00:03<00:04, 15.62it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  51% 68/134 [00:04<00:04, 15.27it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  52% 70/134 [00:04<00:04, 15.00it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  54% 72/134 [00:04<00:03, 15.93it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  55% 74/134 [00:04<00:03, 15.18it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  57% 76/134 [00:04<00:04, 14.35it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  58% 78/134 [00:04<00:03, 15.65it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  60% 81/134 [00:04<00:03, 17.65it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  62% 83/134 [00:04<00:03, 16.61it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  63% 85/134 [00:05<00:03, 15.34it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  65% 87/134 [00:05<00:03, 15.40it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  66% 89/134 [00:05<00:02, 15.51it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  68% 91/134 [00:05<00:02, 15.38it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  69% 93/134 [00:05<00:02, 15.12it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  71% 95/134 [00:05<00:02, 14.66it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  72% 97/134 [00:05<00:02, 15.10it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  74% 99/134 [00:06<00:02, 14.74it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  75% 101/134 [00:06<00:02, 14.42it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  77% 103/134 [00:06<00:02, 14.23it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  78% 105/134 [00:06<00:01, 15.49it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  80% 107/134 [00:06<00:01, 14.52it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  82% 110/134 [00:06<00:01, 16.75it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  84% 112/134 [00:06<00:01, 17.03it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  85% 114/134 [00:06<00:01, 17.72it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  87% 117/134 [00:07<00:00, 19.25it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  89% 119/134 [00:07<00:00, 18.11it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  91% 122/134 [00:07<00:00, 18.96it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  93% 124/134 [00:07<00:00, 18.81it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  94% 126/134 [00:07<00:00, 18.99it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  96% 128/134 [00:07<00:00, 18.48it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  97% 130/134 [00:07<00:00, 17.59it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  99% 132/134 [00:07<00:00, 17.19it/s]\u001b[A\n",
            "                                                                          \u001b[A2024-01-01 18:15:16 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 1.196 | nll_loss 0.141 | ppl 1.1 | wps 76258.9 | wpb 4507.2 | bsz 394.8 | num_updates 5240 | best_loss 1.196\n",
            "2024-01-01 18:15:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 5240 updates\n",
            "2024-01-01 18:15:16 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 18:15:16 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 18:15:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/deu-models/checkpoint_best.pt (epoch 5 @ 5240 updates, score 1.196) (writing took 0.9972864289993595 seconds)\n",
            "2024-01-01 18:15:17 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2024-01-01 18:15:17 | INFO | train | epoch 005 | loss 1.192 | nll_loss 0.237 | ppl 1.18 | wps 30983.7 | ups 6.78 | wpb 4572.2 | bsz 399.5 | num_updates 5240 | lr 0.000436852 | gnorm 0.215 | clip 0.2 | train_wall 140 | gb_free 13.6 | wall 768\n",
            "2024-01-01 18:15:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1048\n",
            "epoch 006:   0% 0/1048 [00:00<?, ?it/s]2024-01-01 18:15:17 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2024-01-01 18:15:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006:  72% 759/1048 [01:46<00:48,  5.97it/s, loss=1.18, nll_loss=0.224, ppl=1.17, wps=32723.1, ups=7.16, wpb=4568.7, bsz=400, num_updates=5900, lr=0.000411693, gnorm=0.193, clip=0, train_wall=13, gb_free=13.7, wall=860]2024-01-01 18:17:03 | INFO | fairseq_cli.train | Stopping training due to num_updates: 6000 >= max_update: 6000\n",
            "2024-01-01 18:17:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/134 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   1% 2/134 [00:00<00:10, 12.14it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   3% 4/134 [00:00<00:09, 13.63it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   4% 6/134 [00:00<00:09, 14.06it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   6% 8/134 [00:00<00:09, 13.98it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   7% 10/134 [00:00<00:08, 14.45it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   9% 12/134 [00:00<00:07, 15.56it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  10% 14/134 [00:00<00:08, 14.50it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  12% 16/134 [00:01<00:07, 14.80it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  13% 18/134 [00:01<00:07, 14.76it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  16% 21/134 [00:01<00:07, 15.73it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  17% 23/134 [00:01<00:06, 16.50it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  19% 25/134 [00:01<00:06, 17.18it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  20% 27/134 [00:01<00:06, 16.98it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  22% 30/134 [00:01<00:06, 16.66it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  25% 33/134 [00:02<00:05, 18.60it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  26% 35/134 [00:02<00:05, 18.89it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  28% 37/134 [00:02<00:05, 19.02it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  30% 40/134 [00:02<00:04, 19.49it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  31% 42/134 [00:02<00:04, 19.37it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  34% 45/134 [00:02<00:04, 19.86it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  35% 47/134 [00:02<00:04, 19.63it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  37% 49/134 [00:02<00:04, 19.57it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  39% 52/134 [00:03<00:04, 19.55it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  40% 54/134 [00:03<00:04, 18.94it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  43% 57/134 [00:03<00:03, 19.46it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  44% 59/134 [00:03<00:03, 19.32it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  46% 62/134 [00:03<00:03, 20.34it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  49% 65/134 [00:03<00:03, 19.73it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  51% 68/134 [00:03<00:03, 20.63it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  53% 71/134 [00:03<00:03, 20.14it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  55% 74/134 [00:04<00:03, 19.19it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  57% 77/134 [00:04<00:02, 19.85it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  60% 80/134 [00:04<00:02, 19.98it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  62% 83/134 [00:04<00:02, 19.70it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  64% 86/134 [00:04<00:02, 20.36it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  66% 89/134 [00:04<00:02, 20.80it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  69% 92/134 [00:05<00:02, 20.66it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  71% 95/134 [00:05<00:01, 20.51it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  73% 98/134 [00:05<00:01, 19.61it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  75% 100/134 [00:05<00:01, 19.44it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  77% 103/134 [00:05<00:01, 20.32it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  79% 106/134 [00:05<00:01, 20.62it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  81% 109/134 [00:05<00:01, 20.50it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  84% 112/134 [00:06<00:01, 20.36it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  86% 115/134 [00:06<00:00, 21.16it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  88% 118/134 [00:06<00:00, 20.52it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  90% 121/134 [00:06<00:00, 20.33it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  93% 124/134 [00:06<00:00, 19.86it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  95% 127/134 [00:06<00:00, 19.47it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  96% 129/134 [00:06<00:00, 18.96it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  98% 131/134 [00:06<00:00, 18.50it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset: 100% 134/134 [00:07<00:00, 20.78it/s]\u001b[A\n",
            "                                                                          \u001b[A2024-01-01 18:17:10 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 1.195 | nll_loss 0.14 | ppl 1.1 | wps 85686.9 | wpb 4507.2 | bsz 394.8 | num_updates 6000 | best_loss 1.195\n",
            "2024-01-01 18:17:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 6000 updates\n",
            "2024-01-01 18:17:10 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 18:17:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/morphology_project/checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 18:17:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/deu-models/checkpoint_best.pt (epoch 6 @ 6000 updates, score 1.195) (writing took 0.9818551460011804 seconds)\n",
            "2024-01-01 18:17:11 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2024-01-01 18:17:11 | INFO | train | epoch 006 | loss 1.185 | nll_loss 0.229 | ppl 1.17 | wps 30541.7 | ups 6.64 | wpb 4602.8 | bsz 399.3 | num_updates 6000 | lr 0.000408248 | gnorm 0.209 | clip 0.4 | train_wall 102 | gb_free 13.6 | wall 882\n",
            "2024-01-01 18:17:11 | INFO | fairseq_cli.train | done training in 881.8 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!chmod 777 test.sh\n",
        "!./test.sh deu fake_copy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvJLmTRCwU_Y",
        "outputId": "4eba1a67-0fd2-4b2e-9d42-b804edc3fb51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-01 18:18:12.729701: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-01 18:18:12.729764: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-01 18:18:12.731154: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-01 18:18:12.738529: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-01 18:18:14.034955: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-01 18:18:21 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/deu-models/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin/deu/', 'source_lang': 'deu.fake_copy.input', 'target_lang': 'deu.fake_copy.output', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-01-01 18:18:21 | INFO | fairseq.tasks.translation | [deu.fake_copy.input] dictionary: 112 types\n",
            "2024-01-01 18:18:21 | INFO | fairseq.tasks.translation | [deu.fake_copy.output] dictionary: 88 types\n",
            "2024-01-01 18:18:21 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/deu-models/checkpoint_best.pt\n",
            "2024-01-01 18:18:22 | INFO | fairseq.data.data_utils | loaded 51,525 examples from: data-bin/deu/test.deu.fake_copy.input-deu.fake_copy.output.deu.fake_copy.input\n",
            "2024-01-01 18:18:22 | INFO | fairseq.data.data_utils | loaded 51,525 examples from: data-bin/deu/test.deu.fake_copy.input-deu.fake_copy.output.deu.fake_copy.output\n",
            "2024-01-01 18:18:22 | INFO | fairseq.tasks.translation | data-bin/deu/ test deu.fake_copy.input-deu.fake_copy.output 51525 examples\n",
            "2024-01-01 18:22:56 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2024-01-01 18:22:56 | INFO | fairseq_cli.generate | Translated 51,525 sentences (596,448 tokens) in 69.0s (746.39 sentences/s, 8640.12 tokens/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "bR15htWkEmu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_deu_random = create_results_table(\"test_deu_random_results.txt\")\n",
        "results_deu_no_overlap = create_results_table(\"test_deu_no_overlap_results.txt\")\n",
        "results_deu_fake_copy = create_results_table(\"test_deu_fake_copy_results.txt\")"
      ],
      "metadata": {
        "id": "2BSrWKaXEoSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_true, y_pre\n",
        "print(f'accuracy random: {accuracy_score(results_deu_random.target, results_deu_random.predicted)}')\n",
        "print(f'accuracy no_overlap: {accuracy_score(results_deu_no_overlap.target, results_deu_no_overlap.predicted)}')\n",
        "print(f'accuracy fake_copy: {accuracy_score(results_deu_fake_copy.target, results_deu_fake_copy.predicted)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZLUsuBbEzYN",
        "outputId": "407afcfb-230b-4d94-f921-928e1e7fc1c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy random: 0.878532216122508\n",
            "accuracy no_overlap: 0.8795730228044638\n",
            "accuracy fake_copy: 0.8678117418728772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, these are very good results as well (even if they don't reach as much accuracy as the Catalan model, which was predictable since Catalan is quite regular and the dataset is simpler). Taking into account the higher complexity of the data (mixing nouns, verbs and adjectives) the accuracy is quite good (almost reaching 90%). We can also be sure that the great amount of data populating the dataset benefits the learning process of the model.\n",
        "\n",
        "It is curious to see that the German model, even if \"cheating\" (`random` model that has information of the test in the training step) achieves a similar (even slightly lower) performance that the other two models (`no_overlap` and `fake_copy`).\n",
        "\n",
        "Let's take a look at where those models fail.\n",
        "\n",
        "We will first take a look at some mistakes our best model has done:"
      ],
      "metadata": {
        "id": "AcOX_vscb_md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_deu_no_overlap[results_deu_no_overlap.target != results_deu_no_overlap.predicted].sample(15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "tkQh3s6zdREI",
        "outputId": "413b55b9-7214-4407-e4f6-11d65b9aa176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      lemma  \\\n",
              "idx                                           \n",
              "36925        k a r t o g r a p h i e r e n    \n",
              "41886                  Ã¼ b e r z i e h e n    \n",
              "36294                  e r d r o s s e l n    \n",
              "14185              R e v o l v e r h e l d    \n",
              "35682                      k n i s t e r n    \n",
              "29825                      m i t e s s e n    \n",
              "16669                T a g e s b e g i n n    \n",
              "30363                        u m g e h e n    \n",
              "1505                     Z a h n p a s t a    \n",
              "24388                  b e s c h e i d e n    \n",
              "6689     S a u e r s t o f f f l a s c h e    \n",
              "13429          P l a s t i k b e s t e c k    \n",
              "32800                  Ã¼ b e r o r d n e n    \n",
              "11174                    S t e i n b o c k    \n",
              "689                    M o d e g e w Ã¼ r z    \n",
              "\n",
              "                                                 source_s  \\\n",
              "idx                                                         \n",
              "36925     k a r t o g r a p h i e r e n # V SBJV SG 1 PRS   \n",
              "41886                Ã¼ b e r z i e h e n # V IND PL 3 PRS   \n",
              "36294                    e r d r o s s e l n # V IMP SG 2   \n",
              "14185             R e v o l v e r h e l d # N GEN MASC PL   \n",
              "35682                   k n i s t e r n # V SBJV SG 1 PRS   \n",
              "29825                        m i t e s s e n # V IMP SG 2   \n",
              "16669               T a g e s b e g i n n # N GEN MASC SG   \n",
              "30363                     u m g e h e n # V SBJV SG 2 PRS   \n",
              "1505                     Z a h n p a s t a # N ACC FEM PL   \n",
              "24388                    b e s c h e i d e n # V.PTCP PST   \n",
              "6689     S a u e r s t o f f f l a s c h e # N NOM FEM SG   \n",
              "13429         P l a s t i k b e s t e c k # N DAT NEUT SG   \n",
              "32800                Ã¼ b e r o r d n e n # V IND SG 1 PRS   \n",
              "11174                   S t e i n b o c k # N GEN MASC SG   \n",
              "689                   M o d e g e w Ã¼ r z # N DAT NEUT PL   \n",
              "\n",
              "                                    target                         predicted  \n",
              "idx                                                                           \n",
              "36925          k a r t o g r a p h i e r e     r a p h i e r e _ k a r t o g  \n",
              "41886                z i e h e n _ Ã¼ b e r               Ã¼ b e r z i e h e n  \n",
              "36294                    e r d r o s s l e                 e r d r o s s e l  \n",
              "14185          R e v o l v e r h e l d e n         R e v o l v e r h e l d e  \n",
              "35682                        k n i s t r e                   k n i s t e r e  \n",
              "29825                                e s s                   e s s e _ m i t  \n",
              "16669              T a g e s b e g i n n s         T a g e s b e g i n n e s  \n",
              "30363                      u m g e h e s t                 g e h e s t _ u m  \n",
              "1505                 Z a h n p a s t a e n               Z a h n p a s t a s  \n",
              "24388                  b e s c h i e d e n               b e s c h e i d e t  \n",
              "6689     S a u e r s t o f f f l a s c h e   S a u e r s t o f f l a s c h e  \n",
              "13429          P l a s t i k b e s t e c k     P l a s t i k b e s t e c k e  \n",
              "32800                  o r d n e _ Ã¼ b e r                 Ã¼ b e r o r d n e  \n",
              "11174                  S t e i n b o c k s             S t e i n b o c k e s  \n",
              "689                M o d e g e w Ã¼ r z e n         M o d e g e w Ã¼ r z e r n  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6cdcac58-047f-4d18-b4a9-25870f5a6fbf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lemma</th>\n",
              "      <th>source_s</th>\n",
              "      <th>target</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>idx</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>36925</th>\n",
              "      <td>k a r t o g r a p h i e r e n</td>\n",
              "      <td>k a r t o g r a p h i e r e n # V SBJV SG 1 PRS</td>\n",
              "      <td>k a r t o g r a p h i e r e</td>\n",
              "      <td>r a p h i e r e _ k a r t o g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41886</th>\n",
              "      <td>Ã¼ b e r z i e h e n</td>\n",
              "      <td>Ã¼ b e r z i e h e n # V IND PL 3 PRS</td>\n",
              "      <td>z i e h e n _ Ã¼ b e r</td>\n",
              "      <td>Ã¼ b e r z i e h e n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36294</th>\n",
              "      <td>e r d r o s s e l n</td>\n",
              "      <td>e r d r o s s e l n # V IMP SG 2</td>\n",
              "      <td>e r d r o s s l e</td>\n",
              "      <td>e r d r o s s e l</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14185</th>\n",
              "      <td>R e v o l v e r h e l d</td>\n",
              "      <td>R e v o l v e r h e l d # N GEN MASC PL</td>\n",
              "      <td>R e v o l v e r h e l d e n</td>\n",
              "      <td>R e v o l v e r h e l d e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35682</th>\n",
              "      <td>k n i s t e r n</td>\n",
              "      <td>k n i s t e r n # V SBJV SG 1 PRS</td>\n",
              "      <td>k n i s t r e</td>\n",
              "      <td>k n i s t e r e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29825</th>\n",
              "      <td>m i t e s s e n</td>\n",
              "      <td>m i t e s s e n # V IMP SG 2</td>\n",
              "      <td>e s s</td>\n",
              "      <td>e s s e _ m i t</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16669</th>\n",
              "      <td>T a g e s b e g i n n</td>\n",
              "      <td>T a g e s b e g i n n # N GEN MASC SG</td>\n",
              "      <td>T a g e s b e g i n n s</td>\n",
              "      <td>T a g e s b e g i n n e s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30363</th>\n",
              "      <td>u m g e h e n</td>\n",
              "      <td>u m g e h e n # V SBJV SG 2 PRS</td>\n",
              "      <td>u m g e h e s t</td>\n",
              "      <td>g e h e s t _ u m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1505</th>\n",
              "      <td>Z a h n p a s t a</td>\n",
              "      <td>Z a h n p a s t a # N ACC FEM PL</td>\n",
              "      <td>Z a h n p a s t a e n</td>\n",
              "      <td>Z a h n p a s t a s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24388</th>\n",
              "      <td>b e s c h e i d e n</td>\n",
              "      <td>b e s c h e i d e n # V.PTCP PST</td>\n",
              "      <td>b e s c h i e d e n</td>\n",
              "      <td>b e s c h e i d e t</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6689</th>\n",
              "      <td>S a u e r s t o f f f l a s c h e</td>\n",
              "      <td>S a u e r s t o f f f l a s c h e # N NOM FEM SG</td>\n",
              "      <td>S a u e r s t o f f f l a s c h e</td>\n",
              "      <td>S a u e r s t o f f l a s c h e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13429</th>\n",
              "      <td>P l a s t i k b e s t e c k</td>\n",
              "      <td>P l a s t i k b e s t e c k # N DAT NEUT SG</td>\n",
              "      <td>P l a s t i k b e s t e c k</td>\n",
              "      <td>P l a s t i k b e s t e c k e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32800</th>\n",
              "      <td>Ã¼ b e r o r d n e n</td>\n",
              "      <td>Ã¼ b e r o r d n e n # V IND SG 1 PRS</td>\n",
              "      <td>o r d n e _ Ã¼ b e r</td>\n",
              "      <td>Ã¼ b e r o r d n e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11174</th>\n",
              "      <td>S t e i n b o c k</td>\n",
              "      <td>S t e i n b o c k # N GEN MASC SG</td>\n",
              "      <td>S t e i n b o c k s</td>\n",
              "      <td>S t e i n b o c k e s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>689</th>\n",
              "      <td>M o d e g e w Ã¼ r z</td>\n",
              "      <td>M o d e g e w Ã¼ r z # N DAT NEUT PL</td>\n",
              "      <td>M o d e g e w Ã¼ r z e n</td>\n",
              "      <td>M o d e g e w Ã¼ r z e r n</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6cdcac58-047f-4d18-b4a9-25870f5a6fbf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6cdcac58-047f-4d18-b4a9-25870f5a6fbf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6cdcac58-047f-4d18-b4a9-25870f5a6fbf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1344e449-5614-4589-8d51-02f4fe3f139e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1344e449-5614-4589-8d51-02f4fe3f139e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1344e449-5614-4589-8d51-02f4fe3f139e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How interesting!\n",
        "We can see that some mistakes that could have been common are:\n",
        "* **Ortography mistakes**: Albatrosse and\tAlbatrÃ¶se are almost identical but there is one missing `s` and the \"umlaÃ¼t\" symbol (Â¨) in the o.\n",
        "* **Afix positioning**: we observe several underscores (_) which show the need of locating other words in between, as we have said German is an agglutinative language and structures can become complex very quickly.\n",
        "* **Letter swaps**: \"sichern\" has been derived as \"sichre\" when it should be \"sicher\". This phenomena is quite common when it comes to `-re` particles, but our model has not been able to infer this.\n",
        "* There are other mistakes that the model clearly predicted incorrectly, such as the last sample shown \"Trilemmata\" and \"Trilemmas\"\n",
        "\n",
        "* Note, also, that this model deals with **lowercase and uppercase**. This could be hinting the model what lemmas are nouns (starting in uppercase) and what are not (lowercase in all the word) but at the same time is making the model to interpret twice as much characters than if we only had lowercase.\n",
        "Basically, lowercasing is adding an unnecessary complexity to the model, given that the \"is a noun\"-information is already provided in the morphology tags (the information that appears after the # in `source_s`). One improvement to see if the model performs better (or not) is to train again replacing the uppercase letters with lowercase and see if \"the effort\" put into differentiating between upper and lowercase is canalized into learning to predict better."
      ],
      "metadata": {
        "id": "rBUpWCUud0rT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_grammatical_category_correctness(results_deu_random)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "OuwQlOvy4E43",
        "outputId": "844ce116-27a1-4e50-b4b7-ad22cdc607d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_grammatical_category_correctness(results_deu_no_overlap)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "6oxG_Y3L3N_m",
        "outputId": "84815135-0a2f-4685-cca3-5f3f900ec79a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_grammatical_category_correctness(results_deu_fake_copy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "Cs7lJQad3iFR",
        "outputId": "ddbd5704-d9c5-4ed6-dd35-21f56b02959d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The `random` split is the one with less predictable capability when it comes to adjectives.\n",
        "* The `random` split predicts better the nouns than the verbs, whereas the `fake_copy` one is the other way around. The `no_overlap` seems to have a similar competence with both Nouns and Verbs.\n",
        "* Overall, all models seem to have a \"linear\" relationship with the accuracy and the amount of data. That is, the more data a category has, the more accurate predictions it will have. However, more errors we will have. In other words, the distributions are similar between correct and incorrect samples according to the size of each group."
      ],
      "metadata": {
        "id": "WqIRlIL84eSs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given that the accuracy is similar in all the datasets and we have not observed any significant difference in the distribution of errors or correct predictions, we will be taking a look at the `no_overlap` dataset results.\n",
        "\n",
        "We will now analyze which specific morphological tags the model has performed best with. We will start with Nouns:"
      ],
      "metadata": {
        "id": "glKMPReS6PWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_N_deu_no_overlap = results_deu_no_overlap[results_deu_no_overlap.grammatical_category == 'N']\n",
        "results_N_deu_no_overlap['case'] = results_N_deu_no_overlap.source_s.map(get_case)\n",
        "results_N_deu_no_overlap['gender'] = results_N_deu_no_overlap.source_s.map(get_gender)\n",
        "results_N_deu_no_overlap['number'] = results_N_deu_no_overlap.source_s.map(get_number)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djccMt8t6KMJ",
        "outputId": "c3b89dfd-1b98-41f2-f168-3603f47f7945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-e156ec0a551a>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  results_N_deu_no_overlap['case'] = results_N_deu_no_overlap.source_s.map(get_case)\n",
            "<ipython-input-22-e156ec0a551a>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  results_N_deu_no_overlap['gender'] = results_N_deu_no_overlap.source_s.map(get_gender)\n",
            "<ipython-input-22-e156ec0a551a>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  results_N_deu_no_overlap['number'] = results_N_deu_no_overlap.source_s.map(get_number)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each noun has a case:\n",
        "* Nominative\n",
        "* Accusative\n",
        "* Dative\n",
        "* Genitive"
      ],
      "metadata": {
        "id": "Fikj7eqhNRT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_N_deu_no_overlap.case.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV2PtxZG8cKF",
        "outputId": "744dc483-a761-4c06-b341-c7be3e9a0901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['NOM', 'ACC', 'DAT', 'GEN'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gener can be:\n",
        "* Masculine\n",
        "* Feminine\n",
        "* Neutral\n",
        "* Or any combination of the previous.\n",
        "\n",
        "(*) You can see 'NA' as one of the genders. It wasn't specified in the data so we added this tag in the processing of the data."
      ],
      "metadata": {
        "id": "DSPfHpLyNXjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_N_deu_no_overlap.gender.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SKbLzHx8fMZ",
        "outputId": "5d9309e2-3678-4c7c-bd0f-a3775699d444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['NEUT', 'MASC', 'FEM', 'NA', 'MASC+FEM', 'MASC+NEUT', 'FEM+NEUT'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number can only be singular or plural:"
      ],
      "metadata": {
        "id": "WN37LfWvNqf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_N_deu_no_overlap.number.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RwMuAu-9d7h",
        "outputId": "a3c76268-eeea-47bf-dd85-ef220330f5a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['SG', 'PL'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contingency_table = pd.crosstab(results_N_deu_no_overlap.correct, [results_N_deu_no_overlap.case, results_N_deu_no_overlap.gender, results_N_deu_no_overlap.number])\n",
        "contingency_table = contingency_table.T.reset_index()\n",
        "contingency_table['total_samples'] = contingency_table[False] + contingency_table[True]\n",
        "contingency_table['accuracy'] = contingency_table[True] / contingency_table['total_samples']\n",
        "contingency_table = contingency_table.sort_values(by=['accuracy'], ascending=False)"
      ],
      "metadata": {
        "id": "p8UZ1t7A_9a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contingency_table[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "hB4NtPQPs0qg",
        "outputId": "bbdea062-5a31-4be0-a95b-eccfb0b6619c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2 style=\"text-align: center;\"></h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table style=\"display:inline\"></td></th><th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2 style=\"text-align: center;\"></h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>b</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table style=\"display:inline\"></td></th>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contingency_table[-10:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        },
        "id": "WpIuwJ2IFoo4",
        "outputId": "27553981-2eea-4947-ff50-c8bc0512307c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2 style=\"text-align: center;\"></h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>correct</th>\n",
              "      <th>case</th>\n",
              "      <th>gender</th>\n",
              "      <th>number</th>\n",
              "      <th>False</th>\n",
              "      <th>True</th>\n",
              "      <th>total_samples</th>\n",
              "      <th>accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ACC</td>\n",
              "      <td>MASC+NEUT</td>\n",
              "      <td>SG</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>23</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ACC</td>\n",
              "      <td>FEM+NEUT</td>\n",
              "      <td>SG</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>NOM</td>\n",
              "      <td>MASC+NEUT</td>\n",
              "      <td>SG</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>23</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>DAT</td>\n",
              "      <td>FEM+NEUT</td>\n",
              "      <td>SG</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>NOM</td>\n",
              "      <td>MASC+FEM</td>\n",
              "      <td>SG</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>NOM</td>\n",
              "      <td>FEM+NEUT</td>\n",
              "      <td>SG</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>DAT</td>\n",
              "      <td>MASC+FEM</td>\n",
              "      <td>PL</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>ACC</td>\n",
              "      <td>NA</td>\n",
              "      <td>SG</td>\n",
              "      <td>1</td>\n",
              "      <td>208</td>\n",
              "      <td>209</td>\n",
              "      <td>0.995215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>NOM</td>\n",
              "      <td>NA</td>\n",
              "      <td>SG</td>\n",
              "      <td>1</td>\n",
              "      <td>208</td>\n",
              "      <td>209</td>\n",
              "      <td>0.995215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>NOM</td>\n",
              "      <td>MASC</td>\n",
              "      <td>SG</td>\n",
              "      <td>5</td>\n",
              "      <td>1033</td>\n",
              "      <td>1038</td>\n",
              "      <td>0.995183</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table style=\"display:inline\"></td></th><th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2 style=\"text-align: center;\"></h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>correct</th>\n",
              "      <th>case</th>\n",
              "      <th>gender</th>\n",
              "      <th>number</th>\n",
              "      <th>False</th>\n",
              "      <th>True</th>\n",
              "      <th>total_samples</th>\n",
              "      <th>accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>NOM</td>\n",
              "      <td>MASC</td>\n",
              "      <td>PL</td>\n",
              "      <td>239</td>\n",
              "      <td>745</td>\n",
              "      <td>984</td>\n",
              "      <td>0.757114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>GEN</td>\n",
              "      <td>NEUT</td>\n",
              "      <td>PL</td>\n",
              "      <td>136</td>\n",
              "      <td>416</td>\n",
              "      <td>552</td>\n",
              "      <td>0.753623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>GEN</td>\n",
              "      <td>FEM+NEUT</td>\n",
              "      <td>SG</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>GEN</td>\n",
              "      <td>MASC</td>\n",
              "      <td>SG</td>\n",
              "      <td>470</td>\n",
              "      <td>912</td>\n",
              "      <td>1382</td>\n",
              "      <td>0.659913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>DAT</td>\n",
              "      <td>MASC+FEM</td>\n",
              "      <td>SG</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>14</td>\n",
              "      <td>0.642857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>GEN</td>\n",
              "      <td>MASC+FEM</td>\n",
              "      <td>SG</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>0.214286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>NOM</td>\n",
              "      <td>FEM+NEUT</td>\n",
              "      <td>PL</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>DAT</td>\n",
              "      <td>FEM+NEUT</td>\n",
              "      <td>PL</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>GEN</td>\n",
              "      <td>FEM+NEUT</td>\n",
              "      <td>PL</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ACC</td>\n",
              "      <td>FEM+NEUT</td>\n",
              "      <td>PL</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table style=\"display:inline\"></td></th>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions, improvements and future work"
      ],
      "metadata": {
        "id": "BciHvtvc4Kb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We have obtained a good accuracy in all models (almost 90%). However, as mentioned in the Catalan section, the `random` split results can be misleading, since we are training with information that appear later on the test set. This makes the model \"remember\" structures rather than infer them, thus the model is likely to not generalise well when facing an unseen sample.\n",
        "* All three models have a similar distribution of errors and correct predictions, this is why some of our conclusions have been only analysed in the `no_overlap` model, which is the one providing better accuracy:\n",
        "  * Our model struggles with irregular verbs or ortographical structures. Punctuation signs such as diaeresis or changes in the letter order when declining a word (re $â†”$ er) make our model struggle.\n",
        "  * A possible improvement (to be confirmed) could be training the model without casing, since in this data we include words starting with upper and lowercase, which can add complexity to the model. However, this would not imply a great improvement overall. This can be applied to any of the models (`random`, `no_overlap`, `fake_copy`).\n"
      ],
      "metadata": {
        "id": "Zuz8VwLcgFJm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GOVEu5K-gde-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General Conclusions\n",
        "\n",
        "* All models in both languages have taken less than 10 minutes each to train.\n",
        "* Training a Neural Morphological Generator is easy when you have good data and does not require very big models.\n",
        "* `random` split results can be misleading, since we are leaking test information during the training. This model is prone to \"remember\" rather than acutally infer, which is a more desirable behaviour when training a neural network.\n",
        "* In terms of perfromance, there seems to be no big difference between `no_overlap` and `fake_copy` models.\n",
        "* Catalan models have achieved an accuracy of 95%, higher than German models: either because of the simpler structure of the data (only verbs) or because the grammar is more regular. The Catalan dataset also had more balanced samples, which also helps.\n",
        "* The German models still have proved to be quite good, with almost 90% accuracy.\n",
        "* Both morphologies were quite different.\n",
        "* Given the high performance it is hard to improve models more. However new improvements could be:\n",
        "  * Removing casing in the German dataset.\n",
        "  * Experimenting with the network size, to reduce resources and make the models lighter.\n",
        "  * Create a new neural network with both datasets and make it bilingual."
      ],
      "metadata": {
        "id": "6U2ZofN8O8io"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FZn8v9xyXlbX",
        "ldQLAZ5twqJV",
        "yenoYOuMuHBV",
        "eCd9SRY9ekUw",
        "6NhE9JKCYopt",
        "hOYXsQivAAWW",
        "y51WWydWAVX4",
        "dwww3CJnupmq",
        "snbk9s4musT1",
        "7gTRMW2Rrx91",
        "3PPhOvsRsxIa"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}